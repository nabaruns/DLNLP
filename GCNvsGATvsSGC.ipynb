{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/4tb/nabarun/nlp/GAT\n"
     ]
    }
   ],
   "source": [
    "%cd /4tb/nabarun/nlp/GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: cora\n",
      "----- Opt. hyperparams -----\n",
      "lr: 0.005\n",
      "l2_coef: 0.0005\n",
      "----- Archi. hyperparams -----\n",
      "nb. layers: 1\n",
      "nb. units per layer: [8]\n",
      "nb. attention heads: [8, 1]\n",
      "residual: False\n",
      "nonlinearity: <function elu at 0x7f5fa26772f0>\n",
      "model: <class 'models.gat.GAT'>\n",
      "(2708, 2708)\n",
      "(2708, 1433)\n",
      "WARNING:tensorflow:From train.py:68: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /4tb/nabarun/nlp/GAT/utils/layers.py:9: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /4tb/nabarun/nlp/GAT/utils/layers.py:11: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv1D` instead.\n",
      "WARNING:tensorflow:From /home/nabarun/.local/lib/python3.6/site-packages/tensorflow_core/python/layers/convolutional.py:218: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /4tb/nabarun/nlp/GAT/models/base_gattn.py:41: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /4tb/nabarun/nlp/GAT/models/base_gattn.py:12: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /4tb/nabarun/nlp/GAT/models/base_gattn.py:17: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:89: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:91: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:91: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:97: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2021-01-20 23:35:31.878468: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2021-01-20 23:35:31.894220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\n",
      "pciBusID: 0000:4b:00.0\n",
      "2021-01-20 23:35:31.895422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: \n",
      "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\n",
      "pciBusID: 0000:4c:00.0\n",
      "2021-01-20 23:35:31.896370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 2 with properties: \n",
      "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\n",
      "pciBusID: 0000:4d:00.0\n",
      "2021-01-20 23:35:31.897312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 3 with properties: \n",
      "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\n",
      "pciBusID: 0000:4e:00.0\n",
      "2021-01-20 23:35:31.897484: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory\n",
      "2021-01-20 23:35:31.897577: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory\n",
      "2021-01-20 23:35:31.897663: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory\n",
      "2021-01-20 23:35:31.897748: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory\n",
      "2021-01-20 23:35:31.897836: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory\n",
      "2021-01-20 23:35:31.897922: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory\n",
      "2021-01-20 23:35:31.902810: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-01-20 23:35:31.902866: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-01-20 23:35:31.903307: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-01-20 23:35:31.929588: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000205000 Hz\n",
      "2021-01-20 23:35:31.930206: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x69cf820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-01-20 23:35:31.930243: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-01-20 23:35:32.487338: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7895bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2021-01-20 23:35:32.487385: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2021-01-20 23:35:32.487403: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2021-01-20 23:35:32.487415: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2021-01-20 23:35:32.487425: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2021-01-20 23:35:32.487834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-01-20 23:35:32.487855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "Training: loss = 1.94526, acc = 0.13571 | Val: loss = 1.94491, acc = 0.13200\n",
      "Training: loss = 1.94003, acc = 0.21429 | Val: loss = 1.94226, acc = 0.10400\n",
      "Training: loss = 1.95146, acc = 0.12857 | Val: loss = 1.94253, acc = 0.07200\n",
      "Training: loss = 1.93490, acc = 0.23571 | Val: loss = 1.94116, acc = 0.08600\n",
      "Training: loss = 1.94604, acc = 0.17857 | Val: loss = 1.93940, acc = 0.13000\n",
      "Training: loss = 1.93461, acc = 0.23571 | Val: loss = 1.93725, acc = 0.20600\n",
      "Training: loss = 1.92753, acc = 0.24286 | Val: loss = 1.93584, acc = 0.22600\n",
      "Training: loss = 1.92861, acc = 0.19286 | Val: loss = 1.93484, acc = 0.24400\n",
      "Training: loss = 1.92560, acc = 0.21429 | Val: loss = 1.93329, acc = 0.24000\n",
      "Training: loss = 1.92648, acc = 0.17143 | Val: loss = 1.93122, acc = 0.23400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: loss = 1.92896, acc = 0.21429 | Val: loss = 1.92753, acc = 0.23600\n",
      "Training: loss = 1.91714, acc = 0.27143 | Val: loss = 1.92375, acc = 0.24400\n",
      "Training: loss = 1.92282, acc = 0.26429 | Val: loss = 1.92017, acc = 0.28800\n",
      "Training: loss = 1.89736, acc = 0.30714 | Val: loss = 1.91612, acc = 0.34200\n",
      "Training: loss = 1.90848, acc = 0.23571 | Val: loss = 1.91282, acc = 0.54800\n",
      "Training: loss = 1.91894, acc = 0.24286 | Val: loss = 1.90933, acc = 0.71200\n",
      "Training: loss = 1.92028, acc = 0.28571 | Val: loss = 1.90591, acc = 0.76600\n",
      "Training: loss = 1.89989, acc = 0.27857 | Val: loss = 1.90285, acc = 0.76200\n",
      "Training: loss = 1.89811, acc = 0.27857 | Val: loss = 1.90042, acc = 0.75000\n",
      "Training: loss = 1.90163, acc = 0.29286 | Val: loss = 1.89764, acc = 0.72800\n",
      "Training: loss = 1.90832, acc = 0.26429 | Val: loss = 1.89432, acc = 0.69600\n",
      "Training: loss = 1.87745, acc = 0.36429 | Val: loss = 1.89091, acc = 0.69000\n",
      "Training: loss = 1.87642, acc = 0.35000 | Val: loss = 1.88751, acc = 0.67800\n",
      "Training: loss = 1.87680, acc = 0.39286 | Val: loss = 1.88435, acc = 0.69600\n",
      "Training: loss = 1.86853, acc = 0.33571 | Val: loss = 1.88136, acc = 0.72000\n",
      "Training: loss = 1.87496, acc = 0.43571 | Val: loss = 1.87909, acc = 0.75200\n",
      "Training: loss = 1.86848, acc = 0.43571 | Val: loss = 1.87703, acc = 0.77400\n",
      "Training: loss = 1.87761, acc = 0.30714 | Val: loss = 1.87562, acc = 0.78200\n",
      "Training: loss = 1.85409, acc = 0.39286 | Val: loss = 1.87455, acc = 0.77600\n",
      "Training: loss = 1.84515, acc = 0.42857 | Val: loss = 1.87305, acc = 0.78000\n",
      "Training: loss = 1.82152, acc = 0.47857 | Val: loss = 1.87129, acc = 0.77600\n",
      "Training: loss = 1.85205, acc = 0.35000 | Val: loss = 1.86971, acc = 0.78000\n",
      "Training: loss = 1.84201, acc = 0.40000 | Val: loss = 1.86841, acc = 0.77000\n",
      "Training: loss = 1.86962, acc = 0.37857 | Val: loss = 1.86654, acc = 0.76200\n",
      "Training: loss = 1.81282, acc = 0.45714 | Val: loss = 1.86469, acc = 0.73800\n",
      "Training: loss = 1.82746, acc = 0.45000 | Val: loss = 1.86228, acc = 0.72600\n",
      "Training: loss = 1.83857, acc = 0.43571 | Val: loss = 1.85950, acc = 0.71800\n",
      "Training: loss = 1.78733, acc = 0.45714 | Val: loss = 1.85615, acc = 0.71400\n",
      "Training: loss = 1.81341, acc = 0.46429 | Val: loss = 1.85231, acc = 0.74000\n",
      "Training: loss = 1.80285, acc = 0.50714 | Val: loss = 1.84803, acc = 0.75200\n",
      "Training: loss = 1.80023, acc = 0.47857 | Val: loss = 1.84355, acc = 0.75800\n",
      "Training: loss = 1.80340, acc = 0.45714 | Val: loss = 1.83908, acc = 0.75200\n",
      "Training: loss = 1.81165, acc = 0.44286 | Val: loss = 1.83446, acc = 0.75800\n",
      "Training: loss = 1.79316, acc = 0.39286 | Val: loss = 1.83022, acc = 0.73000\n",
      "Training: loss = 1.75645, acc = 0.47143 | Val: loss = 1.82719, acc = 0.71400\n",
      "Training: loss = 1.78864, acc = 0.44286 | Val: loss = 1.82312, acc = 0.68800\n",
      "Training: loss = 1.79746, acc = 0.38571 | Val: loss = 1.81865, acc = 0.66600\n",
      "Training: loss = 1.75384, acc = 0.50714 | Val: loss = 1.81460, acc = 0.63800\n",
      "Training: loss = 1.74570, acc = 0.51429 | Val: loss = 1.80964, acc = 0.61600\n",
      "Training: loss = 1.75057, acc = 0.44286 | Val: loss = 1.80471, acc = 0.61800\n",
      "Training: loss = 1.74208, acc = 0.47143 | Val: loss = 1.79949, acc = 0.62000\n",
      "Training: loss = 1.75733, acc = 0.44286 | Val: loss = 1.79374, acc = 0.64400\n",
      "Training: loss = 1.68457, acc = 0.51429 | Val: loss = 1.78779, acc = 0.68400\n",
      "Training: loss = 1.76995, acc = 0.42857 | Val: loss = 1.78125, acc = 0.71600\n",
      "Training: loss = 1.77298, acc = 0.37857 | Val: loss = 1.77492, acc = 0.75000\n",
      "Training: loss = 1.66050, acc = 0.52857 | Val: loss = 1.77028, acc = 0.77800\n",
      "Training: loss = 1.77046, acc = 0.42143 | Val: loss = 1.76541, acc = 0.80200\n",
      "Training: loss = 1.74006, acc = 0.45714 | Val: loss = 1.76102, acc = 0.80600\n",
      "Training: loss = 1.73220, acc = 0.43571 | Val: loss = 1.75638, acc = 0.81200\n",
      "Training: loss = 1.69782, acc = 0.42857 | Val: loss = 1.75256, acc = 0.80800\n",
      "Training: loss = 1.69029, acc = 0.50714 | Val: loss = 1.74824, acc = 0.80400\n",
      "Training: loss = 1.66804, acc = 0.50714 | Val: loss = 1.74363, acc = 0.80800\n",
      "Training: loss = 1.67774, acc = 0.53571 | Val: loss = 1.73952, acc = 0.80800\n",
      "Training: loss = 1.68541, acc = 0.49286 | Val: loss = 1.73557, acc = 0.79200\n",
      "Training: loss = 1.62272, acc = 0.49286 | Val: loss = 1.73166, acc = 0.77200\n",
      "Training: loss = 1.70017, acc = 0.46429 | Val: loss = 1.72788, acc = 0.76400\n",
      "Training: loss = 1.65227, acc = 0.45714 | Val: loss = 1.72368, acc = 0.76200\n",
      "Training: loss = 1.61436, acc = 0.54286 | Val: loss = 1.71865, acc = 0.77000\n",
      "Training: loss = 1.65829, acc = 0.53571 | Val: loss = 1.71348, acc = 0.77200\n",
      "Training: loss = 1.65551, acc = 0.48571 | Val: loss = 1.70907, acc = 0.77800\n",
      "Training: loss = 1.62620, acc = 0.49286 | Val: loss = 1.70544, acc = 0.78600\n",
      "Training: loss = 1.60256, acc = 0.50714 | Val: loss = 1.70109, acc = 0.78800\n",
      "Training: loss = 1.60738, acc = 0.50714 | Val: loss = 1.69558, acc = 0.80200\n",
      "Training: loss = 1.55027, acc = 0.52857 | Val: loss = 1.68969, acc = 0.79200\n",
      "Training: loss = 1.58699, acc = 0.55000 | Val: loss = 1.68310, acc = 0.79600\n",
      "Training: loss = 1.62509, acc = 0.52857 | Val: loss = 1.67552, acc = 0.79400\n",
      "Training: loss = 1.60407, acc = 0.53571 | Val: loss = 1.66702, acc = 0.79200\n",
      "Training: loss = 1.55935, acc = 0.56429 | Val: loss = 1.65860, acc = 0.79200\n",
      "Training: loss = 1.57777, acc = 0.52857 | Val: loss = 1.65147, acc = 0.78800\n",
      "Training: loss = 1.64810, acc = 0.47143 | Val: loss = 1.64552, acc = 0.78800\n",
      "Training: loss = 1.58290, acc = 0.52143 | Val: loss = 1.64041, acc = 0.79400\n",
      "Training: loss = 1.63975, acc = 0.45000 | Val: loss = 1.63721, acc = 0.79000\n",
      "Training: loss = 1.64775, acc = 0.47143 | Val: loss = 1.63497, acc = 0.78800\n",
      "Training: loss = 1.61084, acc = 0.52857 | Val: loss = 1.63188, acc = 0.78400\n",
      "Training: loss = 1.61486, acc = 0.46429 | Val: loss = 1.62852, acc = 0.77800\n",
      "Training: loss = 1.61028, acc = 0.45000 | Val: loss = 1.62491, acc = 0.77600\n",
      "Training: loss = 1.54832, acc = 0.50714 | Val: loss = 1.61986, acc = 0.77800\n",
      "Training: loss = 1.49913, acc = 0.55000 | Val: loss = 1.61471, acc = 0.78200\n",
      "Training: loss = 1.53082, acc = 0.47143 | Val: loss = 1.61026, acc = 0.78400\n",
      "Training: loss = 1.53875, acc = 0.49286 | Val: loss = 1.60544, acc = 0.78400\n",
      "Training: loss = 1.50023, acc = 0.52857 | Val: loss = 1.59922, acc = 0.78400\n",
      "Training: loss = 1.54046, acc = 0.50000 | Val: loss = 1.59273, acc = 0.78400\n",
      "Training: loss = 1.44164, acc = 0.53571 | Val: loss = 1.58661, acc = 0.78200\n",
      "Training: loss = 1.56293, acc = 0.46429 | Val: loss = 1.57928, acc = 0.79200\n",
      "Training: loss = 1.51225, acc = 0.55000 | Val: loss = 1.57192, acc = 0.79600\n",
      "Training: loss = 1.58197, acc = 0.52143 | Val: loss = 1.56534, acc = 0.80000\n",
      "Training: loss = 1.55713, acc = 0.52143 | Val: loss = 1.55865, acc = 0.80400\n",
      "Training: loss = 1.49831, acc = 0.51429 | Val: loss = 1.55293, acc = 0.80400\n",
      "Training: loss = 1.56942, acc = 0.52857 | Val: loss = 1.54729, acc = 0.80400\n",
      "Training: loss = 1.47183, acc = 0.51429 | Val: loss = 1.54260, acc = 0.80400\n",
      "Training: loss = 1.54669, acc = 0.46429 | Val: loss = 1.53963, acc = 0.80400\n",
      "Training: loss = 1.54592, acc = 0.47857 | Val: loss = 1.53754, acc = 0.80200\n",
      "Training: loss = 1.56387, acc = 0.47857 | Val: loss = 1.53503, acc = 0.79800\n",
      "Training: loss = 1.51002, acc = 0.52143 | Val: loss = 1.53186, acc = 0.79400\n",
      "Training: loss = 1.43666, acc = 0.55000 | Val: loss = 1.52753, acc = 0.79400\n",
      "Training: loss = 1.51484, acc = 0.52143 | Val: loss = 1.52290, acc = 0.79400\n",
      "Training: loss = 1.54952, acc = 0.40000 | Val: loss = 1.51937, acc = 0.79400\n",
      "Training: loss = 1.52783, acc = 0.45714 | Val: loss = 1.51715, acc = 0.79200\n",
      "Training: loss = 1.45376, acc = 0.55714 | Val: loss = 1.51468, acc = 0.79200\n",
      "Training: loss = 1.44433, acc = 0.53571 | Val: loss = 1.51298, acc = 0.79600\n",
      "Training: loss = 1.55229, acc = 0.50714 | Val: loss = 1.51120, acc = 0.79800\n",
      "Training: loss = 1.45601, acc = 0.47857 | Val: loss = 1.50965, acc = 0.80000\n",
      "Training: loss = 1.52036, acc = 0.45000 | Val: loss = 1.50758, acc = 0.80000\n",
      "Training: loss = 1.50709, acc = 0.47857 | Val: loss = 1.50382, acc = 0.80400\n",
      "Training: loss = 1.50642, acc = 0.48571 | Val: loss = 1.49981, acc = 0.80200\n",
      "Training: loss = 1.50668, acc = 0.47143 | Val: loss = 1.49552, acc = 0.80200\n",
      "Training: loss = 1.41598, acc = 0.54286 | Val: loss = 1.49089, acc = 0.80600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: loss = 1.46839, acc = 0.52143 | Val: loss = 1.48619, acc = 0.80600\n",
      "Training: loss = 1.49614, acc = 0.51429 | Val: loss = 1.48107, acc = 0.81000\n",
      "Training: loss = 1.38322, acc = 0.54286 | Val: loss = 1.47725, acc = 0.81400\n",
      "Training: loss = 1.40473, acc = 0.55714 | Val: loss = 1.47202, acc = 0.81200\n",
      "Training: loss = 1.32141, acc = 0.62857 | Val: loss = 1.46644, acc = 0.81600\n",
      "Training: loss = 1.52024, acc = 0.46429 | Val: loss = 1.46202, acc = 0.81000\n",
      "Training: loss = 1.44256, acc = 0.52143 | Val: loss = 1.45937, acc = 0.80400\n",
      "Training: loss = 1.39420, acc = 0.59286 | Val: loss = 1.45762, acc = 0.80800\n",
      "Training: loss = 1.36718, acc = 0.52857 | Val: loss = 1.45546, acc = 0.80400\n",
      "Training: loss = 1.38985, acc = 0.53571 | Val: loss = 1.45263, acc = 0.80200\n",
      "Training: loss = 1.44074, acc = 0.52143 | Val: loss = 1.44992, acc = 0.80200\n",
      "Training: loss = 1.44626, acc = 0.54286 | Val: loss = 1.44693, acc = 0.80200\n",
      "Training: loss = 1.42867, acc = 0.47143 | Val: loss = 1.44433, acc = 0.80600\n",
      "Training: loss = 1.47957, acc = 0.52143 | Val: loss = 1.44242, acc = 0.80200\n",
      "Training: loss = 1.47668, acc = 0.52857 | Val: loss = 1.44239, acc = 0.80000\n",
      "Training: loss = 1.48858, acc = 0.51429 | Val: loss = 1.44172, acc = 0.80400\n",
      "Training: loss = 1.43825, acc = 0.49286 | Val: loss = 1.44073, acc = 0.80200\n",
      "Training: loss = 1.33251, acc = 0.55714 | Val: loss = 1.44153, acc = 0.79400\n",
      "Training: loss = 1.45882, acc = 0.50000 | Val: loss = 1.44047, acc = 0.78800\n",
      "Training: loss = 1.36240, acc = 0.52143 | Val: loss = 1.43839, acc = 0.78400\n",
      "Training: loss = 1.39595, acc = 0.47143 | Val: loss = 1.43580, acc = 0.78600\n",
      "Training: loss = 1.47970, acc = 0.54286 | Val: loss = 1.43197, acc = 0.78200\n",
      "Training: loss = 1.41055, acc = 0.52143 | Val: loss = 1.42911, acc = 0.78000\n",
      "Training: loss = 1.50458, acc = 0.46429 | Val: loss = 1.42566, acc = 0.77600\n",
      "Training: loss = 1.38294, acc = 0.51429 | Val: loss = 1.42047, acc = 0.76800\n",
      "Training: loss = 1.43299, acc = 0.50000 | Val: loss = 1.41463, acc = 0.77400\n",
      "Training: loss = 1.39546, acc = 0.49286 | Val: loss = 1.40841, acc = 0.78000\n",
      "Training: loss = 1.40779, acc = 0.46429 | Val: loss = 1.40233, acc = 0.78000\n",
      "Training: loss = 1.34020, acc = 0.58571 | Val: loss = 1.39697, acc = 0.78200\n",
      "Training: loss = 1.36550, acc = 0.57143 | Val: loss = 1.39147, acc = 0.78200\n",
      "Training: loss = 1.44931, acc = 0.42857 | Val: loss = 1.38598, acc = 0.79400\n",
      "Training: loss = 1.34978, acc = 0.55714 | Val: loss = 1.38126, acc = 0.79800\n",
      "Training: loss = 1.34252, acc = 0.55000 | Val: loss = 1.37621, acc = 0.80800\n",
      "Training: loss = 1.44151, acc = 0.47857 | Val: loss = 1.37226, acc = 0.81000\n",
      "Training: loss = 1.47397, acc = 0.45714 | Val: loss = 1.36890, acc = 0.81600\n",
      "Training: loss = 1.34170, acc = 0.53571 | Val: loss = 1.36561, acc = 0.81800\n",
      "Training: loss = 1.49487, acc = 0.50714 | Val: loss = 1.36172, acc = 0.82000\n",
      "Training: loss = 1.30952, acc = 0.55000 | Val: loss = 1.35782, acc = 0.81200\n",
      "Training: loss = 1.30028, acc = 0.61429 | Val: loss = 1.35433, acc = 0.81400\n",
      "Training: loss = 1.40269, acc = 0.52857 | Val: loss = 1.35272, acc = 0.81800\n",
      "Training: loss = 1.45231, acc = 0.46429 | Val: loss = 1.35329, acc = 0.81000\n",
      "Training: loss = 1.39333, acc = 0.51429 | Val: loss = 1.35344, acc = 0.80800\n",
      "Training: loss = 1.46976, acc = 0.49286 | Val: loss = 1.35385, acc = 0.80800\n",
      "Training: loss = 1.37798, acc = 0.48571 | Val: loss = 1.35389, acc = 0.80800\n",
      "Training: loss = 1.34457, acc = 0.55000 | Val: loss = 1.35255, acc = 0.80600\n",
      "Training: loss = 1.40686, acc = 0.58571 | Val: loss = 1.35185, acc = 0.80600\n",
      "Training: loss = 1.45645, acc = 0.47857 | Val: loss = 1.35127, acc = 0.80200\n",
      "Training: loss = 1.27015, acc = 0.59286 | Val: loss = 1.35125, acc = 0.80000\n",
      "Training: loss = 1.31458, acc = 0.56429 | Val: loss = 1.35131, acc = 0.79200\n",
      "Training: loss = 1.29599, acc = 0.57857 | Val: loss = 1.34953, acc = 0.79000\n",
      "Training: loss = 1.42506, acc = 0.50000 | Val: loss = 1.34782, acc = 0.79000\n",
      "Training: loss = 1.29172, acc = 0.55714 | Val: loss = 1.34506, acc = 0.78400\n",
      "Training: loss = 1.35618, acc = 0.56429 | Val: loss = 1.34056, acc = 0.78800\n",
      "Training: loss = 1.37573, acc = 0.51429 | Val: loss = 1.33615, acc = 0.78600\n",
      "Training: loss = 1.26907, acc = 0.52857 | Val: loss = 1.33202, acc = 0.78600\n",
      "Training: loss = 1.36954, acc = 0.52857 | Val: loss = 1.32769, acc = 0.78600\n",
      "Training: loss = 1.46074, acc = 0.50714 | Val: loss = 1.32458, acc = 0.78400\n",
      "Training: loss = 1.29830, acc = 0.55714 | Val: loss = 1.32024, acc = 0.79000\n",
      "Training: loss = 1.38340, acc = 0.53571 | Val: loss = 1.31642, acc = 0.78800\n",
      "Training: loss = 1.31167, acc = 0.53571 | Val: loss = 1.31326, acc = 0.79200\n",
      "Training: loss = 1.36199, acc = 0.48571 | Val: loss = 1.31128, acc = 0.80200\n",
      "Training: loss = 1.29763, acc = 0.59286 | Val: loss = 1.30811, acc = 0.80600\n",
      "Training: loss = 1.43866, acc = 0.50000 | Val: loss = 1.30420, acc = 0.80600\n",
      "Training: loss = 1.37525, acc = 0.55714 | Val: loss = 1.30046, acc = 0.80200\n",
      "Training: loss = 1.33146, acc = 0.53571 | Val: loss = 1.29794, acc = 0.80800\n",
      "Training: loss = 1.40838, acc = 0.52143 | Val: loss = 1.29620, acc = 0.80200\n",
      "Training: loss = 1.27060, acc = 0.62143 | Val: loss = 1.29418, acc = 0.80600\n",
      "Training: loss = 1.26282, acc = 0.58571 | Val: loss = 1.29132, acc = 0.81200\n",
      "Training: loss = 1.22806, acc = 0.56429 | Val: loss = 1.29132, acc = 0.81200\n",
      "Training: loss = 1.33095, acc = 0.58571 | Val: loss = 1.29005, acc = 0.81000\n",
      "Training: loss = 1.34558, acc = 0.51429 | Val: loss = 1.28994, acc = 0.81000\n",
      "Training: loss = 1.36839, acc = 0.52857 | Val: loss = 1.29026, acc = 0.81000\n",
      "Training: loss = 1.29948, acc = 0.55000 | Val: loss = 1.28949, acc = 0.81200\n",
      "Training: loss = 1.29691, acc = 0.54286 | Val: loss = 1.28867, acc = 0.81200\n",
      "Training: loss = 1.41024, acc = 0.50714 | Val: loss = 1.28702, acc = 0.81600\n",
      "Training: loss = 1.49530, acc = 0.44286 | Val: loss = 1.28670, acc = 0.81200\n",
      "Training: loss = 1.29002, acc = 0.57143 | Val: loss = 1.28616, acc = 0.81000\n",
      "Training: loss = 1.35086, acc = 0.52857 | Val: loss = 1.28566, acc = 0.80800\n",
      "Training: loss = 1.30760, acc = 0.49286 | Val: loss = 1.28504, acc = 0.80800\n",
      "Training: loss = 1.38862, acc = 0.55000 | Val: loss = 1.28482, acc = 0.80600\n",
      "Training: loss = 1.28437, acc = 0.62143 | Val: loss = 1.28518, acc = 0.80000\n",
      "Training: loss = 1.45485, acc = 0.41429 | Val: loss = 1.28659, acc = 0.79000\n",
      "Training: loss = 1.24980, acc = 0.56429 | Val: loss = 1.28699, acc = 0.79000\n",
      "Training: loss = 1.24763, acc = 0.55000 | Val: loss = 1.28633, acc = 0.79200\n",
      "Training: loss = 1.33269, acc = 0.56429 | Val: loss = 1.28475, acc = 0.79600\n",
      "Training: loss = 1.20546, acc = 0.59286 | Val: loss = 1.28270, acc = 0.79800\n",
      "Training: loss = 1.38262, acc = 0.50714 | Val: loss = 1.28115, acc = 0.79400\n",
      "Training: loss = 1.27085, acc = 0.56429 | Val: loss = 1.27763, acc = 0.79800\n",
      "Training: loss = 1.35942, acc = 0.53571 | Val: loss = 1.27339, acc = 0.79800\n",
      "Training: loss = 1.15208, acc = 0.62857 | Val: loss = 1.27077, acc = 0.79400\n",
      "Training: loss = 1.36586, acc = 0.47143 | Val: loss = 1.26798, acc = 0.79800\n",
      "Training: loss = 1.38056, acc = 0.52143 | Val: loss = 1.26718, acc = 0.79800\n",
      "Training: loss = 1.19927, acc = 0.59286 | Val: loss = 1.26346, acc = 0.80200\n",
      "Training: loss = 1.36358, acc = 0.55000 | Val: loss = 1.26049, acc = 0.80600\n",
      "Training: loss = 1.27132, acc = 0.60000 | Val: loss = 1.25679, acc = 0.80800\n",
      "Training: loss = 1.28165, acc = 0.57857 | Val: loss = 1.25166, acc = 0.80600\n",
      "Training: loss = 1.23576, acc = 0.57143 | Val: loss = 1.24673, acc = 0.81000\n",
      "Training: loss = 1.27074, acc = 0.59286 | Val: loss = 1.24260, acc = 0.80600\n",
      "Training: loss = 1.22635, acc = 0.65714 | Val: loss = 1.23821, acc = 0.80600\n",
      "Training: loss = 1.28882, acc = 0.60000 | Val: loss = 1.23486, acc = 0.80600\n",
      "Training: loss = 1.32288, acc = 0.50000 | Val: loss = 1.23050, acc = 0.80200\n",
      "Training: loss = 1.34806, acc = 0.49286 | Val: loss = 1.22702, acc = 0.80600\n",
      "Training: loss = 1.34901, acc = 0.49286 | Val: loss = 1.22438, acc = 0.81000\n",
      "Training: loss = 1.25803, acc = 0.58571 | Val: loss = 1.22261, acc = 0.80800\n",
      "Training: loss = 1.26909, acc = 0.55714 | Val: loss = 1.22196, acc = 0.81000\n",
      "Training: loss = 1.33082, acc = 0.48571 | Val: loss = 1.22172, acc = 0.80800\n",
      "Training: loss = 1.27296, acc = 0.54286 | Val: loss = 1.22042, acc = 0.81000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: loss = 1.31407, acc = 0.50714 | Val: loss = 1.21924, acc = 0.81200\n",
      "Training: loss = 1.19936, acc = 0.55000 | Val: loss = 1.21913, acc = 0.81400\n",
      "Training: loss = 1.20346, acc = 0.57857 | Val: loss = 1.21891, acc = 0.81200\n",
      "Training: loss = 1.29880, acc = 0.55714 | Val: loss = 1.22013, acc = 0.81600\n",
      "Training: loss = 1.33228, acc = 0.54286 | Val: loss = 1.22167, acc = 0.80400\n",
      "Training: loss = 1.28001, acc = 0.55714 | Val: loss = 1.22171, acc = 0.80600\n",
      "Training: loss = 1.24857, acc = 0.58571 | Val: loss = 1.22225, acc = 0.80400\n",
      "Training: loss = 1.35232, acc = 0.48571 | Val: loss = 1.22494, acc = 0.80200\n",
      "Training: loss = 1.43780, acc = 0.46429 | Val: loss = 1.22756, acc = 0.79600\n",
      "Training: loss = 1.27862, acc = 0.52143 | Val: loss = 1.23147, acc = 0.79600\n",
      "Training: loss = 1.46528, acc = 0.44286 | Val: loss = 1.23303, acc = 0.79800\n",
      "Training: loss = 1.28176, acc = 0.54286 | Val: loss = 1.23240, acc = 0.79400\n",
      "Training: loss = 1.24192, acc = 0.55000 | Val: loss = 1.23147, acc = 0.79600\n",
      "Training: loss = 1.27678, acc = 0.53571 | Val: loss = 1.23301, acc = 0.79600\n",
      "Training: loss = 1.31530, acc = 0.51429 | Val: loss = 1.23257, acc = 0.79800\n",
      "Training: loss = 1.34047, acc = 0.51429 | Val: loss = 1.23207, acc = 0.79400\n",
      "Training: loss = 1.34481, acc = 0.53571 | Val: loss = 1.23072, acc = 0.79000\n",
      "Training: loss = 1.27100, acc = 0.55000 | Val: loss = 1.22921, acc = 0.78400\n",
      "Training: loss = 1.29188, acc = 0.54286 | Val: loss = 1.22772, acc = 0.78600\n",
      "Training: loss = 1.15091, acc = 0.64286 | Val: loss = 1.22548, acc = 0.78600\n",
      "Training: loss = 1.35808, acc = 0.49286 | Val: loss = 1.22342, acc = 0.78800\n",
      "Training: loss = 1.34713, acc = 0.51429 | Val: loss = 1.22101, acc = 0.78800\n",
      "Training: loss = 1.29645, acc = 0.49286 | Val: loss = 1.21921, acc = 0.78800\n",
      "Training: loss = 1.18178, acc = 0.59286 | Val: loss = 1.21589, acc = 0.79000\n",
      "Training: loss = 1.17168, acc = 0.58571 | Val: loss = 1.21167, acc = 0.79200\n",
      "Training: loss = 1.28947, acc = 0.51429 | Val: loss = 1.20970, acc = 0.79600\n",
      "Training: loss = 1.21708, acc = 0.57143 | Val: loss = 1.20902, acc = 0.79400\n",
      "Training: loss = 1.38058, acc = 0.53571 | Val: loss = 1.20567, acc = 0.80200\n",
      "Training: loss = 1.34548, acc = 0.51429 | Val: loss = 1.20202, acc = 0.80000\n",
      "Training: loss = 1.23523, acc = 0.56429 | Val: loss = 1.19645, acc = 0.80400\n",
      "Training: loss = 1.33057, acc = 0.52143 | Val: loss = 1.19093, acc = 0.80600\n",
      "Training: loss = 1.21999, acc = 0.54286 | Val: loss = 1.18649, acc = 0.81000\n",
      "Training: loss = 1.24603, acc = 0.55714 | Val: loss = 1.18352, acc = 0.81000\n",
      "Training: loss = 1.17341, acc = 0.57143 | Val: loss = 1.18156, acc = 0.81400\n",
      "Training: loss = 1.42318, acc = 0.47857 | Val: loss = 1.18149, acc = 0.80800\n",
      "Training: loss = 1.31536, acc = 0.51429 | Val: loss = 1.18163, acc = 0.81000\n",
      "Training: loss = 1.29919, acc = 0.52143 | Val: loss = 1.18096, acc = 0.81000\n",
      "Training: loss = 1.37851, acc = 0.50000 | Val: loss = 1.17918, acc = 0.80600\n",
      "Training: loss = 1.23773, acc = 0.54286 | Val: loss = 1.17873, acc = 0.80200\n",
      "Training: loss = 1.33310, acc = 0.47857 | Val: loss = 1.17669, acc = 0.79600\n",
      "Training: loss = 1.26296, acc = 0.57143 | Val: loss = 1.17505, acc = 0.79600\n",
      "Training: loss = 1.29308, acc = 0.51429 | Val: loss = 1.17406, acc = 0.79800\n",
      "Training: loss = 1.24669, acc = 0.52143 | Val: loss = 1.17326, acc = 0.79800\n",
      "Training: loss = 1.25103, acc = 0.56429 | Val: loss = 1.17265, acc = 0.80400\n",
      "Training: loss = 1.32293, acc = 0.52143 | Val: loss = 1.17124, acc = 0.80400\n",
      "Training: loss = 1.38418, acc = 0.47143 | Val: loss = 1.17022, acc = 0.80000\n",
      "Training: loss = 1.20967, acc = 0.58571 | Val: loss = 1.16846, acc = 0.79800\n",
      "Training: loss = 1.23897, acc = 0.55000 | Val: loss = 1.16697, acc = 0.79600\n",
      "Training: loss = 1.22797, acc = 0.57857 | Val: loss = 1.16563, acc = 0.79600\n",
      "Training: loss = 1.15954, acc = 0.59286 | Val: loss = 1.16557, acc = 0.79600\n",
      "Training: loss = 1.16073, acc = 0.60714 | Val: loss = 1.16552, acc = 0.79600\n",
      "Training: loss = 1.26156, acc = 0.57143 | Val: loss = 1.16505, acc = 0.79000\n",
      "Training: loss = 1.29138, acc = 0.55000 | Val: loss = 1.16337, acc = 0.78600\n",
      "Training: loss = 1.18485, acc = 0.61429 | Val: loss = 1.16056, acc = 0.79400\n",
      "Training: loss = 1.28765, acc = 0.52857 | Val: loss = 1.15874, acc = 0.79600\n",
      "Training: loss = 1.24133, acc = 0.55714 | Val: loss = 1.15844, acc = 0.80400\n",
      "Training: loss = 1.19084, acc = 0.58571 | Val: loss = 1.15894, acc = 0.80600\n",
      "Training: loss = 1.26005, acc = 0.50714 | Val: loss = 1.16121, acc = 0.80800\n",
      "Training: loss = 1.23192, acc = 0.60000 | Val: loss = 1.16145, acc = 0.80400\n",
      "Training: loss = 1.26206, acc = 0.57857 | Val: loss = 1.16087, acc = 0.80400\n",
      "Training: loss = 1.28643, acc = 0.55714 | Val: loss = 1.15971, acc = 0.80400\n",
      "Training: loss = 1.26652, acc = 0.52857 | Val: loss = 1.15718, acc = 0.80600\n",
      "Training: loss = 1.23768, acc = 0.57857 | Val: loss = 1.15754, acc = 0.80800\n",
      "Training: loss = 1.20372, acc = 0.57857 | Val: loss = 1.15769, acc = 0.80600\n",
      "Training: loss = 1.14440, acc = 0.62143 | Val: loss = 1.15636, acc = 0.80400\n",
      "Training: loss = 1.23937, acc = 0.53571 | Val: loss = 1.15529, acc = 0.80600\n",
      "Training: loss = 1.26955, acc = 0.47143 | Val: loss = 1.15295, acc = 0.80600\n",
      "Training: loss = 1.31141, acc = 0.55714 | Val: loss = 1.15230, acc = 0.80200\n",
      "Training: loss = 1.31975, acc = 0.48571 | Val: loss = 1.15242, acc = 0.79800\n",
      "Training: loss = 1.35147, acc = 0.45714 | Val: loss = 1.15223, acc = 0.80200\n",
      "Training: loss = 1.33368, acc = 0.52857 | Val: loss = 1.15383, acc = 0.80000\n",
      "Training: loss = 1.21839, acc = 0.52857 | Val: loss = 1.15541, acc = 0.79600\n",
      "Training: loss = 1.18732, acc = 0.55000 | Val: loss = 1.15740, acc = 0.80000\n",
      "Training: loss = 1.24412, acc = 0.58571 | Val: loss = 1.15904, acc = 0.80400\n",
      "Training: loss = 1.25699, acc = 0.51429 | Val: loss = 1.16158, acc = 0.80400\n",
      "Training: loss = 1.30825, acc = 0.52143 | Val: loss = 1.16442, acc = 0.79600\n",
      "Training: loss = 1.26134, acc = 0.54286 | Val: loss = 1.16536, acc = 0.79600\n",
      "Training: loss = 1.29158, acc = 0.50714 | Val: loss = 1.16478, acc = 0.79600\n",
      "Training: loss = 1.18398, acc = 0.55000 | Val: loss = 1.16045, acc = 0.79800\n",
      "Training: loss = 1.20791, acc = 0.56429 | Val: loss = 1.15655, acc = 0.79800\n",
      "Training: loss = 1.35952, acc = 0.43571 | Val: loss = 1.15140, acc = 0.79800\n",
      "Training: loss = 1.36670, acc = 0.47143 | Val: loss = 1.14713, acc = 0.80400\n",
      "Training: loss = 1.35260, acc = 0.52143 | Val: loss = 1.14075, acc = 0.80400\n",
      "Training: loss = 1.22161, acc = 0.60714 | Val: loss = 1.13570, acc = 0.80800\n",
      "Training: loss = 1.19327, acc = 0.54286 | Val: loss = 1.13326, acc = 0.80200\n",
      "Training: loss = 1.31504, acc = 0.55000 | Val: loss = 1.13188, acc = 0.80800\n",
      "Training: loss = 1.29577, acc = 0.52857 | Val: loss = 1.13014, acc = 0.81400\n",
      "Training: loss = 1.28128, acc = 0.57143 | Val: loss = 1.12861, acc = 0.81400\n",
      "Training: loss = 1.33001, acc = 0.50714 | Val: loss = 1.12976, acc = 0.81600\n",
      "Training: loss = 1.29560, acc = 0.52857 | Val: loss = 1.13159, acc = 0.80600\n",
      "Training: loss = 1.37997, acc = 0.55000 | Val: loss = 1.13182, acc = 0.80600\n",
      "Training: loss = 1.18987, acc = 0.57143 | Val: loss = 1.13235, acc = 0.80600\n",
      "Training: loss = 1.38825, acc = 0.50000 | Val: loss = 1.13168, acc = 0.80400\n",
      "Training: loss = 1.19324, acc = 0.54286 | Val: loss = 1.13188, acc = 0.80400\n",
      "Training: loss = 1.16506, acc = 0.58571 | Val: loss = 1.13317, acc = 0.79800\n",
      "Training: loss = 1.27766, acc = 0.53571 | Val: loss = 1.13350, acc = 0.79600\n",
      "Training: loss = 1.25219, acc = 0.55000 | Val: loss = 1.13329, acc = 0.79400\n",
      "Training: loss = 1.15604, acc = 0.55714 | Val: loss = 1.13219, acc = 0.80200\n",
      "Training: loss = 1.22537, acc = 0.56429 | Val: loss = 1.13173, acc = 0.79800\n",
      "Training: loss = 1.32247, acc = 0.53571 | Val: loss = 1.13201, acc = 0.79200\n",
      "Training: loss = 1.21913, acc = 0.55714 | Val: loss = 1.13176, acc = 0.79600\n",
      "Training: loss = 1.17788, acc = 0.55714 | Val: loss = 1.13096, acc = 0.79400\n",
      "Training: loss = 1.23212, acc = 0.53571 | Val: loss = 1.12977, acc = 0.80000\n",
      "Training: loss = 1.28705, acc = 0.55714 | Val: loss = 1.12757, acc = 0.79600\n",
      "Training: loss = 1.26323, acc = 0.50714 | Val: loss = 1.12658, acc = 0.79800\n",
      "Training: loss = 1.26948, acc = 0.52143 | Val: loss = 1.12492, acc = 0.80200\n",
      "Training: loss = 1.23051, acc = 0.53571 | Val: loss = 1.12350, acc = 0.80400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: loss = 1.21564, acc = 0.54286 | Val: loss = 1.12356, acc = 0.80200\n",
      "Training: loss = 1.23828, acc = 0.55000 | Val: loss = 1.12264, acc = 0.80200\n",
      "Training: loss = 1.14277, acc = 0.55714 | Val: loss = 1.12134, acc = 0.80400\n",
      "Training: loss = 1.21835, acc = 0.55000 | Val: loss = 1.11902, acc = 0.80600\n",
      "Training: loss = 1.29255, acc = 0.52857 | Val: loss = 1.11634, acc = 0.80800\n",
      "Training: loss = 1.27084, acc = 0.52857 | Val: loss = 1.11802, acc = 0.80400\n",
      "Training: loss = 1.36480, acc = 0.48571 | Val: loss = 1.11840, acc = 0.80400\n",
      "Training: loss = 1.21132, acc = 0.55714 | Val: loss = 1.11932, acc = 0.80000\n",
      "Training: loss = 1.19255, acc = 0.60714 | Val: loss = 1.12048, acc = 0.80200\n",
      "Training: loss = 1.22236, acc = 0.53571 | Val: loss = 1.12400, acc = 0.80200\n",
      "Training: loss = 1.15010, acc = 0.62857 | Val: loss = 1.12796, acc = 0.79400\n",
      "Training: loss = 1.26480, acc = 0.55000 | Val: loss = 1.13271, acc = 0.78800\n",
      "Training: loss = 1.18064, acc = 0.57857 | Val: loss = 1.13604, acc = 0.78800\n",
      "Training: loss = 1.22898, acc = 0.55714 | Val: loss = 1.13850, acc = 0.78400\n",
      "Training: loss = 1.24767, acc = 0.54286 | Val: loss = 1.13942, acc = 0.78000\n",
      "Training: loss = 1.16741, acc = 0.57857 | Val: loss = 1.13876, acc = 0.78000\n",
      "Training: loss = 1.17027, acc = 0.58571 | Val: loss = 1.13838, acc = 0.77600\n",
      "Training: loss = 1.31529, acc = 0.52857 | Val: loss = 1.13738, acc = 0.77600\n",
      "Training: loss = 1.26886, acc = 0.60714 | Val: loss = 1.13463, acc = 0.78200\n",
      "Training: loss = 1.25501, acc = 0.57143 | Val: loss = 1.13150, acc = 0.78400\n",
      "Training: loss = 1.26751, acc = 0.55714 | Val: loss = 1.12765, acc = 0.79200\n",
      "Training: loss = 1.13731, acc = 0.58571 | Val: loss = 1.12427, acc = 0.80000\n",
      "Training: loss = 1.20556, acc = 0.57143 | Val: loss = 1.12109, acc = 0.80000\n",
      "Training: loss = 1.13186, acc = 0.62143 | Val: loss = 1.11646, acc = 0.80200\n",
      "Training: loss = 1.26718, acc = 0.52143 | Val: loss = 1.11324, acc = 0.80000\n",
      "Training: loss = 1.16502, acc = 0.57857 | Val: loss = 1.11091, acc = 0.80000\n",
      "Training: loss = 1.23822, acc = 0.55000 | Val: loss = 1.10877, acc = 0.79800\n",
      "Training: loss = 1.22585, acc = 0.53571 | Val: loss = 1.10723, acc = 0.80200\n",
      "Training: loss = 1.31397, acc = 0.46429 | Val: loss = 1.10968, acc = 0.80400\n",
      "Training: loss = 1.19411, acc = 0.62143 | Val: loss = 1.11280, acc = 0.80200\n",
      "Training: loss = 1.33008, acc = 0.56429 | Val: loss = 1.11723, acc = 0.80600\n",
      "Training: loss = 1.24702, acc = 0.52143 | Val: loss = 1.12338, acc = 0.80800\n",
      "Training: loss = 1.15612, acc = 0.55000 | Val: loss = 1.12933, acc = 0.80600\n",
      "Training: loss = 1.16845, acc = 0.58571 | Val: loss = 1.13354, acc = 0.80000\n",
      "Training: loss = 1.14624, acc = 0.59286 | Val: loss = 1.13678, acc = 0.79200\n",
      "Training: loss = 1.19415, acc = 0.58571 | Val: loss = 1.13665, acc = 0.78800\n",
      "Training: loss = 1.17323, acc = 0.54286 | Val: loss = 1.13510, acc = 0.78800\n",
      "Training: loss = 1.20155, acc = 0.56429 | Val: loss = 1.13315, acc = 0.78400\n",
      "Training: loss = 1.25401, acc = 0.56429 | Val: loss = 1.13148, acc = 0.77600\n",
      "Training: loss = 1.20173, acc = 0.62143 | Val: loss = 1.12965, acc = 0.77600\n",
      "Training: loss = 1.25257, acc = 0.59286 | Val: loss = 1.12398, acc = 0.78000\n",
      "Training: loss = 1.23434, acc = 0.51429 | Val: loss = 1.11741, acc = 0.78200\n",
      "Training: loss = 1.20578, acc = 0.56429 | Val: loss = 1.11047, acc = 0.79000\n",
      "Training: loss = 1.31592, acc = 0.50714 | Val: loss = 1.10431, acc = 0.79200\n",
      "Training: loss = 1.29340, acc = 0.54286 | Val: loss = 1.09915, acc = 0.79600\n",
      "Training: loss = 1.21603, acc = 0.53571 | Val: loss = 1.09561, acc = 0.80200\n",
      "Training: loss = 1.10815, acc = 0.57857 | Val: loss = 1.09171, acc = 0.80200\n",
      "Training: loss = 1.23563, acc = 0.53571 | Val: loss = 1.08843, acc = 0.80400\n",
      "Training: loss = 1.21096, acc = 0.57857 | Val: loss = 1.08562, acc = 0.80600\n",
      "Training: loss = 1.21317, acc = 0.57857 | Val: loss = 1.08340, acc = 0.81200\n",
      "Training: loss = 1.28224, acc = 0.48571 | Val: loss = 1.08205, acc = 0.81400\n",
      "Training: loss = 1.23544, acc = 0.52143 | Val: loss = 1.08295, acc = 0.81400\n",
      "Training: loss = 1.22744, acc = 0.52143 | Val: loss = 1.08692, acc = 0.80400\n",
      "Training: loss = 1.24819, acc = 0.52143 | Val: loss = 1.09196, acc = 0.80600\n",
      "Training: loss = 1.30508, acc = 0.52857 | Val: loss = 1.09757, acc = 0.80200\n",
      "Training: loss = 1.33172, acc = 0.52857 | Val: loss = 1.10162, acc = 0.80200\n",
      "Training: loss = 1.19408, acc = 0.57857 | Val: loss = 1.10525, acc = 0.80400\n",
      "Training: loss = 1.20221, acc = 0.55000 | Val: loss = 1.10957, acc = 0.80000\n",
      "Training: loss = 1.29037, acc = 0.54286 | Val: loss = 1.11382, acc = 0.79400\n",
      "Training: loss = 1.12520, acc = 0.60714 | Val: loss = 1.11774, acc = 0.79000\n",
      "Training: loss = 1.18776, acc = 0.58571 | Val: loss = 1.11921, acc = 0.79000\n",
      "Training: loss = 1.28062, acc = 0.53571 | Val: loss = 1.11733, acc = 0.79400\n",
      "Training: loss = 1.23879, acc = 0.58571 | Val: loss = 1.11457, acc = 0.79600\n",
      "Training: loss = 1.23064, acc = 0.52857 | Val: loss = 1.11095, acc = 0.80000\n",
      "Training: loss = 1.37071, acc = 0.43571 | Val: loss = 1.10592, acc = 0.79800\n",
      "Training: loss = 1.24185, acc = 0.55000 | Val: loss = 1.10210, acc = 0.79600\n",
      "Training: loss = 1.14882, acc = 0.60714 | Val: loss = 1.10034, acc = 0.79600\n",
      "Training: loss = 1.23888, acc = 0.51429 | Val: loss = 1.09979, acc = 0.79800\n",
      "Training: loss = 1.13971, acc = 0.59286 | Val: loss = 1.09777, acc = 0.79800\n",
      "Training: loss = 1.15978, acc = 0.59286 | Val: loss = 1.09822, acc = 0.79600\n",
      "Training: loss = 1.22911, acc = 0.52143 | Val: loss = 1.09716, acc = 0.80000\n",
      "Training: loss = 1.06393, acc = 0.58571 | Val: loss = 1.09588, acc = 0.80000\n",
      "Training: loss = 1.25006, acc = 0.55714 | Val: loss = 1.09573, acc = 0.79400\n",
      "Training: loss = 1.13784, acc = 0.56429 | Val: loss = 1.09533, acc = 0.79600\n",
      "Training: loss = 1.33907, acc = 0.53571 | Val: loss = 1.09547, acc = 0.80200\n",
      "Training: loss = 1.18273, acc = 0.54286 | Val: loss = 1.09384, acc = 0.80200\n",
      "Training: loss = 1.19906, acc = 0.52857 | Val: loss = 1.09233, acc = 0.80000\n",
      "Training: loss = 1.22441, acc = 0.55000 | Val: loss = 1.09274, acc = 0.80200\n",
      "Training: loss = 1.28302, acc = 0.53571 | Val: loss = 1.09408, acc = 0.80200\n",
      "Training: loss = 1.16157, acc = 0.53571 | Val: loss = 1.09470, acc = 0.80000\n",
      "Training: loss = 1.23731, acc = 0.59286 | Val: loss = 1.09514, acc = 0.80400\n",
      "Training: loss = 1.24216, acc = 0.54286 | Val: loss = 1.09455, acc = 0.80400\n",
      "Training: loss = 1.33332, acc = 0.50714 | Val: loss = 1.09145, acc = 0.80600\n",
      "Training: loss = 1.27280, acc = 0.52857 | Val: loss = 1.08725, acc = 0.80400\n",
      "Training: loss = 1.12997, acc = 0.57857 | Val: loss = 1.08557, acc = 0.80800\n",
      "Training: loss = 1.25131, acc = 0.52143 | Val: loss = 1.08633, acc = 0.81000\n",
      "Training: loss = 1.09247, acc = 0.60000 | Val: loss = 1.08760, acc = 0.81200\n",
      "Training: loss = 1.32780, acc = 0.51429 | Val: loss = 1.08934, acc = 0.81200\n",
      "Training: loss = 1.21526, acc = 0.52143 | Val: loss = 1.08962, acc = 0.80800\n",
      "Training: loss = 1.29026, acc = 0.52143 | Val: loss = 1.09039, acc = 0.80600\n",
      "Training: loss = 1.16397, acc = 0.57857 | Val: loss = 1.08923, acc = 0.80600\n",
      "Training: loss = 1.19147, acc = 0.55714 | Val: loss = 1.08830, acc = 0.80600\n",
      "Training: loss = 1.32178, acc = 0.51429 | Val: loss = 1.08655, acc = 0.80400\n",
      "Training: loss = 1.14118, acc = 0.57857 | Val: loss = 1.08497, acc = 0.80600\n",
      "Training: loss = 1.18551, acc = 0.56429 | Val: loss = 1.08321, acc = 0.81200\n",
      "Training: loss = 1.16711, acc = 0.62143 | Val: loss = 1.08151, acc = 0.80600\n",
      "Training: loss = 1.30722, acc = 0.50714 | Val: loss = 1.08022, acc = 0.80600\n",
      "Training: loss = 1.11582, acc = 0.59286 | Val: loss = 1.08145, acc = 0.80200\n",
      "Training: loss = 1.12624, acc = 0.60000 | Val: loss = 1.08057, acc = 0.80400\n",
      "Training: loss = 1.24904, acc = 0.52857 | Val: loss = 1.08158, acc = 0.80200\n",
      "Training: loss = 1.19474, acc = 0.55714 | Val: loss = 1.08184, acc = 0.80200\n",
      "Training: loss = 1.22424, acc = 0.55714 | Val: loss = 1.08291, acc = 0.80600\n",
      "Training: loss = 1.36769, acc = 0.47857 | Val: loss = 1.08296, acc = 0.80200\n",
      "Training: loss = 1.14380, acc = 0.60714 | Val: loss = 1.08330, acc = 0.80400\n",
      "Training: loss = 1.14256, acc = 0.60000 | Val: loss = 1.08326, acc = 0.80000\n",
      "Training: loss = 1.27130, acc = 0.49286 | Val: loss = 1.08231, acc = 0.79800\n",
      "Training: loss = 1.06362, acc = 0.58571 | Val: loss = 1.08193, acc = 0.79800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: loss = 1.22162, acc = 0.55000 | Val: loss = 1.08254, acc = 0.80000\n",
      "Training: loss = 1.28676, acc = 0.52143 | Val: loss = 1.08494, acc = 0.80400\n",
      "Training: loss = 1.04339, acc = 0.67143 | Val: loss = 1.08585, acc = 0.80400\n",
      "Training: loss = 1.18767, acc = 0.55714 | Val: loss = 1.08603, acc = 0.80800\n",
      "Training: loss = 1.35953, acc = 0.50714 | Val: loss = 1.08945, acc = 0.80600\n",
      "Training: loss = 1.24716, acc = 0.56429 | Val: loss = 1.09334, acc = 0.80000\n",
      "Training: loss = 1.14009, acc = 0.59286 | Val: loss = 1.09667, acc = 0.79400\n",
      "Training: loss = 1.21644, acc = 0.57143 | Val: loss = 1.09914, acc = 0.79400\n",
      "Training: loss = 1.17334, acc = 0.61429 | Val: loss = 1.09988, acc = 0.79400\n",
      "Training: loss = 1.31094, acc = 0.56429 | Val: loss = 1.10176, acc = 0.79000\n",
      "Training: loss = 1.24989, acc = 0.52143 | Val: loss = 1.10455, acc = 0.79200\n",
      "Training: loss = 1.21229, acc = 0.57143 | Val: loss = 1.10432, acc = 0.79800\n",
      "Training: loss = 1.38421, acc = 0.51429 | Val: loss = 1.10223, acc = 0.80000\n",
      "Training: loss = 1.26819, acc = 0.51429 | Val: loss = 1.09966, acc = 0.80200\n",
      "Training: loss = 1.09687, acc = 0.63571 | Val: loss = 1.09814, acc = 0.80600\n",
      "Training: loss = 1.14096, acc = 0.57143 | Val: loss = 1.09580, acc = 0.80600\n",
      "Training: loss = 1.31112, acc = 0.50000 | Val: loss = 1.09447, acc = 0.80600\n",
      "Training: loss = 1.16384, acc = 0.57857 | Val: loss = 1.09216, acc = 0.80800\n",
      "Training: loss = 1.18942, acc = 0.54286 | Val: loss = 1.08982, acc = 0.80800\n",
      "Training: loss = 1.38017, acc = 0.45714 | Val: loss = 1.08728, acc = 0.80800\n",
      "Training: loss = 1.11663, acc = 0.60714 | Val: loss = 1.08494, acc = 0.80600\n",
      "Training: loss = 1.25965, acc = 0.50000 | Val: loss = 1.08187, acc = 0.81000\n",
      "Training: loss = 1.16950, acc = 0.57857 | Val: loss = 1.07825, acc = 0.80800\n",
      "Training: loss = 1.31829, acc = 0.47857 | Val: loss = 1.07553, acc = 0.81000\n",
      "Training: loss = 1.27933, acc = 0.48571 | Val: loss = 1.07227, acc = 0.81200\n",
      "Training: loss = 1.23812, acc = 0.54286 | Val: loss = 1.07021, acc = 0.81000\n",
      "Training: loss = 1.21902, acc = 0.52143 | Val: loss = 1.06766, acc = 0.81000\n",
      "Training: loss = 1.20551, acc = 0.53571 | Val: loss = 1.06559, acc = 0.81200\n",
      "Training: loss = 1.20361, acc = 0.55714 | Val: loss = 1.06462, acc = 0.81200\n",
      "Training: loss = 1.02339, acc = 0.64286 | Val: loss = 1.06407, acc = 0.80400\n",
      "Training: loss = 1.05970, acc = 0.61429 | Val: loss = 1.06362, acc = 0.80400\n",
      "Training: loss = 1.12983, acc = 0.60000 | Val: loss = 1.06282, acc = 0.80600\n",
      "Training: loss = 1.15638, acc = 0.58571 | Val: loss = 1.06346, acc = 0.80600\n",
      "Training: loss = 1.24370, acc = 0.54286 | Val: loss = 1.06520, acc = 0.80600\n",
      "Training: loss = 1.16316, acc = 0.57857 | Val: loss = 1.06752, acc = 0.80400\n",
      "Training: loss = 1.18066, acc = 0.59286 | Val: loss = 1.06747, acc = 0.80400\n",
      "Training: loss = 1.12853, acc = 0.56429 | Val: loss = 1.06767, acc = 0.80800\n",
      "Training: loss = 1.05239, acc = 0.62857 | Val: loss = 1.06640, acc = 0.80600\n",
      "Training: loss = 1.18875, acc = 0.54286 | Val: loss = 1.06383, acc = 0.80600\n",
      "Training: loss = 1.19842, acc = 0.55714 | Val: loss = 1.06269, acc = 0.80400\n",
      "Training: loss = 1.11816, acc = 0.62143 | Val: loss = 1.06162, acc = 0.80000\n",
      "Training: loss = 1.18984, acc = 0.54286 | Val: loss = 1.06265, acc = 0.79800\n",
      "Training: loss = 1.19538, acc = 0.57857 | Val: loss = 1.06206, acc = 0.79800\n",
      "Training: loss = 1.23961, acc = 0.57143 | Val: loss = 1.06399, acc = 0.80200\n",
      "Training: loss = 1.26117, acc = 0.55000 | Val: loss = 1.06493, acc = 0.80600\n",
      "Training: loss = 1.08772, acc = 0.63571 | Val: loss = 1.06436, acc = 0.81000\n",
      "Training: loss = 1.05188, acc = 0.59286 | Val: loss = 1.06305, acc = 0.80600\n",
      "Training: loss = 1.16507, acc = 0.56429 | Val: loss = 1.06194, acc = 0.80600\n",
      "Training: loss = 1.25008, acc = 0.51429 | Val: loss = 1.06132, acc = 0.80600\n",
      "Training: loss = 1.14155, acc = 0.57143 | Val: loss = 1.06246, acc = 0.80200\n",
      "Training: loss = 1.19013, acc = 0.56429 | Val: loss = 1.06227, acc = 0.79800\n",
      "Training: loss = 1.13316, acc = 0.60000 | Val: loss = 1.06050, acc = 0.80200\n",
      "Training: loss = 1.16354, acc = 0.58571 | Val: loss = 1.05804, acc = 0.80800\n",
      "Training: loss = 1.20396, acc = 0.52857 | Val: loss = 1.05611, acc = 0.80400\n",
      "Training: loss = 1.15932, acc = 0.57143 | Val: loss = 1.05422, acc = 0.80600\n",
      "Training: loss = 1.20783, acc = 0.53571 | Val: loss = 1.05265, acc = 0.80400\n",
      "Training: loss = 1.19449, acc = 0.54286 | Val: loss = 1.05275, acc = 0.80200\n",
      "Training: loss = 1.26054, acc = 0.51429 | Val: loss = 1.05596, acc = 0.80200\n",
      "Training: loss = 1.19870, acc = 0.54286 | Val: loss = 1.05796, acc = 0.79400\n",
      "Training: loss = 1.23324, acc = 0.52857 | Val: loss = 1.05922, acc = 0.79400\n",
      "Training: loss = 1.10420, acc = 0.60000 | Val: loss = 1.06018, acc = 0.79000\n",
      "Training: loss = 1.16975, acc = 0.52857 | Val: loss = 1.06105, acc = 0.79000\n",
      "Training: loss = 1.27169, acc = 0.51429 | Val: loss = 1.06131, acc = 0.79200\n",
      "Training: loss = 1.34542, acc = 0.50000 | Val: loss = 1.06066, acc = 0.78600\n",
      "Training: loss = 1.19378, acc = 0.54286 | Val: loss = 1.05903, acc = 0.78800\n",
      "Training: loss = 1.26729, acc = 0.52143 | Val: loss = 1.05777, acc = 0.78800\n",
      "Training: loss = 1.08836, acc = 0.59286 | Val: loss = 1.05505, acc = 0.78800\n",
      "Training: loss = 1.22920, acc = 0.51429 | Val: loss = 1.05550, acc = 0.79200\n",
      "Training: loss = 1.15366, acc = 0.57143 | Val: loss = 1.05588, acc = 0.79400\n",
      "Training: loss = 1.13583, acc = 0.62143 | Val: loss = 1.05833, acc = 0.79400\n",
      "Training: loss = 1.27053, acc = 0.57143 | Val: loss = 1.06115, acc = 0.79000\n",
      "Training: loss = 1.16559, acc = 0.53571 | Val: loss = 1.06413, acc = 0.79200\n",
      "Training: loss = 1.15621, acc = 0.57143 | Val: loss = 1.06579, acc = 0.79600\n",
      "Training: loss = 1.13817, acc = 0.58571 | Val: loss = 1.06626, acc = 0.79600\n",
      "Training: loss = 1.07141, acc = 0.58571 | Val: loss = 1.06439, acc = 0.80200\n",
      "Training: loss = 1.26635, acc = 0.52143 | Val: loss = 1.06106, acc = 0.80000\n",
      "Training: loss = 1.32245, acc = 0.52143 | Val: loss = 1.05747, acc = 0.80000\n",
      "Training: loss = 1.24574, acc = 0.45000 | Val: loss = 1.05438, acc = 0.80600\n",
      "Training: loss = 1.15187, acc = 0.55000 | Val: loss = 1.05028, acc = 0.79800\n",
      "Training: loss = 1.01611, acc = 0.66429 | Val: loss = 1.04465, acc = 0.80400\n",
      "Training: loss = 1.06348, acc = 0.65000 | Val: loss = 1.03994, acc = 0.80400\n",
      "Training: loss = 1.18123, acc = 0.60000 | Val: loss = 1.03609, acc = 0.80600\n",
      "Training: loss = 1.18295, acc = 0.57143 | Val: loss = 1.03441, acc = 0.81200\n",
      "Training: loss = 1.16073, acc = 0.58571 | Val: loss = 1.03356, acc = 0.81200\n",
      "Training: loss = 1.09237, acc = 0.59286 | Val: loss = 1.03434, acc = 0.81000\n",
      "Training: loss = 1.27928, acc = 0.52857 | Val: loss = 1.03576, acc = 0.80200\n",
      "Training: loss = 1.04409, acc = 0.63571 | Val: loss = 1.03674, acc = 0.80200\n",
      "Training: loss = 1.08990, acc = 0.62857 | Val: loss = 1.03732, acc = 0.79400\n",
      "Training: loss = 1.28635, acc = 0.58571 | Val: loss = 1.03736, acc = 0.79600\n",
      "Training: loss = 1.17558, acc = 0.54286 | Val: loss = 1.03967, acc = 0.79400\n",
      "Training: loss = 1.21560, acc = 0.59286 | Val: loss = 1.04045, acc = 0.79000\n",
      "Training: loss = 1.28227, acc = 0.53571 | Val: loss = 1.04187, acc = 0.78800\n",
      "Training: loss = 1.33521, acc = 0.50000 | Val: loss = 1.04526, acc = 0.78600\n",
      "Training: loss = 1.12366, acc = 0.61429 | Val: loss = 1.04803, acc = 0.78400\n",
      "Training: loss = 1.23556, acc = 0.54286 | Val: loss = 1.05221, acc = 0.78600\n",
      "Training: loss = 1.12771, acc = 0.53571 | Val: loss = 1.05683, acc = 0.78400\n",
      "Training: loss = 1.18838, acc = 0.53571 | Val: loss = 1.05758, acc = 0.78600\n",
      "Training: loss = 1.16762, acc = 0.55714 | Val: loss = 1.05357, acc = 0.78600\n",
      "Training: loss = 1.12056, acc = 0.55714 | Val: loss = 1.04919, acc = 0.78800\n",
      "Training: loss = 1.13044, acc = 0.62143 | Val: loss = 1.04503, acc = 0.79400\n",
      "Training: loss = 1.20989, acc = 0.58571 | Val: loss = 1.03993, acc = 0.79600\n",
      "Training: loss = 1.08777, acc = 0.58571 | Val: loss = 1.03586, acc = 0.80200\n",
      "Training: loss = 1.09466, acc = 0.63571 | Val: loss = 1.03243, acc = 0.80600\n",
      "Training: loss = 1.12659, acc = 0.61429 | Val: loss = 1.03065, acc = 0.80800\n",
      "Training: loss = 1.18927, acc = 0.52143 | Val: loss = 1.03039, acc = 0.80600\n",
      "Training: loss = 1.34169, acc = 0.47857 | Val: loss = 1.03007, acc = 0.81000\n",
      "Training: loss = 1.16722, acc = 0.55714 | Val: loss = 1.03230, acc = 0.80800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: loss = 1.29987, acc = 0.50000 | Val: loss = 1.03406, acc = 0.80800\n",
      "Training: loss = 1.08674, acc = 0.60000 | Val: loss = 1.03736, acc = 0.80800\n",
      "Training: loss = 1.20370, acc = 0.54286 | Val: loss = 1.03872, acc = 0.81000\n",
      "Training: loss = 1.20176, acc = 0.57143 | Val: loss = 1.03769, acc = 0.81000\n",
      "Training: loss = 1.11036, acc = 0.57857 | Val: loss = 1.03768, acc = 0.81000\n",
      "Training: loss = 1.13826, acc = 0.55714 | Val: loss = 1.03758, acc = 0.80800\n",
      "Training: loss = 1.17341, acc = 0.54286 | Val: loss = 1.03675, acc = 0.80600\n",
      "Training: loss = 1.18196, acc = 0.55000 | Val: loss = 1.03536, acc = 0.80400\n",
      "Training: loss = 1.15585, acc = 0.56429 | Val: loss = 1.03560, acc = 0.80400\n",
      "Training: loss = 1.19839, acc = 0.59286 | Val: loss = 1.03190, acc = 0.81000\n",
      "Training: loss = 1.24076, acc = 0.52857 | Val: loss = 1.03010, acc = 0.80800\n",
      "Training: loss = 1.16970, acc = 0.60000 | Val: loss = 1.02873, acc = 0.79600\n",
      "Training: loss = 1.10229, acc = 0.57857 | Val: loss = 1.02877, acc = 0.79600\n",
      "Training: loss = 1.12700, acc = 0.60000 | Val: loss = 1.02973, acc = 0.79600\n",
      "Training: loss = 1.13199, acc = 0.60714 | Val: loss = 1.03034, acc = 0.79200\n",
      "Training: loss = 1.21121, acc = 0.50000 | Val: loss = 1.03195, acc = 0.79200\n",
      "Training: loss = 1.09420, acc = 0.60000 | Val: loss = 1.03254, acc = 0.79200\n",
      "Training: loss = 1.15130, acc = 0.60000 | Val: loss = 1.03166, acc = 0.79200\n",
      "Training: loss = 1.23040, acc = 0.52143 | Val: loss = 1.03313, acc = 0.79400\n",
      "Training: loss = 1.12539, acc = 0.58571 | Val: loss = 1.03419, acc = 0.79200\n",
      "Training: loss = 1.14760, acc = 0.60714 | Val: loss = 1.03551, acc = 0.79000\n",
      "Training: loss = 1.23570, acc = 0.55000 | Val: loss = 1.03501, acc = 0.78800\n",
      "Training: loss = 1.16694, acc = 0.53571 | Val: loss = 1.03448, acc = 0.79400\n",
      "Training: loss = 1.16471, acc = 0.56429 | Val: loss = 1.03426, acc = 0.79800\n",
      "Training: loss = 1.22129, acc = 0.57857 | Val: loss = 1.03631, acc = 0.79800\n",
      "Training: loss = 1.20047, acc = 0.57857 | Val: loss = 1.03956, acc = 0.79600\n",
      "Training: loss = 1.23071, acc = 0.55000 | Val: loss = 1.04224, acc = 0.79600\n",
      "Training: loss = 1.12722, acc = 0.57857 | Val: loss = 1.04618, acc = 0.79200\n",
      "Training: loss = 1.19374, acc = 0.54286 | Val: loss = 1.05030, acc = 0.79400\n",
      "Training: loss = 1.22595, acc = 0.52143 | Val: loss = 1.05511, acc = 0.79200\n",
      "Training: loss = 1.15809, acc = 0.60000 | Val: loss = 1.05670, acc = 0.79400\n",
      "Training: loss = 1.24438, acc = 0.55714 | Val: loss = 1.05566, acc = 0.79400\n",
      "Training: loss = 1.17066, acc = 0.57143 | Val: loss = 1.05246, acc = 0.79800\n",
      "Training: loss = 1.03767, acc = 0.61429 | Val: loss = 1.04648, acc = 0.80200\n",
      "Training: loss = 1.21753, acc = 0.52857 | Val: loss = 1.04472, acc = 0.80000\n",
      "Training: loss = 1.26351, acc = 0.50000 | Val: loss = 1.04363, acc = 0.80600\n",
      "Training: loss = 1.20640, acc = 0.50714 | Val: loss = 1.04005, acc = 0.80800\n",
      "Training: loss = 1.16535, acc = 0.59286 | Val: loss = 1.03671, acc = 0.81000\n",
      "Training: loss = 1.12794, acc = 0.53571 | Val: loss = 1.03233, acc = 0.81000\n",
      "Training: loss = 1.14116, acc = 0.62143 | Val: loss = 1.02915, acc = 0.81400\n",
      "Training: loss = 1.16098, acc = 0.53571 | Val: loss = 1.02600, acc = 0.81000\n",
      "Training: loss = 1.26544, acc = 0.55000 | Val: loss = 1.02340, acc = 0.81400\n",
      "Training: loss = 1.20431, acc = 0.55000 | Val: loss = 1.02096, acc = 0.81200\n",
      "Training: loss = 1.16341, acc = 0.54286 | Val: loss = 1.02172, acc = 0.81200\n",
      "Training: loss = 1.28713, acc = 0.52143 | Val: loss = 1.02146, acc = 0.81200\n",
      "Training: loss = 1.13537, acc = 0.60714 | Val: loss = 1.02372, acc = 0.80400\n",
      "Training: loss = 1.16229, acc = 0.55000 | Val: loss = 1.02357, acc = 0.79800\n",
      "Training: loss = 1.16398, acc = 0.59286 | Val: loss = 1.02516, acc = 0.79800\n",
      "Training: loss = 1.06724, acc = 0.64286 | Val: loss = 1.02765, acc = 0.79800\n",
      "Training: loss = 1.14336, acc = 0.60714 | Val: loss = 1.03103, acc = 0.79600\n",
      "Training: loss = 1.18483, acc = 0.52143 | Val: loss = 1.03538, acc = 0.79400\n",
      "Training: loss = 1.19160, acc = 0.60714 | Val: loss = 1.03980, acc = 0.79400\n",
      "Training: loss = 1.20830, acc = 0.58571 | Val: loss = 1.04226, acc = 0.78800\n",
      "Training: loss = 1.16451, acc = 0.55714 | Val: loss = 1.04555, acc = 0.79000\n",
      "Training: loss = 1.26454, acc = 0.55714 | Val: loss = 1.04932, acc = 0.79200\n",
      "Training: loss = 1.29924, acc = 0.52143 | Val: loss = 1.05034, acc = 0.78600\n",
      "Training: loss = 1.13232, acc = 0.58571 | Val: loss = 1.05206, acc = 0.78400\n",
      "Training: loss = 1.15070, acc = 0.59286 | Val: loss = 1.05313, acc = 0.78600\n",
      "Training: loss = 1.10775, acc = 0.57857 | Val: loss = 1.05250, acc = 0.78800\n",
      "Training: loss = 1.27410, acc = 0.51429 | Val: loss = 1.05135, acc = 0.79200\n",
      "Training: loss = 1.15904, acc = 0.57143 | Val: loss = 1.04949, acc = 0.79200\n",
      "Training: loss = 1.05112, acc = 0.65714 | Val: loss = 1.04921, acc = 0.79400\n",
      "Training: loss = 1.15869, acc = 0.57857 | Val: loss = 1.04895, acc = 0.79800\n",
      "Training: loss = 1.07769, acc = 0.58571 | Val: loss = 1.04818, acc = 0.80000\n",
      "Training: loss = 1.07028, acc = 0.52857 | Val: loss = 1.04823, acc = 0.80000\n",
      "Training: loss = 1.34384, acc = 0.49286 | Val: loss = 1.04807, acc = 0.80000\n",
      "Training: loss = 1.18210, acc = 0.58571 | Val: loss = 1.04941, acc = 0.79800\n",
      "Training: loss = 1.23056, acc = 0.52857 | Val: loss = 1.05191, acc = 0.80000\n",
      "Training: loss = 1.22904, acc = 0.55714 | Val: loss = 1.05575, acc = 0.79400\n",
      "Training: loss = 1.18611, acc = 0.59286 | Val: loss = 1.05759, acc = 0.79600\n",
      "Training: loss = 1.10892, acc = 0.62143 | Val: loss = 1.05803, acc = 0.79600\n",
      "Training: loss = 1.16802, acc = 0.57857 | Val: loss = 1.05722, acc = 0.79600\n",
      "Training: loss = 1.06086, acc = 0.65000 | Val: loss = 1.05704, acc = 0.79800\n",
      "Training: loss = 1.12063, acc = 0.60714 | Val: loss = 1.05659, acc = 0.79000\n",
      "Training: loss = 1.18910, acc = 0.54286 | Val: loss = 1.05338, acc = 0.79000\n",
      "Training: loss = 1.18727, acc = 0.56429 | Val: loss = 1.05064, acc = 0.78800\n",
      "Training: loss = 1.15381, acc = 0.55000 | Val: loss = 1.04790, acc = 0.78800\n",
      "Training: loss = 1.15331, acc = 0.58571 | Val: loss = 1.04574, acc = 0.79000\n",
      "Training: loss = 1.16301, acc = 0.55000 | Val: loss = 1.04244, acc = 0.79200\n",
      "Training: loss = 1.10679, acc = 0.62143 | Val: loss = 1.04025, acc = 0.79200\n",
      "Training: loss = 1.05961, acc = 0.62143 | Val: loss = 1.03767, acc = 0.78800\n",
      "Training: loss = 1.06780, acc = 0.61429 | Val: loss = 1.03428, acc = 0.79000\n",
      "Training: loss = 1.14825, acc = 0.59286 | Val: loss = 1.03214, acc = 0.79200\n",
      "Training: loss = 1.12203, acc = 0.61429 | Val: loss = 1.02992, acc = 0.79800\n",
      "Training: loss = 1.21552, acc = 0.56429 | Val: loss = 1.02885, acc = 0.80000\n",
      "Training: loss = 1.19552, acc = 0.57857 | Val: loss = 1.02770, acc = 0.80000\n",
      "Training: loss = 1.13418, acc = 0.58571 | Val: loss = 1.02761, acc = 0.80000\n",
      "Training: loss = 1.10890, acc = 0.54286 | Val: loss = 1.02711, acc = 0.80000\n",
      "Training: loss = 1.23438, acc = 0.57857 | Val: loss = 1.02652, acc = 0.80200\n",
      "Training: loss = 1.19032, acc = 0.55714 | Val: loss = 1.02695, acc = 0.80000\n",
      "Training: loss = 1.23516, acc = 0.50714 | Val: loss = 1.02743, acc = 0.80200\n",
      "Training: loss = 1.33926, acc = 0.52857 | Val: loss = 1.02815, acc = 0.80000\n",
      "Training: loss = 1.23028, acc = 0.55714 | Val: loss = 1.03098, acc = 0.80000\n",
      "Training: loss = 1.22733, acc = 0.51429 | Val: loss = 1.03537, acc = 0.80200\n",
      "Training: loss = 1.21772, acc = 0.51429 | Val: loss = 1.03954, acc = 0.80000\n",
      "Training: loss = 1.25047, acc = 0.57857 | Val: loss = 1.04440, acc = 0.80000\n",
      "Training: loss = 1.24578, acc = 0.52857 | Val: loss = 1.04868, acc = 0.79400\n",
      "Training: loss = 1.18376, acc = 0.55000 | Val: loss = 1.05348, acc = 0.79200\n",
      "Training: loss = 1.20291, acc = 0.56429 | Val: loss = 1.05483, acc = 0.79600\n",
      "Training: loss = 1.23756, acc = 0.51429 | Val: loss = 1.05481, acc = 0.79400\n",
      "Training: loss = 1.14080, acc = 0.62143 | Val: loss = 1.05222, acc = 0.79400\n",
      "Training: loss = 1.20445, acc = 0.52857 | Val: loss = 1.05094, acc = 0.79600\n",
      "Training: loss = 1.04219, acc = 0.62143 | Val: loss = 1.04771, acc = 0.80200\n",
      "Training: loss = 1.04581, acc = 0.62143 | Val: loss = 1.04491, acc = 0.80200\n",
      "Training: loss = 1.13299, acc = 0.55714 | Val: loss = 1.04246, acc = 0.80400\n",
      "Training: loss = 1.08801, acc = 0.58571 | Val: loss = 1.03862, acc = 0.80200\n",
      "Training: loss = 1.23540, acc = 0.57857 | Val: loss = 1.03116, acc = 0.80200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: loss = 1.14763, acc = 0.57857 | Val: loss = 1.02443, acc = 0.80800\n",
      "Training: loss = 1.24686, acc = 0.52857 | Val: loss = 1.01955, acc = 0.80400\n",
      "Training: loss = 1.13317, acc = 0.55714 | Val: loss = 1.01577, acc = 0.80400\n",
      "Training: loss = 1.15000, acc = 0.60714 | Val: loss = 1.01220, acc = 0.80400\n",
      "Training: loss = 1.17102, acc = 0.58571 | Val: loss = 1.00934, acc = 0.80800\n",
      "Training: loss = 1.01278, acc = 0.63571 | Val: loss = 1.00667, acc = 0.80800\n",
      "Training: loss = 1.15342, acc = 0.53571 | Val: loss = 1.00482, acc = 0.80800\n",
      "Training: loss = 1.31681, acc = 0.50000 | Val: loss = 1.00474, acc = 0.80800\n",
      "Training: loss = 1.03347, acc = 0.62143 | Val: loss = 1.00552, acc = 0.80600\n",
      "Training: loss = 1.26136, acc = 0.57857 | Val: loss = 1.00870, acc = 0.80200\n",
      "Training: loss = 1.08788, acc = 0.55714 | Val: loss = 1.01158, acc = 0.79800\n",
      "Training: loss = 1.20310, acc = 0.52143 | Val: loss = 1.01506, acc = 0.79600\n",
      "Training: loss = 1.24540, acc = 0.55714 | Val: loss = 1.01884, acc = 0.79800\n",
      "Training: loss = 1.06282, acc = 0.60000 | Val: loss = 1.02409, acc = 0.79800\n",
      "Training: loss = 1.04993, acc = 0.60000 | Val: loss = 1.02730, acc = 0.80000\n",
      "Training: loss = 1.15664, acc = 0.58571 | Val: loss = 1.03165, acc = 0.79800\n",
      "Training: loss = 1.22859, acc = 0.57143 | Val: loss = 1.03611, acc = 0.79400\n",
      "Training: loss = 1.11576, acc = 0.60714 | Val: loss = 1.04105, acc = 0.79400\n",
      "Training: loss = 1.33794, acc = 0.50000 | Val: loss = 1.04292, acc = 0.79400\n",
      "Training: loss = 1.27663, acc = 0.53571 | Val: loss = 1.04370, acc = 0.79400\n",
      "Training: loss = 1.26450, acc = 0.55000 | Val: loss = 1.04599, acc = 0.79200\n",
      "Training: loss = 1.14434, acc = 0.59286 | Val: loss = 1.04587, acc = 0.79200\n",
      "Training: loss = 1.15207, acc = 0.58571 | Val: loss = 1.04624, acc = 0.79400\n",
      "Training: loss = 1.21341, acc = 0.54286 | Val: loss = 1.04706, acc = 0.79200\n",
      "Training: loss = 1.20152, acc = 0.52857 | Val: loss = 1.04606, acc = 0.79200\n",
      "Training: loss = 1.18437, acc = 0.55000 | Val: loss = 1.04193, acc = 0.79400\n",
      "Training: loss = 1.21610, acc = 0.57143 | Val: loss = 1.03762, acc = 0.79400\n",
      "Training: loss = 1.21212, acc = 0.51429 | Val: loss = 1.03328, acc = 0.79600\n",
      "Training: loss = 1.23282, acc = 0.50000 | Val: loss = 1.02930, acc = 0.79600\n",
      "Training: loss = 1.18735, acc = 0.56429 | Val: loss = 1.02674, acc = 0.79400\n",
      "Training: loss = 1.18239, acc = 0.56429 | Val: loss = 1.02560, acc = 0.79400\n",
      "Training: loss = 1.11413, acc = 0.62143 | Val: loss = 1.02565, acc = 0.80200\n",
      "Training: loss = 1.24034, acc = 0.59286 | Val: loss = 1.02509, acc = 0.79800\n",
      "Training: loss = 1.25476, acc = 0.58571 | Val: loss = 1.02452, acc = 0.80400\n",
      "Training: loss = 1.17357, acc = 0.59286 | Val: loss = 1.02364, acc = 0.80600\n",
      "Training: loss = 1.21175, acc = 0.49286 | Val: loss = 1.02263, acc = 0.80600\n",
      "Training: loss = 1.14513, acc = 0.56429 | Val: loss = 1.01990, acc = 0.80600\n",
      "Training: loss = 1.14000, acc = 0.57143 | Val: loss = 1.01733, acc = 0.80600\n",
      "Training: loss = 1.30812, acc = 0.48571 | Val: loss = 1.01648, acc = 0.80600\n",
      "Training: loss = 1.14423, acc = 0.56429 | Val: loss = 1.01693, acc = 0.80400\n",
      "Training: loss = 1.19517, acc = 0.55000 | Val: loss = 1.01722, acc = 0.80200\n",
      "Training: loss = 1.06809, acc = 0.62857 | Val: loss = 1.01669, acc = 0.79800\n",
      "Training: loss = 1.18377, acc = 0.59286 | Val: loss = 1.01670, acc = 0.79600\n",
      "Training: loss = 1.14403, acc = 0.57143 | Val: loss = 1.01673, acc = 0.79800\n",
      "Training: loss = 1.04466, acc = 0.63571 | Val: loss = 1.01637, acc = 0.79800\n",
      "Training: loss = 1.09665, acc = 0.58571 | Val: loss = 1.01703, acc = 0.79800\n",
      "Training: loss = 1.09775, acc = 0.62857 | Val: loss = 1.01709, acc = 0.79600\n",
      "Training: loss = 1.22885, acc = 0.53571 | Val: loss = 1.01635, acc = 0.79600\n",
      "Training: loss = 1.17742, acc = 0.50714 | Val: loss = 1.01779, acc = 0.79400\n",
      "Training: loss = 1.18832, acc = 0.56429 | Val: loss = 1.01975, acc = 0.79400\n",
      "Training: loss = 1.18824, acc = 0.56429 | Val: loss = 1.02219, acc = 0.79600\n",
      "Training: loss = 1.24475, acc = 0.50714 | Val: loss = 1.02486, acc = 0.79800\n",
      "Training: loss = 1.19130, acc = 0.54286 | Val: loss = 1.02888, acc = 0.80000\n",
      "Training: loss = 1.11220, acc = 0.57143 | Val: loss = 1.03168, acc = 0.80000\n",
      "Training: loss = 1.11326, acc = 0.55000 | Val: loss = 1.03144, acc = 0.79800\n",
      "Training: loss = 1.11238, acc = 0.60714 | Val: loss = 1.02927, acc = 0.79800\n",
      "Training: loss = 1.19291, acc = 0.48571 | Val: loss = 1.02772, acc = 0.79400\n",
      "Training: loss = 1.15518, acc = 0.55714 | Val: loss = 1.02608, acc = 0.79400\n",
      "Training: loss = 1.17644, acc = 0.54286 | Val: loss = 1.02229, acc = 0.79600\n",
      "Training: loss = 1.12873, acc = 0.57857 | Val: loss = 1.01865, acc = 0.80000\n",
      "Training: loss = 1.06778, acc = 0.62143 | Val: loss = 1.01604, acc = 0.79800\n",
      "Training: loss = 1.16097, acc = 0.55714 | Val: loss = 1.01312, acc = 0.80400\n",
      "Training: loss = 1.01928, acc = 0.67857 | Val: loss = 1.00848, acc = 0.80800\n",
      "Training: loss = 1.11478, acc = 0.58571 | Val: loss = 1.00396, acc = 0.80800\n",
      "Training: loss = 1.20789, acc = 0.55000 | Val: loss = 1.00085, acc = 0.81400\n",
      "Training: loss = 1.16506, acc = 0.52143 | Val: loss = 0.99749, acc = 0.81400\n",
      "Training: loss = 1.21952, acc = 0.56429 | Val: loss = 0.99473, acc = 0.81600\n",
      "Training: loss = 1.20650, acc = 0.56429 | Val: loss = 0.99416, acc = 0.81400\n",
      "Training: loss = 1.23158, acc = 0.57143 | Val: loss = 0.99512, acc = 0.81000\n",
      "Training: loss = 1.26487, acc = 0.55000 | Val: loss = 0.99633, acc = 0.81000\n",
      "Training: loss = 1.19676, acc = 0.55000 | Val: loss = 0.99771, acc = 0.80600\n",
      "Training: loss = 1.18186, acc = 0.54286 | Val: loss = 0.99821, acc = 0.80600\n",
      "Training: loss = 1.06102, acc = 0.58571 | Val: loss = 0.99920, acc = 0.80200\n",
      "Training: loss = 1.25511, acc = 0.52143 | Val: loss = 0.99878, acc = 0.80200\n",
      "Training: loss = 1.11144, acc = 0.62143 | Val: loss = 0.99876, acc = 0.80400\n",
      "Training: loss = 1.20225, acc = 0.52143 | Val: loss = 0.99946, acc = 0.80600\n",
      "Training: loss = 1.15721, acc = 0.54286 | Val: loss = 1.00129, acc = 0.81000\n",
      "Training: loss = 1.14207, acc = 0.60000 | Val: loss = 1.00541, acc = 0.81600\n",
      "Training: loss = 1.20651, acc = 0.55000 | Val: loss = 1.01024, acc = 0.81400\n",
      "Training: loss = 1.16612, acc = 0.56429 | Val: loss = 1.01534, acc = 0.80800\n",
      "Training: loss = 1.08391, acc = 0.56429 | Val: loss = 1.02244, acc = 0.80600\n",
      "Training: loss = 1.06993, acc = 0.60714 | Val: loss = 1.02980, acc = 0.80800\n",
      "Training: loss = 1.08248, acc = 0.59286 | Val: loss = 1.03740, acc = 0.80600\n",
      "Training: loss = 1.19747, acc = 0.53571 | Val: loss = 1.04555, acc = 0.80200\n",
      "Training: loss = 1.13115, acc = 0.62143 | Val: loss = 1.05261, acc = 0.80000\n",
      "Training: loss = 1.14293, acc = 0.60714 | Val: loss = 1.05658, acc = 0.79800\n",
      "Training: loss = 1.15194, acc = 0.57143 | Val: loss = 1.05682, acc = 0.79800\n",
      "Training: loss = 1.30020, acc = 0.47857 | Val: loss = 1.05351, acc = 0.79600\n",
      "Training: loss = 1.12039, acc = 0.63571 | Val: loss = 1.04808, acc = 0.79800\n",
      "Training: loss = 1.26027, acc = 0.52143 | Val: loss = 1.04219, acc = 0.79800\n",
      "Training: loss = 1.07454, acc = 0.57857 | Val: loss = 1.03733, acc = 0.79600\n",
      "Training: loss = 1.16706, acc = 0.55714 | Val: loss = 1.03262, acc = 0.80000\n",
      "Training: loss = 1.16781, acc = 0.55000 | Val: loss = 1.02572, acc = 0.79800\n",
      "Training: loss = 1.14071, acc = 0.57857 | Val: loss = 1.01806, acc = 0.79200\n",
      "Training: loss = 1.05645, acc = 0.64286 | Val: loss = 1.01229, acc = 0.79200\n",
      "Training: loss = 1.05736, acc = 0.61429 | Val: loss = 1.00781, acc = 0.79600\n",
      "Training: loss = 1.13669, acc = 0.60000 | Val: loss = 1.00147, acc = 0.79800\n",
      "Training: loss = 1.12239, acc = 0.57143 | Val: loss = 0.99545, acc = 0.79800\n",
      "Training: loss = 1.33337, acc = 0.52857 | Val: loss = 0.99434, acc = 0.80200\n",
      "Training: loss = 1.21578, acc = 0.53571 | Val: loss = 0.99375, acc = 0.80200\n",
      "Training: loss = 1.06236, acc = 0.59286 | Val: loss = 0.99414, acc = 0.80400\n",
      "Training: loss = 1.17281, acc = 0.54286 | Val: loss = 0.99517, acc = 0.80400\n",
      "Training: loss = 1.11059, acc = 0.60714 | Val: loss = 0.99670, acc = 0.80400\n",
      "Training: loss = 1.22557, acc = 0.55000 | Val: loss = 0.99820, acc = 0.80800\n",
      "Training: loss = 1.12183, acc = 0.60000 | Val: loss = 1.00078, acc = 0.80600\n",
      "Training: loss = 1.12166, acc = 0.58571 | Val: loss = 1.00355, acc = 0.80600\n",
      "Training: loss = 1.15250, acc = 0.55714 | Val: loss = 1.00496, acc = 0.80600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: loss = 1.23066, acc = 0.53571 | Val: loss = 1.00464, acc = 0.80400\n",
      "Training: loss = 1.22979, acc = 0.50000 | Val: loss = 1.00452, acc = 0.80400\n",
      "Training: loss = 1.20087, acc = 0.53571 | Val: loss = 1.00460, acc = 0.80200\n",
      "Training: loss = 1.19886, acc = 0.58571 | Val: loss = 1.00486, acc = 0.80200\n",
      "Training: loss = 1.16540, acc = 0.55714 | Val: loss = 1.00635, acc = 0.80200\n",
      "Training: loss = 1.13227, acc = 0.57143 | Val: loss = 1.00891, acc = 0.80200\n",
      "Training: loss = 1.23454, acc = 0.56429 | Val: loss = 1.01239, acc = 0.80400\n",
      "Training: loss = 1.12054, acc = 0.56429 | Val: loss = 1.01518, acc = 0.80400\n",
      "Training: loss = 1.30177, acc = 0.53571 | Val: loss = 1.01822, acc = 0.80200\n",
      "Training: loss = 1.23385, acc = 0.55714 | Val: loss = 1.02279, acc = 0.80000\n",
      "Training: loss = 1.24177, acc = 0.53571 | Val: loss = 1.02690, acc = 0.79800\n",
      "Training: loss = 1.23316, acc = 0.55000 | Val: loss = 1.03099, acc = 0.80000\n",
      "Training: loss = 1.19081, acc = 0.50000 | Val: loss = 1.03298, acc = 0.80000\n",
      "Training: loss = 1.15030, acc = 0.60000 | Val: loss = 1.03731, acc = 0.80000\n",
      "Training: loss = 1.23644, acc = 0.52857 | Val: loss = 1.03812, acc = 0.79800\n",
      "Training: loss = 1.07164, acc = 0.58571 | Val: loss = 1.03773, acc = 0.79800\n",
      "Training: loss = 1.09627, acc = 0.57143 | Val: loss = 1.03514, acc = 0.79400\n",
      "Training: loss = 1.17520, acc = 0.60000 | Val: loss = 1.03169, acc = 0.79400\n",
      "Training: loss = 1.19090, acc = 0.52857 | Val: loss = 1.02637, acc = 0.79400\n",
      "Training: loss = 1.11500, acc = 0.60714 | Val: loss = 1.01990, acc = 0.79600\n",
      "Training: loss = 1.10289, acc = 0.56429 | Val: loss = 1.01261, acc = 0.79400\n",
      "Training: loss = 1.13678, acc = 0.55000 | Val: loss = 1.00511, acc = 0.79400\n",
      "Training: loss = 1.23733, acc = 0.56429 | Val: loss = 0.99511, acc = 0.79800\n",
      "Training: loss = 1.11978, acc = 0.59286 | Val: loss = 0.98564, acc = 0.80400\n",
      "Training: loss = 1.21926, acc = 0.54286 | Val: loss = 0.97862, acc = 0.80800\n",
      "Training: loss = 1.20941, acc = 0.53571 | Val: loss = 0.97345, acc = 0.81000\n",
      "Training: loss = 1.15133, acc = 0.56429 | Val: loss = 0.97126, acc = 0.81600\n",
      "Training: loss = 1.18657, acc = 0.54286 | Val: loss = 0.97131, acc = 0.81800\n",
      "Training: loss = 1.11860, acc = 0.60714 | Val: loss = 0.97401, acc = 0.81800\n",
      "Training: loss = 1.14475, acc = 0.57143 | Val: loss = 0.97862, acc = 0.81000\n",
      "Training: loss = 1.20970, acc = 0.52143 | Val: loss = 0.98366, acc = 0.80400\n",
      "Training: loss = 1.20523, acc = 0.55714 | Val: loss = 0.99070, acc = 0.80400\n",
      "Training: loss = 1.14644, acc = 0.55714 | Val: loss = 0.99805, acc = 0.79600\n",
      "Training: loss = 1.12142, acc = 0.57857 | Val: loss = 1.00488, acc = 0.79400\n",
      "Training: loss = 1.05241, acc = 0.63571 | Val: loss = 1.01156, acc = 0.79000\n",
      "Training: loss = 1.19067, acc = 0.59286 | Val: loss = 1.01709, acc = 0.79800\n",
      "Training: loss = 1.12468, acc = 0.62143 | Val: loss = 1.01959, acc = 0.80200\n",
      "Training: loss = 1.25159, acc = 0.50714 | Val: loss = 1.02259, acc = 0.80000\n",
      "Training: loss = 1.27831, acc = 0.52143 | Val: loss = 1.02402, acc = 0.80000\n",
      "Training: loss = 1.12623, acc = 0.62143 | Val: loss = 1.02314, acc = 0.80400\n",
      "Training: loss = 1.15227, acc = 0.57143 | Val: loss = 1.01802, acc = 0.80400\n",
      "Training: loss = 1.13337, acc = 0.62143 | Val: loss = 1.01271, acc = 0.80600\n",
      "Training: loss = 1.21743, acc = 0.56429 | Val: loss = 1.00867, acc = 0.81200\n",
      "Training: loss = 1.04435, acc = 0.60714 | Val: loss = 1.00434, acc = 0.80800\n",
      "Training: loss = 1.17731, acc = 0.60714 | Val: loss = 1.00074, acc = 0.80600\n",
      "Training: loss = 1.24173, acc = 0.50714 | Val: loss = 0.99812, acc = 0.80800\n",
      "Training: loss = 1.14515, acc = 0.58571 | Val: loss = 0.99679, acc = 0.81200\n",
      "Training: loss = 1.10414, acc = 0.57857 | Val: loss = 0.99546, acc = 0.81200\n",
      "Training: loss = 1.34136, acc = 0.47143 | Val: loss = 0.99720, acc = 0.81400\n",
      "Training: loss = 1.15948, acc = 0.60000 | Val: loss = 0.99855, acc = 0.81400\n",
      "Training: loss = 1.24857, acc = 0.49286 | Val: loss = 1.00168, acc = 0.81400\n",
      "Training: loss = 1.03781, acc = 0.62143 | Val: loss = 1.00492, acc = 0.81200\n",
      "Training: loss = 1.19847, acc = 0.52857 | Val: loss = 1.00942, acc = 0.80000\n",
      "Training: loss = 1.15718, acc = 0.56429 | Val: loss = 1.01353, acc = 0.79600\n",
      "Training: loss = 1.18294, acc = 0.58571 | Val: loss = 1.01371, acc = 0.79200\n",
      "Training: loss = 1.15556, acc = 0.56429 | Val: loss = 1.01228, acc = 0.79000\n",
      "Training: loss = 1.06685, acc = 0.63571 | Val: loss = 1.01143, acc = 0.79000\n",
      "Training: loss = 1.11289, acc = 0.60714 | Val: loss = 1.00958, acc = 0.79000\n",
      "Training: loss = 1.00216, acc = 0.65000 | Val: loss = 1.00468, acc = 0.79000\n",
      "Training: loss = 1.10910, acc = 0.58571 | Val: loss = 0.99966, acc = 0.79400\n",
      "Training: loss = 1.21786, acc = 0.56429 | Val: loss = 0.99520, acc = 0.80000\n",
      "Training: loss = 1.20703, acc = 0.54286 | Val: loss = 0.98954, acc = 0.80200\n",
      "Training: loss = 1.16801, acc = 0.54286 | Val: loss = 0.98548, acc = 0.80000\n",
      "Training: loss = 1.09911, acc = 0.63571 | Val: loss = 0.98236, acc = 0.80200\n",
      "Training: loss = 1.22036, acc = 0.55000 | Val: loss = 0.98019, acc = 0.80000\n",
      "Training: loss = 1.32189, acc = 0.52857 | Val: loss = 0.97981, acc = 0.79800\n",
      "Training: loss = 1.17536, acc = 0.56429 | Val: loss = 0.98058, acc = 0.80000\n",
      "Training: loss = 1.18185, acc = 0.60000 | Val: loss = 0.98306, acc = 0.80400\n",
      "Training: loss = 1.14637, acc = 0.59286 | Val: loss = 0.98704, acc = 0.80200\n",
      "Training: loss = 1.17659, acc = 0.59286 | Val: loss = 0.98903, acc = 0.79800\n",
      "Training: loss = 1.13915, acc = 0.57143 | Val: loss = 0.98975, acc = 0.80600\n",
      "Training: loss = 1.11624, acc = 0.55714 | Val: loss = 0.99131, acc = 0.80400\n",
      "Training: loss = 1.14881, acc = 0.57857 | Val: loss = 0.99437, acc = 0.80600\n",
      "Training: loss = 1.09926, acc = 0.62143 | Val: loss = 0.99748, acc = 0.80600\n",
      "Training: loss = 1.23421, acc = 0.54286 | Val: loss = 1.00072, acc = 0.80200\n",
      "Training: loss = 1.19872, acc = 0.52857 | Val: loss = 1.00302, acc = 0.80200\n",
      "Training: loss = 1.23509, acc = 0.51429 | Val: loss = 1.00407, acc = 0.80400\n",
      "Training: loss = 1.16466, acc = 0.57857 | Val: loss = 1.00468, acc = 0.80400\n",
      "Training: loss = 1.17746, acc = 0.52143 | Val: loss = 1.00674, acc = 0.79600\n",
      "Training: loss = 1.15996, acc = 0.57143 | Val: loss = 1.00924, acc = 0.79800\n",
      "Training: loss = 1.14756, acc = 0.58571 | Val: loss = 1.01504, acc = 0.79400\n",
      "Training: loss = 1.13723, acc = 0.58571 | Val: loss = 1.01980, acc = 0.79200\n",
      "Training: loss = 1.04752, acc = 0.60714 | Val: loss = 1.02222, acc = 0.79000\n",
      "Training: loss = 1.13427, acc = 0.54286 | Val: loss = 1.02330, acc = 0.79000\n",
      "Training: loss = 1.17717, acc = 0.56429 | Val: loss = 1.02378, acc = 0.79000\n",
      "Training: loss = 1.22414, acc = 0.49286 | Val: loss = 1.02180, acc = 0.79400\n",
      "Training: loss = 1.25934, acc = 0.52857 | Val: loss = 1.01798, acc = 0.79000\n",
      "Training: loss = 1.11987, acc = 0.57143 | Val: loss = 1.01466, acc = 0.79400\n",
      "Training: loss = 1.05418, acc = 0.62143 | Val: loss = 1.01013, acc = 0.79400\n",
      "Training: loss = 1.06406, acc = 0.60000 | Val: loss = 1.00684, acc = 0.79400\n",
      "Training: loss = 1.10502, acc = 0.62143 | Val: loss = 1.00487, acc = 0.79000\n",
      "Training: loss = 1.19776, acc = 0.57143 | Val: loss = 1.00081, acc = 0.79000\n",
      "Training: loss = 1.16247, acc = 0.56429 | Val: loss = 0.99653, acc = 0.79200\n",
      "Training: loss = 1.03564, acc = 0.66429 | Val: loss = 0.99062, acc = 0.79600\n",
      "Training: loss = 1.10908, acc = 0.59286 | Val: loss = 0.98666, acc = 0.79400\n",
      "Training: loss = 1.15381, acc = 0.56429 | Val: loss = 0.98375, acc = 0.79400\n",
      "Training: loss = 1.18712, acc = 0.55000 | Val: loss = 0.98172, acc = 0.79400\n",
      "Training: loss = 1.16640, acc = 0.57143 | Val: loss = 0.98073, acc = 0.80600\n",
      "Training: loss = 1.17315, acc = 0.57143 | Val: loss = 0.97940, acc = 0.80600\n",
      "Training: loss = 1.25608, acc = 0.55714 | Val: loss = 0.97747, acc = 0.80400\n",
      "Training: loss = 1.12405, acc = 0.63571 | Val: loss = 0.97614, acc = 0.80600\n",
      "Training: loss = 1.06840, acc = 0.62857 | Val: loss = 0.97473, acc = 0.80800\n",
      "Training: loss = 1.23964, acc = 0.52857 | Val: loss = 0.97336, acc = 0.81400\n",
      "Training: loss = 1.09554, acc = 0.62143 | Val: loss = 0.97198, acc = 0.81400\n",
      "Training: loss = 1.27032, acc = 0.50714 | Val: loss = 0.97290, acc = 0.81400\n",
      "Training: loss = 1.15034, acc = 0.57143 | Val: loss = 0.97399, acc = 0.81600\n",
      "Training: loss = 1.16335, acc = 0.56429 | Val: loss = 0.97524, acc = 0.81800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: loss = 1.07878, acc = 0.60714 | Val: loss = 0.97966, acc = 0.81800\n",
      "Training: loss = 0.95512, acc = 0.65714 | Val: loss = 0.98466, acc = 0.81800\n",
      "Training: loss = 1.13952, acc = 0.55000 | Val: loss = 0.98731, acc = 0.81200\n",
      "Training: loss = 1.14721, acc = 0.57857 | Val: loss = 0.98604, acc = 0.81000\n",
      "Training: loss = 1.12281, acc = 0.56429 | Val: loss = 0.98361, acc = 0.80800\n",
      "Training: loss = 1.19617, acc = 0.54286 | Val: loss = 0.98188, acc = 0.81000\n",
      "Training: loss = 1.11291, acc = 0.61429 | Val: loss = 0.97993, acc = 0.81000\n",
      "Training: loss = 1.11278, acc = 0.58571 | Val: loss = 0.97984, acc = 0.81000\n",
      "Training: loss = 1.16232, acc = 0.55000 | Val: loss = 0.98233, acc = 0.80400\n",
      "Training: loss = 1.22874, acc = 0.50714 | Val: loss = 0.98716, acc = 0.80600\n",
      "Training: loss = 1.23667, acc = 0.54286 | Val: loss = 0.99333, acc = 0.79800\n",
      "Training: loss = 1.07493, acc = 0.64286 | Val: loss = 0.99991, acc = 0.79400\n",
      "Training: loss = 1.05845, acc = 0.57143 | Val: loss = 1.00675, acc = 0.79400\n",
      "Training: loss = 1.19951, acc = 0.59286 | Val: loss = 1.01230, acc = 0.79800\n",
      "Training: loss = 1.20490, acc = 0.47857 | Val: loss = 1.01339, acc = 0.80000\n",
      "Training: loss = 1.05312, acc = 0.60000 | Val: loss = 1.01356, acc = 0.79600\n",
      "Training: loss = 1.09057, acc = 0.62143 | Val: loss = 1.01548, acc = 0.79800\n",
      "Training: loss = 1.27507, acc = 0.51429 | Val: loss = 1.01608, acc = 0.79600\n",
      "Training: loss = 1.18099, acc = 0.52143 | Val: loss = 1.01487, acc = 0.79200\n",
      "Training: loss = 1.15349, acc = 0.57143 | Val: loss = 1.01360, acc = 0.79000\n",
      "Early stop! Min loss:  0.9712573885917664 , Max accuracy:  0.8199998140335083\n",
      "Early stop model validation loss:  1.3617230653762817 , accuracy:  0.8199998140335083\n",
      "Test loss: 1.3404353857040405 ; Test accuracy: 0.8249991536140442\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/4tb/nabarun/nlp/DLNLP/dgl_gat\n"
     ]
    }
   ],
   "source": [
    "%cd /4tb/nabarun/nlp/DLNLP/dgl_gat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "Namespace(attn_drop=0.6, dataset='pubmed', early_stop=False, epochs=200, fastmode=False, gpu=-1, in_drop=0.6, lr=0.005, negative_slope=0.2, num_heads=8, num_hidden=8, num_layers=1, num_out_heads=1, residual=False, weight_decay=0.0005)\n",
      "Loading from cache failed, re-processing.\n",
      "Finished data loading and preprocessing.\n",
      "  NumNodes: 19717\n",
      "  NumEdges: 88651\n",
      "  NumFeats: 500\n",
      "  NumClasses: 3\n",
      "  NumTrainingSamples: 60\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done saving data into cached files.\n",
      "/4tb/nabarun/nlp/gcn_text_categorization/venv/lib/python3.6/site-packages/dgl/data/utils.py:285: UserWarning: Property dataset.num_labels will be deprecated, please use dataset.num_classes instead.\n",
      "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
      "/4tb/nabarun/nlp/gcn_text_categorization/venv/lib/python3.6/site-packages/dgl/data/utils.py:285: UserWarning: Property dataset.graph will be deprecated, please use dataset[0] instead.\n",
      "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
      "----Data statistics------'\n",
      "      #Edges 88651\n",
      "      #Classes 3\n",
      "      #Train samples 60\n",
      "      #Val samples 500\n",
      "      #Test samples 1000\n",
      "GAT(\n",
      "  (gat_layers): ModuleList(\n",
      "    (0): GATConv(\n",
      "      (fc): Linear(in_features=500, out_features=64, bias=False)\n",
      "      (feat_drop): Dropout(p=0.6, inplace=False)\n",
      "      (attn_drop): Dropout(p=0.6, inplace=False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (1): GATConv(\n",
      "      (fc): Linear(in_features=64, out_features=3, bias=False)\n",
      "      (feat_drop): Dropout(p=0.6, inplace=False)\n",
      "      (attn_drop): Dropout(p=0.6, inplace=False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "/4tb/nabarun/nlp/gcn_text_categorization/venv/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/4tb/nabarun/nlp/gcn_text_categorization/venv/lib/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Epoch 00000 | Time(s) nan | Loss 1.1022 | TrainAcc 0.2833 | ValAcc 0.4760 | ETputs(KTEPS) nan\n",
      "Epoch 00001 | Time(s) nan | Loss 1.0877 | TrainAcc 0.4167 | ValAcc 0.5460 | ETputs(KTEPS) nan\n",
      "Epoch 00002 | Time(s) nan | Loss 1.0803 | TrainAcc 0.5000 | ValAcc 0.5700 | ETputs(KTEPS) nan\n",
      "Epoch 00003 | Time(s) 0.1596 | Loss 1.0738 | TrainAcc 0.5667 | ValAcc 0.5720 | ETputs(KTEPS) 678.86\n",
      "Epoch 00004 | Time(s) 0.1702 | Loss 1.0492 | TrainAcc 0.6500 | ValAcc 0.5760 | ETputs(KTEPS) 636.72\n",
      "Epoch 00005 | Time(s) 0.1740 | Loss 1.0415 | TrainAcc 0.6167 | ValAcc 0.5820 | ETputs(KTEPS) 622.65\n",
      "Epoch 00006 | Time(s) 0.1722 | Loss 1.0303 | TrainAcc 0.6833 | ValAcc 0.6060 | ETputs(KTEPS) 629.37\n",
      "Epoch 00007 | Time(s) 0.1783 | Loss 1.0437 | TrainAcc 0.6000 | ValAcc 0.6360 | ETputs(KTEPS) 607.63\n",
      "Epoch 00008 | Time(s) 0.2037 | Loss 1.0237 | TrainAcc 0.6833 | ValAcc 0.6500 | ETputs(KTEPS) 532.07\n",
      "Epoch 00009 | Time(s) 0.1994 | Loss 1.0028 | TrainAcc 0.6333 | ValAcc 0.6740 | ETputs(KTEPS) 543.36\n",
      "Epoch 00010 | Time(s) 0.1975 | Loss 0.9859 | TrainAcc 0.7667 | ValAcc 0.6920 | ETputs(KTEPS) 548.62\n",
      "Epoch 00011 | Time(s) 0.1950 | Loss 0.9797 | TrainAcc 0.7500 | ValAcc 0.7100 | ETputs(KTEPS) 555.77\n",
      "Epoch 00012 | Time(s) 0.1901 | Loss 0.9635 | TrainAcc 0.8000 | ValAcc 0.7120 | ETputs(KTEPS) 570.05\n",
      "Epoch 00013 | Time(s) 0.1877 | Loss 0.9795 | TrainAcc 0.6500 | ValAcc 0.7180 | ETputs(KTEPS) 577.41\n",
      "Epoch 00014 | Time(s) 0.1870 | Loss 0.9672 | TrainAcc 0.7500 | ValAcc 0.7260 | ETputs(KTEPS) 579.64\n",
      "Epoch 00015 | Time(s) 0.1863 | Loss 0.9344 | TrainAcc 0.7500 | ValAcc 0.7300 | ETputs(KTEPS) 581.64\n",
      "Epoch 00016 | Time(s) 0.1855 | Loss 0.9534 | TrainAcc 0.7167 | ValAcc 0.7280 | ETputs(KTEPS) 584.19\n",
      "Epoch 00017 | Time(s) 0.1971 | Loss 0.9338 | TrainAcc 0.7333 | ValAcc 0.7320 | ETputs(KTEPS) 549.82\n",
      "Epoch 00018 | Time(s) 0.1940 | Loss 0.9085 | TrainAcc 0.8667 | ValAcc 0.7220 | ETputs(KTEPS) 558.49\n",
      "Epoch 00019 | Time(s) 0.1914 | Loss 0.8632 | TrainAcc 0.8167 | ValAcc 0.7240 | ETputs(KTEPS) 566.06\n",
      "Epoch 00020 | Time(s) 0.1896 | Loss 0.8571 | TrainAcc 0.8167 | ValAcc 0.7220 | ETputs(KTEPS) 571.65\n",
      "Epoch 00021 | Time(s) 0.1895 | Loss 0.8798 | TrainAcc 0.7333 | ValAcc 0.7220 | ETputs(KTEPS) 571.93\n",
      "Epoch 00022 | Time(s) 0.1889 | Loss 0.8329 | TrainAcc 0.8667 | ValAcc 0.7220 | ETputs(KTEPS) 573.62\n",
      "Epoch 00023 | Time(s) 0.1894 | Loss 0.8510 | TrainAcc 0.7500 | ValAcc 0.7240 | ETputs(KTEPS) 572.09\n",
      "Epoch 00024 | Time(s) 0.1881 | Loss 0.8476 | TrainAcc 0.8167 | ValAcc 0.7240 | ETputs(KTEPS) 576.15\n",
      "Epoch 00025 | Time(s) 0.1878 | Loss 0.8563 | TrainAcc 0.7167 | ValAcc 0.7260 | ETputs(KTEPS) 577.01\n",
      "Epoch 00026 | Time(s) 0.1939 | Loss 0.8205 | TrainAcc 0.7333 | ValAcc 0.7280 | ETputs(KTEPS) 558.76\n",
      "Epoch 00027 | Time(s) 0.1933 | Loss 0.8587 | TrainAcc 0.7833 | ValAcc 0.7280 | ETputs(KTEPS) 560.50\n",
      "Epoch 00028 | Time(s) 0.1925 | Loss 0.8294 | TrainAcc 0.7667 | ValAcc 0.7280 | ETputs(KTEPS) 562.85\n",
      "Epoch 00029 | Time(s) 0.1911 | Loss 0.8079 | TrainAcc 0.7000 | ValAcc 0.7340 | ETputs(KTEPS) 566.96\n",
      "Epoch 00030 | Time(s) 0.1903 | Loss 0.7925 | TrainAcc 0.8000 | ValAcc 0.7360 | ETputs(KTEPS) 569.43\n",
      "Epoch 00031 | Time(s) 0.1893 | Loss 0.7925 | TrainAcc 0.7667 | ValAcc 0.7380 | ETputs(KTEPS) 572.52\n",
      "Epoch 00032 | Time(s) 0.1882 | Loss 0.8513 | TrainAcc 0.6500 | ValAcc 0.7400 | ETputs(KTEPS) 575.80\n",
      "Epoch 00033 | Time(s) 0.1871 | Loss 0.8058 | TrainAcc 0.7333 | ValAcc 0.7420 | ETputs(KTEPS) 579.29\n",
      "Epoch 00034 | Time(s) 0.1905 | Loss 0.7210 | TrainAcc 0.8167 | ValAcc 0.7420 | ETputs(KTEPS) 568.98\n",
      "Epoch 00035 | Time(s) 0.1954 | Loss 0.7377 | TrainAcc 0.8167 | ValAcc 0.7380 | ETputs(KTEPS) 554.70\n",
      "Epoch 00036 | Time(s) 0.1947 | Loss 0.7454 | TrainAcc 0.7333 | ValAcc 0.7420 | ETputs(KTEPS) 556.56\n",
      "Epoch 00037 | Time(s) 0.1946 | Loss 0.7548 | TrainAcc 0.7167 | ValAcc 0.7400 | ETputs(KTEPS) 556.81\n",
      "Epoch 00038 | Time(s) 0.1945 | Loss 0.7461 | TrainAcc 0.7667 | ValAcc 0.7420 | ETputs(KTEPS) 557.21\n",
      "Epoch 00039 | Time(s) 0.1942 | Loss 0.7506 | TrainAcc 0.7667 | ValAcc 0.7420 | ETputs(KTEPS) 557.94\n",
      "Epoch 00040 | Time(s) 0.1935 | Loss 0.7497 | TrainAcc 0.7167 | ValAcc 0.7440 | ETputs(KTEPS) 560.09\n",
      "Epoch 00041 | Time(s) 0.1923 | Loss 0.6992 | TrainAcc 0.8167 | ValAcc 0.7440 | ETputs(KTEPS) 563.41\n",
      "Epoch 00042 | Time(s) 0.1922 | Loss 0.7543 | TrainAcc 0.8000 | ValAcc 0.7440 | ETputs(KTEPS) 563.79\n",
      "Epoch 00043 | Time(s) 0.1920 | Loss 0.6172 | TrainAcc 0.8167 | ValAcc 0.7460 | ETputs(KTEPS) 564.44\n",
      "Epoch 00044 | Time(s) 0.1930 | Loss 0.7213 | TrainAcc 0.7333 | ValAcc 0.7460 | ETputs(KTEPS) 561.42\n",
      "Epoch 00045 | Time(s) 0.1926 | Loss 0.6876 | TrainAcc 0.7667 | ValAcc 0.7480 | ETputs(KTEPS) 562.63\n",
      "Epoch 00046 | Time(s) 0.1922 | Loss 0.6782 | TrainAcc 0.7667 | ValAcc 0.7520 | ETputs(KTEPS) 563.91\n",
      "Epoch 00047 | Time(s) 0.1919 | Loss 0.7018 | TrainAcc 0.7333 | ValAcc 0.7540 | ETputs(KTEPS) 564.75\n",
      "Epoch 00048 | Time(s) 0.1916 | Loss 0.7539 | TrainAcc 0.6833 | ValAcc 0.7540 | ETputs(KTEPS) 565.71\n",
      "Epoch 00049 | Time(s) 0.1908 | Loss 0.6308 | TrainAcc 0.8333 | ValAcc 0.7560 | ETputs(KTEPS) 567.82\n",
      "Epoch 00050 | Time(s) 0.1900 | Loss 0.6382 | TrainAcc 0.8333 | ValAcc 0.7560 | ETputs(KTEPS) 570.30\n",
      "Epoch 00051 | Time(s) 0.1890 | Loss 0.6695 | TrainAcc 0.7333 | ValAcc 0.7560 | ETputs(KTEPS) 573.34\n",
      "Epoch 00052 | Time(s) 0.1880 | Loss 0.5788 | TrainAcc 0.8667 | ValAcc 0.7540 | ETputs(KTEPS) 576.44\n",
      "Epoch 00053 | Time(s) 0.1870 | Loss 0.6496 | TrainAcc 0.7833 | ValAcc 0.7540 | ETputs(KTEPS) 579.49\n",
      "Epoch 00054 | Time(s) 0.1870 | Loss 0.6006 | TrainAcc 0.7667 | ValAcc 0.7540 | ETputs(KTEPS) 579.34\n",
      "Epoch 00055 | Time(s) 0.1868 | Loss 0.5633 | TrainAcc 0.9000 | ValAcc 0.7580 | ETputs(KTEPS) 580.13\n",
      "Epoch 00056 | Time(s) 0.1859 | Loss 0.6378 | TrainAcc 0.7500 | ValAcc 0.7580 | ETputs(KTEPS) 582.93\n",
      "Epoch 00057 | Time(s) 0.1855 | Loss 0.6028 | TrainAcc 0.8000 | ValAcc 0.7600 | ETputs(KTEPS) 584.07\n",
      "Epoch 00058 | Time(s) 0.1847 | Loss 0.5512 | TrainAcc 0.8500 | ValAcc 0.7600 | ETputs(KTEPS) 586.62\n",
      "Epoch 00059 | Time(s) 0.1848 | Loss 0.5646 | TrainAcc 0.7833 | ValAcc 0.7600 | ETputs(KTEPS) 586.52\n",
      "Epoch 00060 | Time(s) 0.1847 | Loss 0.5845 | TrainAcc 0.8333 | ValAcc 0.7640 | ETputs(KTEPS) 586.81\n",
      "Epoch 00061 | Time(s) 0.1843 | Loss 0.5470 | TrainAcc 0.8167 | ValAcc 0.7660 | ETputs(KTEPS) 588.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00062 | Time(s) 0.1839 | Loss 0.5844 | TrainAcc 0.8167 | ValAcc 0.7680 | ETputs(KTEPS) 589.40\n",
      "Epoch 00063 | Time(s) 0.1861 | Loss 0.5587 | TrainAcc 0.8333 | ValAcc 0.7740 | ETputs(KTEPS) 582.18\n",
      "Epoch 00064 | Time(s) 0.1854 | Loss 0.5866 | TrainAcc 0.8500 | ValAcc 0.7780 | ETputs(KTEPS) 584.35\n",
      "Epoch 00065 | Time(s) 0.1847 | Loss 0.5645 | TrainAcc 0.8000 | ValAcc 0.7800 | ETputs(KTEPS) 586.56\n",
      "Epoch 00066 | Time(s) 0.1841 | Loss 0.6153 | TrainAcc 0.7167 | ValAcc 0.7800 | ETputs(KTEPS) 588.57\n",
      "Epoch 00067 | Time(s) 0.1840 | Loss 0.6173 | TrainAcc 0.7333 | ValAcc 0.7800 | ETputs(KTEPS) 589.01\n",
      "Epoch 00068 | Time(s) 0.1839 | Loss 0.5660 | TrainAcc 0.8333 | ValAcc 0.7800 | ETputs(KTEPS) 589.13\n",
      "Epoch 00069 | Time(s) 0.1837 | Loss 0.4851 | TrainAcc 0.8333 | ValAcc 0.7820 | ETputs(KTEPS) 589.81\n",
      "Epoch 00070 | Time(s) 0.1833 | Loss 0.5870 | TrainAcc 0.7667 | ValAcc 0.7800 | ETputs(KTEPS) 591.07\n",
      "Epoch 00071 | Time(s) 0.1832 | Loss 0.5543 | TrainAcc 0.8333 | ValAcc 0.7780 | ETputs(KTEPS) 591.45\n",
      "Epoch 00072 | Time(s) 0.1856 | Loss 0.5879 | TrainAcc 0.7333 | ValAcc 0.7800 | ETputs(KTEPS) 583.89\n",
      "Epoch 00073 | Time(s) 0.1853 | Loss 0.5543 | TrainAcc 0.7833 | ValAcc 0.7780 | ETputs(KTEPS) 584.91\n",
      "Epoch 00074 | Time(s) 0.1849 | Loss 0.4834 | TrainAcc 0.8333 | ValAcc 0.7760 | ETputs(KTEPS) 586.08\n",
      "Epoch 00075 | Time(s) 0.1856 | Loss 0.5879 | TrainAcc 0.7667 | ValAcc 0.7760 | ETputs(KTEPS) 583.90\n",
      "Epoch 00076 | Time(s) 0.1849 | Loss 0.5505 | TrainAcc 0.9000 | ValAcc 0.7760 | ETputs(KTEPS) 585.97\n",
      "Epoch 00077 | Time(s) 0.1845 | Loss 0.5744 | TrainAcc 0.7500 | ValAcc 0.7780 | ETputs(KTEPS) 587.37\n",
      "Epoch 00078 | Time(s) 0.1842 | Loss 0.5281 | TrainAcc 0.7833 | ValAcc 0.7760 | ETputs(KTEPS) 588.21\n",
      "Epoch 00079 | Time(s) 0.1841 | Loss 0.5531 | TrainAcc 0.8167 | ValAcc 0.7780 | ETputs(KTEPS) 588.54\n",
      "Epoch 00080 | Time(s) 0.1840 | Loss 0.5220 | TrainAcc 0.8833 | ValAcc 0.7780 | ETputs(KTEPS) 588.80\n",
      "Epoch 00081 | Time(s) 0.1864 | Loss 0.5804 | TrainAcc 0.8000 | ValAcc 0.7780 | ETputs(KTEPS) 581.46\n",
      "Epoch 00082 | Time(s) 0.1863 | Loss 0.5815 | TrainAcc 0.7500 | ValAcc 0.7780 | ETputs(KTEPS) 581.58\n",
      "Epoch 00083 | Time(s) 0.1860 | Loss 0.4711 | TrainAcc 0.8000 | ValAcc 0.7780 | ETputs(KTEPS) 582.70\n",
      "Epoch 00084 | Time(s) 0.1857 | Loss 0.4467 | TrainAcc 0.9000 | ValAcc 0.7780 | ETputs(KTEPS) 583.49\n",
      "Epoch 00085 | Time(s) 0.1855 | Loss 0.5514 | TrainAcc 0.7500 | ValAcc 0.7780 | ETputs(KTEPS) 584.16\n",
      "Epoch 00086 | Time(s) 0.1855 | Loss 0.5222 | TrainAcc 0.8000 | ValAcc 0.7780 | ETputs(KTEPS) 584.20\n",
      "Epoch 00087 | Time(s) 0.1855 | Loss 0.4556 | TrainAcc 0.8333 | ValAcc 0.7760 | ETputs(KTEPS) 584.27\n",
      "Epoch 00088 | Time(s) 0.1854 | Loss 0.4569 | TrainAcc 0.8000 | ValAcc 0.7780 | ETputs(KTEPS) 584.53\n",
      "Epoch 00089 | Time(s) 0.1851 | Loss 0.5219 | TrainAcc 0.7833 | ValAcc 0.7780 | ETputs(KTEPS) 585.46\n",
      "Epoch 00090 | Time(s) 0.1848 | Loss 0.4954 | TrainAcc 0.7667 | ValAcc 0.7780 | ETputs(KTEPS) 586.36\n",
      "Epoch 00091 | Time(s) 0.1847 | Loss 0.4846 | TrainAcc 0.8333 | ValAcc 0.7780 | ETputs(KTEPS) 586.84\n",
      "Epoch 00092 | Time(s) 0.1846 | Loss 0.5735 | TrainAcc 0.7500 | ValAcc 0.7780 | ETputs(KTEPS) 587.17\n",
      "Epoch 00093 | Time(s) 0.1845 | Loss 0.4692 | TrainAcc 0.8000 | ValAcc 0.7800 | ETputs(KTEPS) 587.23\n",
      "Epoch 00094 | Time(s) 0.1845 | Loss 0.4739 | TrainAcc 0.8000 | ValAcc 0.7780 | ETputs(KTEPS) 587.40\n",
      "Epoch 00095 | Time(s) 0.1842 | Loss 0.4538 | TrainAcc 0.8167 | ValAcc 0.7780 | ETputs(KTEPS) 588.22\n",
      "Epoch 00096 | Time(s) 0.1839 | Loss 0.5438 | TrainAcc 0.7000 | ValAcc 0.7780 | ETputs(KTEPS) 589.22\n",
      "Epoch 00097 | Time(s) 0.1837 | Loss 0.3932 | TrainAcc 0.8000 | ValAcc 0.7800 | ETputs(KTEPS) 589.97\n",
      "Epoch 00098 | Time(s) 0.1834 | Loss 0.4625 | TrainAcc 0.8167 | ValAcc 0.7780 | ETputs(KTEPS) 591.01\n",
      "Epoch 00099 | Time(s) 0.1830 | Loss 0.4452 | TrainAcc 0.8000 | ValAcc 0.7780 | ETputs(KTEPS) 592.27\n",
      "Epoch 00100 | Time(s) 0.1842 | Loss 0.5254 | TrainAcc 0.8167 | ValAcc 0.7820 | ETputs(KTEPS) 588.27\n",
      "Epoch 00101 | Time(s) 0.1840 | Loss 0.4914 | TrainAcc 0.8000 | ValAcc 0.7820 | ETputs(KTEPS) 589.08\n",
      "Epoch 00102 | Time(s) 0.1839 | Loss 0.5816 | TrainAcc 0.7333 | ValAcc 0.7820 | ETputs(KTEPS) 589.38\n",
      "Epoch 00103 | Time(s) 0.1838 | Loss 0.4926 | TrainAcc 0.7667 | ValAcc 0.7820 | ETputs(KTEPS) 589.53\n",
      "Epoch 00104 | Time(s) 0.1836 | Loss 0.3985 | TrainAcc 0.8667 | ValAcc 0.7820 | ETputs(KTEPS) 590.18\n",
      "Epoch 00105 | Time(s) 0.1834 | Loss 0.5884 | TrainAcc 0.7500 | ValAcc 0.7820 | ETputs(KTEPS) 591.02\n",
      "Epoch 00106 | Time(s) 0.1831 | Loss 0.4275 | TrainAcc 0.8000 | ValAcc 0.7800 | ETputs(KTEPS) 591.77\n",
      "Epoch 00107 | Time(s) 0.1829 | Loss 0.4872 | TrainAcc 0.8167 | ValAcc 0.7780 | ETputs(KTEPS) 592.59\n",
      "Epoch 00108 | Time(s) 0.1826 | Loss 0.4781 | TrainAcc 0.7667 | ValAcc 0.7800 | ETputs(KTEPS) 593.45\n",
      "Epoch 00109 | Time(s) 0.1824 | Loss 0.5029 | TrainAcc 0.7833 | ValAcc 0.7800 | ETputs(KTEPS) 594.23\n",
      "Epoch 00110 | Time(s) 0.1835 | Loss 0.4750 | TrainAcc 0.7667 | ValAcc 0.7820 | ETputs(KTEPS) 590.69\n",
      "Epoch 00111 | Time(s) 0.1832 | Loss 0.5246 | TrainAcc 0.7667 | ValAcc 0.7800 | ETputs(KTEPS) 591.45\n",
      "Epoch 00112 | Time(s) 0.1829 | Loss 0.5101 | TrainAcc 0.7000 | ValAcc 0.7820 | ETputs(KTEPS) 592.38\n",
      "Epoch 00113 | Time(s) 0.1827 | Loss 0.4680 | TrainAcc 0.8333 | ValAcc 0.7800 | ETputs(KTEPS) 593.17\n",
      "Epoch 00114 | Time(s) 0.1825 | Loss 0.4715 | TrainAcc 0.8500 | ValAcc 0.7780 | ETputs(KTEPS) 593.92\n",
      "Epoch 00115 | Time(s) 0.1822 | Loss 0.4469 | TrainAcc 0.8167 | ValAcc 0.7780 | ETputs(KTEPS) 594.67\n",
      "Epoch 00116 | Time(s) 0.1820 | Loss 0.3795 | TrainAcc 0.8000 | ValAcc 0.7780 | ETputs(KTEPS) 595.42\n",
      "Epoch 00117 | Time(s) 0.1818 | Loss 0.4573 | TrainAcc 0.8000 | ValAcc 0.7820 | ETputs(KTEPS) 596.07\n",
      "Epoch 00118 | Time(s) 0.1824 | Loss 0.5136 | TrainAcc 0.7667 | ValAcc 0.7840 | ETputs(KTEPS) 593.97\n",
      "Epoch 00119 | Time(s) 0.1839 | Loss 0.4490 | TrainAcc 0.8167 | ValAcc 0.7820 | ETputs(KTEPS) 589.40\n",
      "Epoch 00120 | Time(s) 0.1838 | Loss 0.4379 | TrainAcc 0.7500 | ValAcc 0.7880 | ETputs(KTEPS) 589.64\n",
      "Epoch 00121 | Time(s) 0.1837 | Loss 0.4410 | TrainAcc 0.8167 | ValAcc 0.7860 | ETputs(KTEPS) 589.93\n",
      "Epoch 00122 | Time(s) 0.1834 | Loss 0.4614 | TrainAcc 0.8167 | ValAcc 0.7860 | ETputs(KTEPS) 590.75\n",
      "Epoch 00123 | Time(s) 0.1834 | Loss 0.4196 | TrainAcc 0.8167 | ValAcc 0.7860 | ETputs(KTEPS) 590.76\n",
      "Epoch 00124 | Time(s) 0.1834 | Loss 0.4791 | TrainAcc 0.8000 | ValAcc 0.7860 | ETputs(KTEPS) 590.97\n",
      "Epoch 00125 | Time(s) 0.1833 | Loss 0.4853 | TrainAcc 0.7833 | ValAcc 0.7880 | ETputs(KTEPS) 591.08\n",
      "Epoch 00126 | Time(s) 0.1833 | Loss 0.5238 | TrainAcc 0.7500 | ValAcc 0.7920 | ETputs(KTEPS) 591.28\n",
      "Epoch 00127 | Time(s) 0.1832 | Loss 0.4629 | TrainAcc 0.7833 | ValAcc 0.7920 | ETputs(KTEPS) 591.65\n",
      "Epoch 00128 | Time(s) 0.1848 | Loss 0.5032 | TrainAcc 0.7333 | ValAcc 0.7920 | ETputs(KTEPS) 586.40\n",
      "Epoch 00129 | Time(s) 0.1846 | Loss 0.4308 | TrainAcc 0.8000 | ValAcc 0.7920 | ETputs(KTEPS) 587.06\n",
      "Epoch 00130 | Time(s) 0.1846 | Loss 0.5253 | TrainAcc 0.7500 | ValAcc 0.7920 | ETputs(KTEPS) 587.13\n",
      "Epoch 00131 | Time(s) 0.1845 | Loss 0.3972 | TrainAcc 0.8167 | ValAcc 0.7920 | ETputs(KTEPS) 587.32\n",
      "Epoch 00132 | Time(s) 0.1845 | Loss 0.4031 | TrainAcc 0.9000 | ValAcc 0.7900 | ETputs(KTEPS) 587.42\n",
      "Epoch 00133 | Time(s) 0.1843 | Loss 0.3899 | TrainAcc 0.8167 | ValAcc 0.7900 | ETputs(KTEPS) 587.85\n",
      "Epoch 00134 | Time(s) 0.1843 | Loss 0.4096 | TrainAcc 0.8167 | ValAcc 0.7900 | ETputs(KTEPS) 588.03\n",
      "Epoch 00135 | Time(s) 0.1841 | Loss 0.4477 | TrainAcc 0.8167 | ValAcc 0.7920 | ETputs(KTEPS) 588.65\n",
      "Epoch 00136 | Time(s) 0.1839 | Loss 0.4242 | TrainAcc 0.8333 | ValAcc 0.7920 | ETputs(KTEPS) 589.15\n",
      "Epoch 00137 | Time(s) 0.1838 | Loss 0.4308 | TrainAcc 0.8167 | ValAcc 0.7920 | ETputs(KTEPS) 589.58\n",
      "Epoch 00138 | Time(s) 0.1838 | Loss 0.4354 | TrainAcc 0.8000 | ValAcc 0.7920 | ETputs(KTEPS) 589.48\n",
      "Epoch 00139 | Time(s) 0.1838 | Loss 0.4005 | TrainAcc 0.8000 | ValAcc 0.7920 | ETputs(KTEPS) 589.54\n",
      "Epoch 00140 | Time(s) 0.1837 | Loss 0.4778 | TrainAcc 0.8000 | ValAcc 0.7920 | ETputs(KTEPS) 590.05\n",
      "Epoch 00141 | Time(s) 0.1833 | Loss 0.4164 | TrainAcc 0.8667 | ValAcc 0.7920 | ETputs(KTEPS) 591.17\n",
      "Epoch 00142 | Time(s) 0.1830 | Loss 0.3557 | TrainAcc 0.8500 | ValAcc 0.7920 | ETputs(KTEPS) 592.17\n",
      "Epoch 00143 | Time(s) 0.1827 | Loss 0.5408 | TrainAcc 0.7667 | ValAcc 0.7900 | ETputs(KTEPS) 593.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00144 | Time(s) 0.1824 | Loss 0.3478 | TrainAcc 0.8667 | ValAcc 0.7920 | ETputs(KTEPS) 594.11\n",
      "Epoch 00145 | Time(s) 0.1824 | Loss 0.2837 | TrainAcc 0.9000 | ValAcc 0.7900 | ETputs(KTEPS) 594.24\n",
      "Epoch 00146 | Time(s) 0.1824 | Loss 0.3765 | TrainAcc 0.8167 | ValAcc 0.7900 | ETputs(KTEPS) 594.07\n",
      "Epoch 00147 | Time(s) 0.1828 | Loss 0.4163 | TrainAcc 0.7833 | ValAcc 0.7900 | ETputs(KTEPS) 592.86\n",
      "Epoch 00148 | Time(s) 0.1828 | Loss 0.4571 | TrainAcc 0.8333 | ValAcc 0.7920 | ETputs(KTEPS) 592.76\n",
      "Epoch 00149 | Time(s) 0.1829 | Loss 0.4173 | TrainAcc 0.8667 | ValAcc 0.7880 | ETputs(KTEPS) 592.59\n",
      "Epoch 00150 | Time(s) 0.1828 | Loss 0.3357 | TrainAcc 0.9167 | ValAcc 0.7920 | ETputs(KTEPS) 592.71\n",
      "Epoch 00151 | Time(s) 0.1827 | Loss 0.4784 | TrainAcc 0.7667 | ValAcc 0.7920 | ETputs(KTEPS) 593.08\n",
      "Epoch 00152 | Time(s) 0.1825 | Loss 0.4276 | TrainAcc 0.8667 | ValAcc 0.7940 | ETputs(KTEPS) 593.83\n",
      "Epoch 00153 | Time(s) 0.1825 | Loss 0.4515 | TrainAcc 0.8167 | ValAcc 0.7940 | ETputs(KTEPS) 593.92\n",
      "Epoch 00154 | Time(s) 0.1824 | Loss 0.3125 | TrainAcc 0.8833 | ValAcc 0.7940 | ETputs(KTEPS) 594.07\n",
      "Epoch 00155 | Time(s) 0.1824 | Loss 0.3986 | TrainAcc 0.8167 | ValAcc 0.7920 | ETputs(KTEPS) 594.27\n",
      "Epoch 00156 | Time(s) 0.1830 | Loss 0.3780 | TrainAcc 0.8333 | ValAcc 0.7920 | ETputs(KTEPS) 592.31\n",
      "Epoch 00157 | Time(s) 0.1829 | Loss 0.4030 | TrainAcc 0.8500 | ValAcc 0.7920 | ETputs(KTEPS) 592.43\n",
      "Epoch 00158 | Time(s) 0.1829 | Loss 0.4721 | TrainAcc 0.7833 | ValAcc 0.7920 | ETputs(KTEPS) 592.54\n",
      "Epoch 00159 | Time(s) 0.1826 | Loss 0.3336 | TrainAcc 0.8667 | ValAcc 0.7920 | ETputs(KTEPS) 593.35\n",
      "Epoch 00160 | Time(s) 0.1826 | Loss 0.4208 | TrainAcc 0.8000 | ValAcc 0.7920 | ETputs(KTEPS) 593.44\n",
      "Epoch 00161 | Time(s) 0.1825 | Loss 0.3994 | TrainAcc 0.8167 | ValAcc 0.7940 | ETputs(KTEPS) 593.68\n",
      "Epoch 00162 | Time(s) 0.1825 | Loss 0.4455 | TrainAcc 0.7667 | ValAcc 0.7940 | ETputs(KTEPS) 593.76\n",
      "Epoch 00163 | Time(s) 0.1825 | Loss 0.3747 | TrainAcc 0.8167 | ValAcc 0.7920 | ETputs(KTEPS) 593.79\n",
      "Epoch 00164 | Time(s) 0.1823 | Loss 0.3957 | TrainAcc 0.8500 | ValAcc 0.7920 | ETputs(KTEPS) 594.30\n",
      "Epoch 00165 | Time(s) 0.1833 | Loss 0.5118 | TrainAcc 0.7333 | ValAcc 0.7920 | ETputs(KTEPS) 591.18\n",
      "Epoch 00166 | Time(s) 0.1832 | Loss 0.3958 | TrainAcc 0.8500 | ValAcc 0.7920 | ETputs(KTEPS) 591.43\n",
      "Epoch 00167 | Time(s) 0.1831 | Loss 0.4828 | TrainAcc 0.7667 | ValAcc 0.7900 | ETputs(KTEPS) 591.99\n",
      "Epoch 00168 | Time(s) 0.1829 | Loss 0.4298 | TrainAcc 0.8000 | ValAcc 0.7880 | ETputs(KTEPS) 592.43\n",
      "Epoch 00169 | Time(s) 0.1828 | Loss 0.4404 | TrainAcc 0.7833 | ValAcc 0.7860 | ETputs(KTEPS) 592.88\n",
      "Epoch 00170 | Time(s) 0.1826 | Loss 0.4579 | TrainAcc 0.8500 | ValAcc 0.7860 | ETputs(KTEPS) 593.50\n",
      "Epoch 00171 | Time(s) 0.1826 | Loss 0.3783 | TrainAcc 0.8667 | ValAcc 0.7840 | ETputs(KTEPS) 593.55\n",
      "Epoch 00172 | Time(s) 0.1827 | Loss 0.3674 | TrainAcc 0.8833 | ValAcc 0.7840 | ETputs(KTEPS) 592.98\n",
      "Epoch 00173 | Time(s) 0.1827 | Loss 0.4064 | TrainAcc 0.8333 | ValAcc 0.7860 | ETputs(KTEPS) 593.10\n",
      "Epoch 00174 | Time(s) 0.1836 | Loss 0.3680 | TrainAcc 0.8500 | ValAcc 0.7880 | ETputs(KTEPS) 590.37\n",
      "Epoch 00175 | Time(s) 0.1835 | Loss 0.4074 | TrainAcc 0.8833 | ValAcc 0.7880 | ETputs(KTEPS) 590.52\n",
      "Epoch 00176 | Time(s) 0.1834 | Loss 0.3479 | TrainAcc 0.7833 | ValAcc 0.7880 | ETputs(KTEPS) 590.88\n",
      "Epoch 00177 | Time(s) 0.1833 | Loss 0.4163 | TrainAcc 0.8167 | ValAcc 0.7880 | ETputs(KTEPS) 591.06\n",
      "Epoch 00178 | Time(s) 0.1832 | Loss 0.4018 | TrainAcc 0.8167 | ValAcc 0.7880 | ETputs(KTEPS) 591.44\n",
      "Epoch 00179 | Time(s) 0.1831 | Loss 0.3938 | TrainAcc 0.8667 | ValAcc 0.7880 | ETputs(KTEPS) 591.92\n",
      "Epoch 00180 | Time(s) 0.1829 | Loss 0.4403 | TrainAcc 0.8000 | ValAcc 0.7900 | ETputs(KTEPS) 592.34\n",
      "Epoch 00181 | Time(s) 0.1827 | Loss 0.3673 | TrainAcc 0.9167 | ValAcc 0.7880 | ETputs(KTEPS) 593.06\n",
      "Epoch 00182 | Time(s) 0.1825 | Loss 0.4123 | TrainAcc 0.8000 | ValAcc 0.7880 | ETputs(KTEPS) 593.84\n",
      "Epoch 00183 | Time(s) 0.1823 | Loss 0.3834 | TrainAcc 0.8167 | ValAcc 0.7880 | ETputs(KTEPS) 594.59\n",
      "Epoch 00184 | Time(s) 0.1827 | Loss 0.3673 | TrainAcc 0.8500 | ValAcc 0.7900 | ETputs(KTEPS) 593.03\n",
      "Epoch 00185 | Time(s) 0.1827 | Loss 0.4804 | TrainAcc 0.7500 | ValAcc 0.7900 | ETputs(KTEPS) 592.98\n",
      "Epoch 00186 | Time(s) 0.1827 | Loss 0.4198 | TrainAcc 0.8000 | ValAcc 0.7900 | ETputs(KTEPS) 593.19\n",
      "Epoch 00187 | Time(s) 0.1827 | Loss 0.4142 | TrainAcc 0.8167 | ValAcc 0.7900 | ETputs(KTEPS) 593.16\n",
      "Epoch 00188 | Time(s) 0.1827 | Loss 0.3322 | TrainAcc 0.8667 | ValAcc 0.7900 | ETputs(KTEPS) 593.20\n",
      "Epoch 00189 | Time(s) 0.1826 | Loss 0.4707 | TrainAcc 0.7500 | ValAcc 0.7900 | ETputs(KTEPS) 593.35\n",
      "Epoch 00190 | Time(s) 0.1827 | Loss 0.4652 | TrainAcc 0.8000 | ValAcc 0.7880 | ETputs(KTEPS) 593.25\n",
      "Epoch 00191 | Time(s) 0.1826 | Loss 0.4042 | TrainAcc 0.8000 | ValAcc 0.7880 | ETputs(KTEPS) 593.57\n",
      "Epoch 00192 | Time(s) 0.1826 | Loss 0.4361 | TrainAcc 0.8000 | ValAcc 0.7860 | ETputs(KTEPS) 593.59\n",
      "Epoch 00193 | Time(s) 0.1834 | Loss 0.3021 | TrainAcc 0.9167 | ValAcc 0.7860 | ETputs(KTEPS) 590.71\n",
      "Epoch 00194 | Time(s) 0.1834 | Loss 0.3950 | TrainAcc 0.8667 | ValAcc 0.7860 | ETputs(KTEPS) 590.79\n",
      "Epoch 00195 | Time(s) 0.1834 | Loss 0.3863 | TrainAcc 0.8500 | ValAcc 0.7840 | ETputs(KTEPS) 590.72\n",
      "Epoch 00196 | Time(s) 0.1833 | Loss 0.3365 | TrainAcc 0.8500 | ValAcc 0.7860 | ETputs(KTEPS) 591.09\n",
      "Epoch 00197 | Time(s) 0.1833 | Loss 0.5027 | TrainAcc 0.7500 | ValAcc 0.7880 | ETputs(KTEPS) 591.29\n",
      "Epoch 00198 | Time(s) 0.1831 | Loss 0.3291 | TrainAcc 0.8833 | ValAcc 0.7840 | ETputs(KTEPS) 591.73\n",
      "Epoch 00199 | Time(s) 0.1831 | Loss 0.4313 | TrainAcc 0.8333 | ValAcc 0.7840 | ETputs(KTEPS) 591.78\n",
      "\n",
      "Test Accuracy 0.7710\n"
     ]
    }
   ],
   "source": [
    "!../../gcn_text_categorization/venv/bin/python3 train.py --dataset=pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "Namespace(attn_drop=0.6, dataset='citeseer', early_stop=False, epochs=200, fastmode=False, gpu=-1, in_drop=0.6, lr=0.005, negative_slope=0.2, num_heads=8, num_hidden=8, num_layers=1, num_out_heads=1, residual=False, weight_decay=0.0005)\n",
      "Loading from cache failed, re-processing.\n",
      "/4tb/nabarun/nlp/gcn_text_categorization/venv/lib/python3.6/site-packages/dgl/data/citation_graph.py:258: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n",
      "Finished data loading and preprocessing.\n",
      "  NumNodes: 3327\n",
      "  NumEdges: 9228\n",
      "  NumFeats: 3703\n",
      "  NumClasses: 6\n",
      "  NumTrainingSamples: 120\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done saving data into cached files.\n",
      "/4tb/nabarun/nlp/gcn_text_categorization/venv/lib/python3.6/site-packages/dgl/data/utils.py:285: UserWarning: Property dataset.num_labels will be deprecated, please use dataset.num_classes instead.\n",
      "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
      "/4tb/nabarun/nlp/gcn_text_categorization/venv/lib/python3.6/site-packages/dgl/data/utils.py:285: UserWarning: Property dataset.graph will be deprecated, please use dataset[0] instead.\n",
      "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
      "----Data statistics------'\n",
      "      #Edges 9228\n",
      "      #Classes 6\n",
      "      #Train samples 120\n",
      "      #Val samples 500\n",
      "      #Test samples 1000\n",
      "GAT(\n",
      "  (gat_layers): ModuleList(\n",
      "    (0): GATConv(\n",
      "      (fc): Linear(in_features=3703, out_features=64, bias=False)\n",
      "      (feat_drop): Dropout(p=0.6, inplace=False)\n",
      "      (attn_drop): Dropout(p=0.6, inplace=False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (1): GATConv(\n",
      "      (fc): Linear(in_features=64, out_features=6, bias=False)\n",
      "      (feat_drop): Dropout(p=0.6, inplace=False)\n",
      "      (attn_drop): Dropout(p=0.6, inplace=False)\n",
      "      (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "/4tb/nabarun/nlp/gcn_text_categorization/venv/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/4tb/nabarun/nlp/gcn_text_categorization/venv/lib/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Epoch 00000 | Time(s) nan | Loss 1.7947 | TrainAcc 0.2083 | ValAcc 0.4020 | ETputs(KTEPS) nan\n",
      "Epoch 00001 | Time(s) nan | Loss 1.7842 | TrainAcc 0.2667 | ValAcc 0.5080 | ETputs(KTEPS) nan\n",
      "Epoch 00002 | Time(s) nan | Loss 1.7805 | TrainAcc 0.3500 | ValAcc 0.5860 | ETputs(KTEPS) nan\n",
      "Epoch 00003 | Time(s) 0.1338 | Loss 1.7721 | TrainAcc 0.4333 | ValAcc 0.6280 | ETputs(KTEPS) 92.89\n",
      "Epoch 00004 | Time(s) 0.1352 | Loss 1.7667 | TrainAcc 0.5167 | ValAcc 0.6340 | ETputs(KTEPS) 91.93\n",
      "Epoch 00005 | Time(s) 0.1222 | Loss 1.7582 | TrainAcc 0.5667 | ValAcc 0.6500 | ETputs(KTEPS) 101.72\n",
      "Epoch 00006 | Time(s) 0.1184 | Loss 1.7576 | TrainAcc 0.5167 | ValAcc 0.6620 | ETputs(KTEPS) 104.99\n",
      "Epoch 00007 | Time(s) 0.1209 | Loss 1.7301 | TrainAcc 0.6250 | ValAcc 0.6600 | ETputs(KTEPS) 102.82\n",
      "Epoch 00008 | Time(s) 0.1225 | Loss 1.7321 | TrainAcc 0.6500 | ValAcc 0.6800 | ETputs(KTEPS) 101.50\n",
      "Epoch 00009 | Time(s) 0.1193 | Loss 1.7202 | TrainAcc 0.7167 | ValAcc 0.6800 | ETputs(KTEPS) 104.19\n",
      "Epoch 00010 | Time(s) 0.1259 | Loss 1.7253 | TrainAcc 0.7250 | ValAcc 0.6840 | ETputs(KTEPS) 98.77\n",
      "Epoch 00011 | Time(s) 0.1247 | Loss 1.7117 | TrainAcc 0.6583 | ValAcc 0.6860 | ETputs(KTEPS) 99.72\n",
      "Epoch 00012 | Time(s) 0.1235 | Loss 1.7156 | TrainAcc 0.6000 | ValAcc 0.6880 | ETputs(KTEPS) 100.64\n",
      "Epoch 00013 | Time(s) 0.1337 | Loss 1.6937 | TrainAcc 0.6583 | ValAcc 0.6960 | ETputs(KTEPS) 92.97\n",
      "Epoch 00014 | Time(s) 0.1336 | Loss 1.6856 | TrainAcc 0.6583 | ValAcc 0.6940 | ETputs(KTEPS) 93.01\n",
      "Epoch 00015 | Time(s) 0.1306 | Loss 1.6789 | TrainAcc 0.7083 | ValAcc 0.6980 | ETputs(KTEPS) 95.21\n",
      "Epoch 00016 | Time(s) 0.1284 | Loss 1.6651 | TrainAcc 0.6750 | ValAcc 0.7000 | ETputs(KTEPS) 96.79\n",
      "Epoch 00017 | Time(s) 0.1283 | Loss 1.6463 | TrainAcc 0.7417 | ValAcc 0.7040 | ETputs(KTEPS) 96.91\n",
      "Epoch 00018 | Time(s) 0.1282 | Loss 1.6124 | TrainAcc 0.7667 | ValAcc 0.7020 | ETputs(KTEPS) 96.95\n",
      "Epoch 00019 | Time(s) 0.1264 | Loss 1.6345 | TrainAcc 0.7500 | ValAcc 0.7080 | ETputs(KTEPS) 98.34\n",
      "Epoch 00020 | Time(s) 0.1258 | Loss 1.6411 | TrainAcc 0.7000 | ValAcc 0.7080 | ETputs(KTEPS) 98.81\n",
      "Epoch 00021 | Time(s) 0.1263 | Loss 1.6062 | TrainAcc 0.6917 | ValAcc 0.7080 | ETputs(KTEPS) 98.46\n",
      "Epoch 00022 | Time(s) 0.1265 | Loss 1.6116 | TrainAcc 0.7083 | ValAcc 0.7100 | ETputs(KTEPS) 98.24\n",
      "Epoch 00023 | Time(s) 0.1249 | Loss 1.6279 | TrainAcc 0.6583 | ValAcc 0.7100 | ETputs(KTEPS) 99.57\n",
      "Epoch 00024 | Time(s) 0.1236 | Loss 1.5964 | TrainAcc 0.7500 | ValAcc 0.7040 | ETputs(KTEPS) 100.53\n",
      "Epoch 00025 | Time(s) 0.1239 | Loss 1.5681 | TrainAcc 0.7333 | ValAcc 0.7060 | ETputs(KTEPS) 100.35\n",
      "Epoch 00026 | Time(s) 0.1242 | Loss 1.5443 | TrainAcc 0.7083 | ValAcc 0.7060 | ETputs(KTEPS) 100.09\n",
      "Epoch 00027 | Time(s) 0.1248 | Loss 1.5411 | TrainAcc 0.7500 | ValAcc 0.7060 | ETputs(KTEPS) 99.63\n",
      "Epoch 00028 | Time(s) 0.1267 | Loss 1.5505 | TrainAcc 0.6750 | ValAcc 0.7060 | ETputs(KTEPS) 98.13\n",
      "Epoch 00029 | Time(s) 0.1271 | Loss 1.5551 | TrainAcc 0.6833 | ValAcc 0.7040 | ETputs(KTEPS) 97.77\n",
      "Epoch 00030 | Time(s) 0.1275 | Loss 1.5286 | TrainAcc 0.7417 | ValAcc 0.7040 | ETputs(KTEPS) 97.50\n",
      "Epoch 00031 | Time(s) 0.1277 | Loss 1.5028 | TrainAcc 0.7167 | ValAcc 0.7040 | ETputs(KTEPS) 97.35\n",
      "Epoch 00032 | Time(s) 0.1268 | Loss 1.5071 | TrainAcc 0.7583 | ValAcc 0.7060 | ETputs(KTEPS) 98.01\n",
      "Epoch 00033 | Time(s) 0.1258 | Loss 1.4491 | TrainAcc 0.7583 | ValAcc 0.7060 | ETputs(KTEPS) 98.80\n",
      "Epoch 00034 | Time(s) 0.1268 | Loss 1.4554 | TrainAcc 0.7250 | ValAcc 0.7080 | ETputs(KTEPS) 98.02\n",
      "Epoch 00035 | Time(s) 0.1263 | Loss 1.4849 | TrainAcc 0.6833 | ValAcc 0.7120 | ETputs(KTEPS) 98.44\n",
      "Epoch 00036 | Time(s) 0.1266 | Loss 1.4783 | TrainAcc 0.7000 | ValAcc 0.7160 | ETputs(KTEPS) 98.19\n",
      "Epoch 00037 | Time(s) 0.1272 | Loss 1.4553 | TrainAcc 0.7333 | ValAcc 0.7120 | ETputs(KTEPS) 97.73\n",
      "Epoch 00038 | Time(s) 0.1263 | Loss 1.4582 | TrainAcc 0.7500 | ValAcc 0.7140 | ETputs(KTEPS) 98.46\n",
      "Epoch 00039 | Time(s) 0.1255 | Loss 1.3473 | TrainAcc 0.7333 | ValAcc 0.7120 | ETputs(KTEPS) 99.03\n",
      "Epoch 00040 | Time(s) 0.1252 | Loss 1.4055 | TrainAcc 0.7500 | ValAcc 0.7140 | ETputs(KTEPS) 99.31\n",
      "Epoch 00041 | Time(s) 0.1262 | Loss 1.4382 | TrainAcc 0.7167 | ValAcc 0.7140 | ETputs(KTEPS) 98.47\n",
      "Epoch 00042 | Time(s) 0.1260 | Loss 1.4234 | TrainAcc 0.6500 | ValAcc 0.7140 | ETputs(KTEPS) 98.68\n",
      "Epoch 00043 | Time(s) 0.1261 | Loss 1.3960 | TrainAcc 0.7083 | ValAcc 0.7120 | ETputs(KTEPS) 98.59\n",
      "Epoch 00044 | Time(s) 0.1262 | Loss 1.3624 | TrainAcc 0.6917 | ValAcc 0.7100 | ETputs(KTEPS) 98.52\n",
      "Epoch 00045 | Time(s) 0.1254 | Loss 1.3898 | TrainAcc 0.7167 | ValAcc 0.7100 | ETputs(KTEPS) 99.10\n",
      "Epoch 00046 | Time(s) 0.1250 | Loss 1.3317 | TrainAcc 0.7667 | ValAcc 0.7040 | ETputs(KTEPS) 99.47\n",
      "Epoch 00047 | Time(s) 0.1247 | Loss 1.3316 | TrainAcc 0.7167 | ValAcc 0.7040 | ETputs(KTEPS) 99.66\n",
      "Epoch 00048 | Time(s) 0.1250 | Loss 1.3445 | TrainAcc 0.7083 | ValAcc 0.7040 | ETputs(KTEPS) 99.42\n",
      "Epoch 00049 | Time(s) 0.1253 | Loss 1.3525 | TrainAcc 0.6917 | ValAcc 0.7040 | ETputs(KTEPS) 99.22\n",
      "Epoch 00050 | Time(s) 0.1254 | Loss 1.3549 | TrainAcc 0.6583 | ValAcc 0.7040 | ETputs(KTEPS) 99.10\n",
      "Epoch 00051 | Time(s) 0.1249 | Loss 1.3000 | TrainAcc 0.7417 | ValAcc 0.7040 | ETputs(KTEPS) 99.54\n",
      "Epoch 00052 | Time(s) 0.1245 | Loss 1.3182 | TrainAcc 0.6917 | ValAcc 0.7060 | ETputs(KTEPS) 99.84\n",
      "Epoch 00053 | Time(s) 0.1242 | Loss 1.2937 | TrainAcc 0.7000 | ValAcc 0.7060 | ETputs(KTEPS) 100.08\n",
      "Epoch 00054 | Time(s) 0.1243 | Loss 1.2191 | TrainAcc 0.7417 | ValAcc 0.7080 | ETputs(KTEPS) 100.01\n",
      "Epoch 00055 | Time(s) 0.1238 | Loss 1.2363 | TrainAcc 0.7583 | ValAcc 0.7080 | ETputs(KTEPS) 100.45\n",
      "Epoch 00056 | Time(s) 0.1266 | Loss 1.2383 | TrainAcc 0.7250 | ValAcc 0.7080 | ETputs(KTEPS) 98.15\n",
      "Epoch 00057 | Time(s) 0.1268 | Loss 1.2642 | TrainAcc 0.7000 | ValAcc 0.7060 | ETputs(KTEPS) 98.05\n",
      "Epoch 00058 | Time(s) 0.1271 | Loss 1.2616 | TrainAcc 0.7333 | ValAcc 0.7120 | ETputs(KTEPS) 97.79\n",
      "Epoch 00059 | Time(s) 0.1267 | Loss 1.2074 | TrainAcc 0.7417 | ValAcc 0.7140 | ETputs(KTEPS) 98.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00060 | Time(s) 0.1261 | Loss 1.2118 | TrainAcc 0.7250 | ValAcc 0.7140 | ETputs(KTEPS) 98.60\n",
      "Epoch 00061 | Time(s) 0.1260 | Loss 1.2542 | TrainAcc 0.7333 | ValAcc 0.7100 | ETputs(KTEPS) 98.64\n",
      "Epoch 00062 | Time(s) 0.1262 | Loss 1.2100 | TrainAcc 0.7083 | ValAcc 0.7120 | ETputs(KTEPS) 98.50\n",
      "Epoch 00063 | Time(s) 0.1257 | Loss 1.2381 | TrainAcc 0.6750 | ValAcc 0.7100 | ETputs(KTEPS) 98.89\n",
      "Epoch 00064 | Time(s) 0.1254 | Loss 1.1474 | TrainAcc 0.8000 | ValAcc 0.7060 | ETputs(KTEPS) 99.10\n",
      "Epoch 00065 | Time(s) 0.1255 | Loss 1.2000 | TrainAcc 0.7333 | ValAcc 0.7060 | ETputs(KTEPS) 99.07\n",
      "Epoch 00066 | Time(s) 0.1257 | Loss 1.1490 | TrainAcc 0.7250 | ValAcc 0.7060 | ETputs(KTEPS) 98.90\n",
      "Epoch 00067 | Time(s) 0.1254 | Loss 1.1798 | TrainAcc 0.7333 | ValAcc 0.7060 | ETputs(KTEPS) 99.16\n",
      "Epoch 00068 | Time(s) 0.1250 | Loss 1.1440 | TrainAcc 0.7417 | ValAcc 0.7060 | ETputs(KTEPS) 99.42\n",
      "Epoch 00069 | Time(s) 0.1247 | Loss 1.0967 | TrainAcc 0.7833 | ValAcc 0.7060 | ETputs(KTEPS) 99.66\n",
      "Epoch 00070 | Time(s) 0.1267 | Loss 1.1531 | TrainAcc 0.7250 | ValAcc 0.7080 | ETputs(KTEPS) 98.14\n",
      "Epoch 00071 | Time(s) 0.1266 | Loss 1.1879 | TrainAcc 0.6833 | ValAcc 0.7080 | ETputs(KTEPS) 98.18\n",
      "Epoch 00072 | Time(s) 0.1267 | Loss 1.1352 | TrainAcc 0.7167 | ValAcc 0.7080 | ETputs(KTEPS) 98.09\n",
      "Epoch 00073 | Time(s) 0.1269 | Loss 1.0380 | TrainAcc 0.8000 | ValAcc 0.7100 | ETputs(KTEPS) 97.97\n",
      "Epoch 00074 | Time(s) 0.1269 | Loss 1.1145 | TrainAcc 0.7917 | ValAcc 0.7080 | ETputs(KTEPS) 97.99\n",
      "Epoch 00075 | Time(s) 0.1265 | Loss 1.0555 | TrainAcc 0.7667 | ValAcc 0.7080 | ETputs(KTEPS) 98.28\n",
      "Epoch 00076 | Time(s) 0.1264 | Loss 1.0953 | TrainAcc 0.7250 | ValAcc 0.7100 | ETputs(KTEPS) 98.36\n",
      "Epoch 00077 | Time(s) 0.1265 | Loss 1.0314 | TrainAcc 0.7583 | ValAcc 0.7120 | ETputs(KTEPS) 98.28\n",
      "Epoch 00078 | Time(s) 0.1267 | Loss 1.0288 | TrainAcc 0.7833 | ValAcc 0.7120 | ETputs(KTEPS) 98.12\n",
      "Epoch 00079 | Time(s) 0.1275 | Loss 1.0913 | TrainAcc 0.7167 | ValAcc 0.7120 | ETputs(KTEPS) 97.47\n",
      "Epoch 00080 | Time(s) 0.1277 | Loss 1.1684 | TrainAcc 0.6667 | ValAcc 0.7100 | ETputs(KTEPS) 97.34\n",
      "Epoch 00081 | Time(s) 0.1280 | Loss 1.0191 | TrainAcc 0.8000 | ValAcc 0.7140 | ETputs(KTEPS) 97.11\n",
      "Epoch 00082 | Time(s) 0.1280 | Loss 0.9974 | TrainAcc 0.7917 | ValAcc 0.7140 | ETputs(KTEPS) 97.14\n",
      "Epoch 00083 | Time(s) 0.1298 | Loss 1.0823 | TrainAcc 0.7250 | ValAcc 0.7140 | ETputs(KTEPS) 95.75\n",
      "Epoch 00084 | Time(s) 0.1294 | Loss 1.1338 | TrainAcc 0.6667 | ValAcc 0.7100 | ETputs(KTEPS) 96.06\n",
      "Epoch 00085 | Time(s) 0.1291 | Loss 0.9305 | TrainAcc 0.8167 | ValAcc 0.7100 | ETputs(KTEPS) 96.33\n",
      "Epoch 00086 | Time(s) 0.1291 | Loss 0.9752 | TrainAcc 0.7583 | ValAcc 0.7080 | ETputs(KTEPS) 96.32\n",
      "Epoch 00087 | Time(s) 0.1290 | Loss 1.0342 | TrainAcc 0.6917 | ValAcc 0.7080 | ETputs(KTEPS) 96.38\n",
      "Epoch 00088 | Time(s) 0.1286 | Loss 1.0678 | TrainAcc 0.7333 | ValAcc 0.7060 | ETputs(KTEPS) 96.69\n",
      "Epoch 00089 | Time(s) 0.1283 | Loss 0.9970 | TrainAcc 0.7583 | ValAcc 0.7060 | ETputs(KTEPS) 96.90\n",
      "Epoch 00090 | Time(s) 0.1283 | Loss 0.8925 | TrainAcc 0.8167 | ValAcc 0.7060 | ETputs(KTEPS) 96.91\n",
      "Epoch 00091 | Time(s) 0.1283 | Loss 0.9775 | TrainAcc 0.7583 | ValAcc 0.7060 | ETputs(KTEPS) 96.87\n",
      "Epoch 00092 | Time(s) 0.1280 | Loss 1.0251 | TrainAcc 0.7333 | ValAcc 0.7060 | ETputs(KTEPS) 97.15\n",
      "Epoch 00093 | Time(s) 0.1276 | Loss 1.0070 | TrainAcc 0.7417 | ValAcc 0.7060 | ETputs(KTEPS) 97.41\n",
      "Epoch 00094 | Time(s) 0.1276 | Loss 0.9472 | TrainAcc 0.7333 | ValAcc 0.7080 | ETputs(KTEPS) 97.42\n",
      "Epoch 00095 | Time(s) 0.1277 | Loss 0.9653 | TrainAcc 0.8000 | ValAcc 0.7100 | ETputs(KTEPS) 97.35\n",
      "Epoch 00096 | Time(s) 0.1277 | Loss 0.9516 | TrainAcc 0.7750 | ValAcc 0.7100 | ETputs(KTEPS) 97.34\n",
      "Epoch 00097 | Time(s) 0.1277 | Loss 0.8976 | TrainAcc 0.7917 | ValAcc 0.7100 | ETputs(KTEPS) 97.33\n",
      "Epoch 00098 | Time(s) 0.1280 | Loss 0.9432 | TrainAcc 0.7750 | ValAcc 0.7060 | ETputs(KTEPS) 97.12\n",
      "Epoch 00099 | Time(s) 0.1280 | Loss 1.0119 | TrainAcc 0.6917 | ValAcc 0.7060 | ETputs(KTEPS) 97.10\n",
      "Epoch 00100 | Time(s) 0.1277 | Loss 0.9758 | TrainAcc 0.7667 | ValAcc 0.7040 | ETputs(KTEPS) 97.34\n",
      "Epoch 00101 | Time(s) 0.1273 | Loss 0.9736 | TrainAcc 0.7500 | ValAcc 0.7020 | ETputs(KTEPS) 97.65\n",
      "Epoch 00102 | Time(s) 0.1273 | Loss 0.9541 | TrainAcc 0.7667 | ValAcc 0.7020 | ETputs(KTEPS) 97.69\n",
      "Epoch 00103 | Time(s) 0.1270 | Loss 0.8519 | TrainAcc 0.7750 | ValAcc 0.7020 | ETputs(KTEPS) 97.86\n",
      "Epoch 00104 | Time(s) 0.1267 | Loss 0.9720 | TrainAcc 0.6917 | ValAcc 0.7020 | ETputs(KTEPS) 98.08\n",
      "Epoch 00105 | Time(s) 0.1264 | Loss 0.9659 | TrainAcc 0.7583 | ValAcc 0.7020 | ETputs(KTEPS) 98.33\n",
      "Epoch 00106 | Time(s) 0.1264 | Loss 0.9434 | TrainAcc 0.8083 | ValAcc 0.7020 | ETputs(KTEPS) 98.37\n",
      "Epoch 00107 | Time(s) 0.1264 | Loss 0.9631 | TrainAcc 0.7167 | ValAcc 0.7020 | ETputs(KTEPS) 98.33\n",
      "Epoch 00108 | Time(s) 0.1261 | Loss 0.9642 | TrainAcc 0.7250 | ValAcc 0.7020 | ETputs(KTEPS) 98.56\n",
      "Epoch 00109 | Time(s) 0.1258 | Loss 0.9685 | TrainAcc 0.7333 | ValAcc 0.7020 | ETputs(KTEPS) 98.81\n",
      "Epoch 00110 | Time(s) 0.1257 | Loss 0.9034 | TrainAcc 0.7833 | ValAcc 0.7020 | ETputs(KTEPS) 98.89\n",
      "Epoch 00111 | Time(s) 0.1258 | Loss 0.8795 | TrainAcc 0.8000 | ValAcc 0.7020 | ETputs(KTEPS) 98.84\n",
      "Epoch 00112 | Time(s) 0.1264 | Loss 0.8927 | TrainAcc 0.7750 | ValAcc 0.7040 | ETputs(KTEPS) 98.33\n",
      "Epoch 00113 | Time(s) 0.1262 | Loss 0.9772 | TrainAcc 0.7083 | ValAcc 0.7040 | ETputs(KTEPS) 98.51\n",
      "Epoch 00114 | Time(s) 0.1260 | Loss 0.8821 | TrainAcc 0.7750 | ValAcc 0.7020 | ETputs(KTEPS) 98.65\n",
      "Epoch 00115 | Time(s) 0.1260 | Loss 0.9589 | TrainAcc 0.6833 | ValAcc 0.7020 | ETputs(KTEPS) 98.65\n",
      "Epoch 00116 | Time(s) 0.1260 | Loss 0.9882 | TrainAcc 0.6917 | ValAcc 0.7020 | ETputs(KTEPS) 98.63\n",
      "Epoch 00117 | Time(s) 0.1258 | Loss 0.9286 | TrainAcc 0.7417 | ValAcc 0.7020 | ETputs(KTEPS) 98.84\n",
      "Epoch 00118 | Time(s) 0.1256 | Loss 0.9185 | TrainAcc 0.7167 | ValAcc 0.7000 | ETputs(KTEPS) 98.94\n",
      "Epoch 00119 | Time(s) 0.1257 | Loss 0.8876 | TrainAcc 0.7167 | ValAcc 0.7000 | ETputs(KTEPS) 98.87\n",
      "Epoch 00120 | Time(s) 0.1258 | Loss 0.7989 | TrainAcc 0.7583 | ValAcc 0.6960 | ETputs(KTEPS) 98.78\n",
      "Epoch 00121 | Time(s) 0.1256 | Loss 0.9339 | TrainAcc 0.7417 | ValAcc 0.6960 | ETputs(KTEPS) 98.95\n",
      "Epoch 00122 | Time(s) 0.1255 | Loss 0.8848 | TrainAcc 0.7417 | ValAcc 0.6960 | ETputs(KTEPS) 99.09\n",
      "Epoch 00123 | Time(s) 0.1257 | Loss 0.8128 | TrainAcc 0.7500 | ValAcc 0.6960 | ETputs(KTEPS) 98.92\n",
      "Epoch 00124 | Time(s) 0.1257 | Loss 0.8753 | TrainAcc 0.7583 | ValAcc 0.6960 | ETputs(KTEPS) 98.92\n",
      "Epoch 00125 | Time(s) 0.1257 | Loss 0.8776 | TrainAcc 0.8000 | ValAcc 0.7000 | ETputs(KTEPS) 98.92\n",
      "Epoch 00126 | Time(s) 0.1254 | Loss 0.8555 | TrainAcc 0.7750 | ValAcc 0.7000 | ETputs(KTEPS) 99.13\n",
      "Epoch 00127 | Time(s) 0.1261 | Loss 0.8267 | TrainAcc 0.7750 | ValAcc 0.7020 | ETputs(KTEPS) 98.62\n",
      "Epoch 00128 | Time(s) 0.1261 | Loss 0.9548 | TrainAcc 0.7083 | ValAcc 0.7020 | ETputs(KTEPS) 98.56\n",
      "Epoch 00129 | Time(s) 0.1260 | Loss 0.9410 | TrainAcc 0.7250 | ValAcc 0.7020 | ETputs(KTEPS) 98.69\n",
      "Epoch 00130 | Time(s) 0.1258 | Loss 0.8685 | TrainAcc 0.7417 | ValAcc 0.7000 | ETputs(KTEPS) 98.80\n",
      "Epoch 00131 | Time(s) 0.1259 | Loss 0.8991 | TrainAcc 0.7083 | ValAcc 0.7000 | ETputs(KTEPS) 98.72\n",
      "Epoch 00132 | Time(s) 0.1259 | Loss 0.9605 | TrainAcc 0.7167 | ValAcc 0.7000 | ETputs(KTEPS) 98.72\n",
      "Epoch 00133 | Time(s) 0.1257 | Loss 0.8889 | TrainAcc 0.7500 | ValAcc 0.6980 | ETputs(KTEPS) 98.91\n",
      "Epoch 00134 | Time(s) 0.1255 | Loss 0.8408 | TrainAcc 0.7750 | ValAcc 0.6980 | ETputs(KTEPS) 99.03\n",
      "Epoch 00135 | Time(s) 0.1255 | Loss 0.8144 | TrainAcc 0.7750 | ValAcc 0.7000 | ETputs(KTEPS) 99.05\n",
      "Epoch 00136 | Time(s) 0.1256 | Loss 0.8558 | TrainAcc 0.7417 | ValAcc 0.7020 | ETputs(KTEPS) 98.95\n",
      "Epoch 00137 | Time(s) 0.1256 | Loss 0.8477 | TrainAcc 0.7500 | ValAcc 0.7020 | ETputs(KTEPS) 98.94\n",
      "Epoch 00138 | Time(s) 0.1255 | Loss 0.9446 | TrainAcc 0.6833 | ValAcc 0.6980 | ETputs(KTEPS) 99.09\n",
      "Epoch 00139 | Time(s) 0.1253 | Loss 0.8211 | TrainAcc 0.8083 | ValAcc 0.6980 | ETputs(KTEPS) 99.21\n",
      "Epoch 00140 | Time(s) 0.1252 | Loss 0.8142 | TrainAcc 0.7833 | ValAcc 0.6980 | ETputs(KTEPS) 99.31\n",
      "Epoch 00141 | Time(s) 0.1262 | Loss 0.8840 | TrainAcc 0.7167 | ValAcc 0.6960 | ETputs(KTEPS) 98.48\n",
      "Epoch 00142 | Time(s) 0.1260 | Loss 0.8596 | TrainAcc 0.7667 | ValAcc 0.6960 | ETputs(KTEPS) 98.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00143 | Time(s) 0.1259 | Loss 0.8407 | TrainAcc 0.8000 | ValAcc 0.6960 | ETputs(KTEPS) 98.71\n",
      "Epoch 00144 | Time(s) 0.1259 | Loss 0.8443 | TrainAcc 0.8000 | ValAcc 0.6980 | ETputs(KTEPS) 98.71\n",
      "Epoch 00145 | Time(s) 0.1259 | Loss 0.9849 | TrainAcc 0.6667 | ValAcc 0.6980 | ETputs(KTEPS) 98.71\n",
      "Epoch 00146 | Time(s) 0.1258 | Loss 0.8487 | TrainAcc 0.7500 | ValAcc 0.6980 | ETputs(KTEPS) 98.85\n",
      "Epoch 00147 | Time(s) 0.1256 | Loss 0.7684 | TrainAcc 0.7917 | ValAcc 0.6980 | ETputs(KTEPS) 99.01\n",
      "Epoch 00148 | Time(s) 0.1255 | Loss 0.7619 | TrainAcc 0.8083 | ValAcc 0.6960 | ETputs(KTEPS) 99.02\n",
      "Epoch 00149 | Time(s) 0.1258 | Loss 0.8477 | TrainAcc 0.7333 | ValAcc 0.6960 | ETputs(KTEPS) 98.84\n",
      "Epoch 00150 | Time(s) 0.1260 | Loss 0.8600 | TrainAcc 0.7250 | ValAcc 0.6960 | ETputs(KTEPS) 98.69\n",
      "Epoch 00151 | Time(s) 0.1257 | Loss 0.7708 | TrainAcc 0.7917 | ValAcc 0.6980 | ETputs(KTEPS) 98.88\n",
      "Epoch 00152 | Time(s) 0.1255 | Loss 0.8756 | TrainAcc 0.7750 | ValAcc 0.6980 | ETputs(KTEPS) 99.01\n",
      "Epoch 00153 | Time(s) 0.1255 | Loss 0.7496 | TrainAcc 0.7750 | ValAcc 0.6980 | ETputs(KTEPS) 99.01\n",
      "Epoch 00154 | Time(s) 0.1256 | Loss 0.7924 | TrainAcc 0.7667 | ValAcc 0.6980 | ETputs(KTEPS) 99.01\n",
      "Epoch 00155 | Time(s) 0.1263 | Loss 0.8326 | TrainAcc 0.7333 | ValAcc 0.6980 | ETputs(KTEPS) 98.41\n",
      "Epoch 00156 | Time(s) 0.1262 | Loss 0.8313 | TrainAcc 0.7583 | ValAcc 0.6960 | ETputs(KTEPS) 98.52\n",
      "Epoch 00157 | Time(s) 0.1262 | Loss 0.8068 | TrainAcc 0.7917 | ValAcc 0.6960 | ETputs(KTEPS) 98.49\n",
      "Epoch 00158 | Time(s) 0.1261 | Loss 0.8696 | TrainAcc 0.7083 | ValAcc 0.6980 | ETputs(KTEPS) 98.61\n",
      "Epoch 00159 | Time(s) 0.1259 | Loss 0.8097 | TrainAcc 0.7833 | ValAcc 0.6940 | ETputs(KTEPS) 98.76\n",
      "Epoch 00160 | Time(s) 0.1257 | Loss 0.8222 | TrainAcc 0.7833 | ValAcc 0.6940 | ETputs(KTEPS) 98.88\n",
      "Epoch 00161 | Time(s) 0.1258 | Loss 0.7537 | TrainAcc 0.7667 | ValAcc 0.6920 | ETputs(KTEPS) 98.85\n",
      "Epoch 00162 | Time(s) 0.1258 | Loss 0.8084 | TrainAcc 0.7583 | ValAcc 0.6920 | ETputs(KTEPS) 98.79\n",
      "Epoch 00163 | Time(s) 0.1257 | Loss 0.8594 | TrainAcc 0.7500 | ValAcc 0.6920 | ETputs(KTEPS) 98.92\n",
      "Epoch 00164 | Time(s) 0.1255 | Loss 0.8801 | TrainAcc 0.6917 | ValAcc 0.6920 | ETputs(KTEPS) 99.04\n",
      "Epoch 00165 | Time(s) 0.1256 | Loss 0.7349 | TrainAcc 0.8083 | ValAcc 0.6920 | ETputs(KTEPS) 98.99\n",
      "Epoch 00166 | Time(s) 0.1257 | Loss 0.8015 | TrainAcc 0.7750 | ValAcc 0.6920 | ETputs(KTEPS) 98.92\n",
      "Epoch 00167 | Time(s) 0.1257 | Loss 0.8075 | TrainAcc 0.7833 | ValAcc 0.6940 | ETputs(KTEPS) 98.90\n",
      "Epoch 00168 | Time(s) 0.1255 | Loss 0.7668 | TrainAcc 0.7833 | ValAcc 0.6980 | ETputs(KTEPS) 99.01\n",
      "Epoch 00169 | Time(s) 0.1264 | Loss 0.7552 | TrainAcc 0.7917 | ValAcc 0.7000 | ETputs(KTEPS) 98.32\n",
      "Epoch 00170 | Time(s) 0.1264 | Loss 0.8302 | TrainAcc 0.7333 | ValAcc 0.7000 | ETputs(KTEPS) 98.36\n",
      "Epoch 00171 | Time(s) 0.1265 | Loss 0.7789 | TrainAcc 0.7833 | ValAcc 0.7000 | ETputs(KTEPS) 98.29\n",
      "Epoch 00172 | Time(s) 0.1265 | Loss 0.7105 | TrainAcc 0.8083 | ValAcc 0.7000 | ETputs(KTEPS) 98.26\n",
      "Epoch 00173 | Time(s) 0.1263 | Loss 0.7895 | TrainAcc 0.8167 | ValAcc 0.7000 | ETputs(KTEPS) 98.41\n",
      "Epoch 00174 | Time(s) 0.1262 | Loss 0.7111 | TrainAcc 0.8333 | ValAcc 0.7020 | ETputs(KTEPS) 98.46\n",
      "Epoch 00175 | Time(s) 0.1263 | Loss 0.7518 | TrainAcc 0.7417 | ValAcc 0.7000 | ETputs(KTEPS) 98.43\n",
      "Epoch 00176 | Time(s) 0.1263 | Loss 0.8047 | TrainAcc 0.7500 | ValAcc 0.7000 | ETputs(KTEPS) 98.39\n",
      "Epoch 00177 | Time(s) 0.1262 | Loss 0.6989 | TrainAcc 0.8500 | ValAcc 0.7000 | ETputs(KTEPS) 98.54\n",
      "Epoch 00178 | Time(s) 0.1260 | Loss 0.6350 | TrainAcc 0.8500 | ValAcc 0.7000 | ETputs(KTEPS) 98.67\n",
      "Epoch 00179 | Time(s) 0.1261 | Loss 0.6915 | TrainAcc 0.8333 | ValAcc 0.7000 | ETputs(KTEPS) 98.61\n",
      "Epoch 00180 | Time(s) 0.1261 | Loss 0.7502 | TrainAcc 0.7750 | ValAcc 0.7000 | ETputs(KTEPS) 98.58\n",
      "Epoch 00181 | Time(s) 0.1259 | Loss 0.7881 | TrainAcc 0.7833 | ValAcc 0.7000 | ETputs(KTEPS) 98.71\n",
      "Epoch 00182 | Time(s) 0.1258 | Loss 0.8149 | TrainAcc 0.7583 | ValAcc 0.7000 | ETputs(KTEPS) 98.81\n",
      "Epoch 00183 | Time(s) 0.1266 | Loss 0.7836 | TrainAcc 0.7583 | ValAcc 0.7040 | ETputs(KTEPS) 98.21\n",
      "Epoch 00184 | Time(s) 0.1264 | Loss 0.7815 | TrainAcc 0.7833 | ValAcc 0.7020 | ETputs(KTEPS) 98.31\n",
      "Epoch 00185 | Time(s) 0.1263 | Loss 0.7532 | TrainAcc 0.7750 | ValAcc 0.7040 | ETputs(KTEPS) 98.39\n",
      "Epoch 00186 | Time(s) 0.1264 | Loss 0.7260 | TrainAcc 0.7667 | ValAcc 0.7040 | ETputs(KTEPS) 98.35\n",
      "Epoch 00187 | Time(s) 0.1264 | Loss 0.7591 | TrainAcc 0.7917 | ValAcc 0.7040 | ETputs(KTEPS) 98.31\n",
      "Epoch 00188 | Time(s) 0.1264 | Loss 0.7027 | TrainAcc 0.8583 | ValAcc 0.7040 | ETputs(KTEPS) 98.31\n",
      "Epoch 00189 | Time(s) 0.1263 | Loss 0.7683 | TrainAcc 0.7917 | ValAcc 0.7060 | ETputs(KTEPS) 98.45\n",
      "Epoch 00190 | Time(s) 0.1262 | Loss 0.7112 | TrainAcc 0.8250 | ValAcc 0.7060 | ETputs(KTEPS) 98.52\n",
      "Epoch 00191 | Time(s) 0.1261 | Loss 0.7391 | TrainAcc 0.7583 | ValAcc 0.7040 | ETputs(KTEPS) 98.60\n",
      "Epoch 00192 | Time(s) 0.1261 | Loss 0.8850 | TrainAcc 0.7333 | ValAcc 0.7040 | ETputs(KTEPS) 98.58\n",
      "Epoch 00193 | Time(s) 0.1260 | Loss 0.8055 | TrainAcc 0.7583 | ValAcc 0.7040 | ETputs(KTEPS) 98.68\n",
      "Epoch 00194 | Time(s) 0.1259 | Loss 0.7860 | TrainAcc 0.7750 | ValAcc 0.7040 | ETputs(KTEPS) 98.77\n",
      "Epoch 00195 | Time(s) 0.1259 | Loss 0.6929 | TrainAcc 0.7917 | ValAcc 0.7040 | ETputs(KTEPS) 98.73\n",
      "Epoch 00196 | Time(s) 0.1258 | Loss 0.7082 | TrainAcc 0.8083 | ValAcc 0.7040 | ETputs(KTEPS) 98.83\n",
      "Epoch 00197 | Time(s) 0.1267 | Loss 0.6514 | TrainAcc 0.8167 | ValAcc 0.7040 | ETputs(KTEPS) 98.11\n",
      "Epoch 00198 | Time(s) 0.1267 | Loss 0.7020 | TrainAcc 0.8083 | ValAcc 0.7040 | ETputs(KTEPS) 98.15\n",
      "Epoch 00199 | Time(s) 0.1267 | Loss 0.8193 | TrainAcc 0.7333 | ValAcc 0.7040 | ETputs(KTEPS) 98.11\n",
      "\n",
      "Test Accuracy 0.7040\n"
     ]
    }
   ],
   "source": [
    "!../../gcn_text_categorization/venv/bin/python3 train.py --dataset=citeseer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/4tb/nabarun/nlp/DLNLP/gcn\n"
     ]
    }
   ],
   "source": [
    "%cd /4tb/nabarun/nlp/DLNLP/gcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "Namespace(dataset='cora', dropout=0.5, gpu=-1, lr=0.01, n_epochs=200, n_hidden=16, n_layers=1, weight_decay=0.0005)\n",
      "Loading from cache failed, re-processing.\n",
      "Finished data loading and preprocessing.\n",
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done saving data into cached files.\n",
      "/4tb/nabarun/nlp/gcn_text_categorization/venv/lib/python3.6/site-packages/dgl/data/utils.py:285: UserWarning: Property dataset.num_labels will be deprecated, please use dataset.num_classes instead.\n",
      "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
      "/4tb/nabarun/nlp/gcn_text_categorization/venv/lib/python3.6/site-packages/dgl/data/utils.py:285: UserWarning: Property dataset.graph will be deprecated, please use dataset[0] instead.\n",
      "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
      "----Data statistics------'\n",
      "      #Edges 10556\n",
      "      #Classes 7\n",
      "      #Train samples 140\n",
      "      #Val samples 500\n",
      "      #Test samples 1000\n",
      "/4tb/nabarun/nlp/gcn_text_categorization/venv/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/4tb/nabarun/nlp/gcn_text_categorization/venv/lib/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Epoch 00000 | Time(s) nan | Loss 1.9762 | Accuracy 0.1620 | ETputs(KTEPS) nan\n",
      "Epoch 00001 | Time(s) nan | Loss 1.9664 | Accuracy 0.1620 | ETputs(KTEPS) nan\n",
      "Epoch 00002 | Time(s) nan | Loss 1.9612 | Accuracy 0.1620 | ETputs(KTEPS) nan\n",
      "Epoch 00003 | Time(s) 0.1004 | Loss 1.9589 | Accuracy 0.1640 | ETputs(KTEPS) 132.05\n",
      "Epoch 00004 | Time(s) 0.0942 | Loss 1.9512 | Accuracy 0.1640 | ETputs(KTEPS) 140.81\n",
      "Epoch 00005 | Time(s) 0.0968 | Loss 1.9457 | Accuracy 0.1640 | ETputs(KTEPS) 136.99\n",
      "Epoch 00006 | Time(s) 0.0992 | Loss 1.9433 | Accuracy 0.1640 | ETputs(KTEPS) 133.71\n",
      "Epoch 00007 | Time(s) 0.0993 | Loss 1.9334 | Accuracy 0.1640 | ETputs(KTEPS) 133.56\n",
      "Epoch 00008 | Time(s) 0.0978 | Loss 1.9359 | Accuracy 0.1640 | ETputs(KTEPS) 135.65\n",
      "Epoch 00009 | Time(s) 0.0981 | Loss 1.9313 | Accuracy 0.1620 | ETputs(KTEPS) 135.20\n",
      "Epoch 00010 | Time(s) 0.0971 | Loss 1.9192 | Accuracy 0.1620 | ETputs(KTEPS) 136.57\n",
      "Epoch 00011 | Time(s) 0.0972 | Loss 1.9181 | Accuracy 0.1760 | ETputs(KTEPS) 136.45\n",
      "Epoch 00012 | Time(s) 0.0974 | Loss 1.9136 | Accuracy 0.1780 | ETputs(KTEPS) 136.20\n",
      "Epoch 00013 | Time(s) 0.0969 | Loss 1.9083 | Accuracy 0.2180 | ETputs(KTEPS) 136.94\n",
      "Epoch 00014 | Time(s) 0.0959 | Loss 1.9071 | Accuracy 0.2920 | ETputs(KTEPS) 138.25\n",
      "Epoch 00015 | Time(s) 0.0952 | Loss 1.8904 | Accuracy 0.3400 | ETputs(KTEPS) 139.27\n",
      "Epoch 00016 | Time(s) 0.0951 | Loss 1.8926 | Accuracy 0.3960 | ETputs(KTEPS) 139.45\n",
      "Epoch 00017 | Time(s) 0.0962 | Loss 1.8877 | Accuracy 0.4260 | ETputs(KTEPS) 137.81\n",
      "Epoch 00018 | Time(s) 0.0961 | Loss 1.8815 | Accuracy 0.4140 | ETputs(KTEPS) 138.06\n",
      "Epoch 00019 | Time(s) 0.0977 | Loss 1.8664 | Accuracy 0.3900 | ETputs(KTEPS) 135.71\n",
      "Epoch 00020 | Time(s) 0.0977 | Loss 1.8678 | Accuracy 0.3700 | ETputs(KTEPS) 135.78\n",
      "Epoch 00021 | Time(s) 0.0972 | Loss 1.8562 | Accuracy 0.3560 | ETputs(KTEPS) 136.45\n",
      "Epoch 00022 | Time(s) 0.0967 | Loss 1.8483 | Accuracy 0.3600 | ETputs(KTEPS) 137.21\n",
      "Epoch 00023 | Time(s) 0.0961 | Loss 1.8625 | Accuracy 0.3660 | ETputs(KTEPS) 138.03\n",
      "Epoch 00024 | Time(s) 0.0968 | Loss 1.8269 | Accuracy 0.3760 | ETputs(KTEPS) 136.97\n",
      "Epoch 00025 | Time(s) 0.0985 | Loss 1.8493 | Accuracy 0.3920 | ETputs(KTEPS) 134.65\n",
      "Epoch 00026 | Time(s) 0.0985 | Loss 1.8361 | Accuracy 0.4200 | ETputs(KTEPS) 134.70\n",
      "Epoch 00027 | Time(s) 0.0984 | Loss 1.8321 | Accuracy 0.4560 | ETputs(KTEPS) 134.80\n",
      "Epoch 00028 | Time(s) 0.0987 | Loss 1.8145 | Accuracy 0.4720 | ETputs(KTEPS) 134.44\n",
      "Epoch 00029 | Time(s) 0.0986 | Loss 1.7982 | Accuracy 0.4940 | ETputs(KTEPS) 134.54\n",
      "Epoch 00030 | Time(s) 0.0981 | Loss 1.7957 | Accuracy 0.5280 | ETputs(KTEPS) 135.22\n",
      "Epoch 00031 | Time(s) 0.0979 | Loss 1.8033 | Accuracy 0.5600 | ETputs(KTEPS) 135.46\n",
      "Epoch 00032 | Time(s) 0.0978 | Loss 1.7721 | Accuracy 0.5740 | ETputs(KTEPS) 135.65\n",
      "Epoch 00033 | Time(s) 0.0986 | Loss 1.7597 | Accuracy 0.5820 | ETputs(KTEPS) 134.47\n",
      "Epoch 00034 | Time(s) 0.0984 | Loss 1.7674 | Accuracy 0.6060 | ETputs(KTEPS) 134.76\n",
      "Epoch 00035 | Time(s) 0.0986 | Loss 1.7551 | Accuracy 0.6220 | ETputs(KTEPS) 134.57\n",
      "Epoch 00036 | Time(s) 0.0992 | Loss 1.7398 | Accuracy 0.6420 | ETputs(KTEPS) 133.77\n",
      "Epoch 00037 | Time(s) 0.0994 | Loss 1.7244 | Accuracy 0.6480 | ETputs(KTEPS) 133.50\n",
      "Epoch 00038 | Time(s) 0.0991 | Loss 1.7169 | Accuracy 0.6720 | ETputs(KTEPS) 133.79\n",
      "Epoch 00039 | Time(s) 0.0991 | Loss 1.7168 | Accuracy 0.6780 | ETputs(KTEPS) 133.79\n",
      "Epoch 00040 | Time(s) 0.0990 | Loss 1.6811 | Accuracy 0.6860 | ETputs(KTEPS) 134.01\n",
      "Epoch 00041 | Time(s) 0.0986 | Loss 1.6867 | Accuracy 0.6860 | ETputs(KTEPS) 134.50\n",
      "Epoch 00042 | Time(s) 0.0985 | Loss 1.6670 | Accuracy 0.6920 | ETputs(KTEPS) 134.61\n",
      "Epoch 00043 | Time(s) 0.0981 | Loss 1.6703 | Accuracy 0.6880 | ETputs(KTEPS) 135.18\n",
      "Epoch 00044 | Time(s) 0.0980 | Loss 1.6451 | Accuracy 0.6840 | ETputs(KTEPS) 135.38\n",
      "Epoch 00045 | Time(s) 0.0979 | Loss 1.6312 | Accuracy 0.6840 | ETputs(KTEPS) 135.50\n",
      "Epoch 00046 | Time(s) 0.0993 | Loss 1.6113 | Accuracy 0.6800 | ETputs(KTEPS) 133.62\n",
      "Epoch 00047 | Time(s) 0.0992 | Loss 1.5940 | Accuracy 0.6880 | ETputs(KTEPS) 133.70\n",
      "Epoch 00048 | Time(s) 0.0993 | Loss 1.5885 | Accuracy 0.6860 | ETputs(KTEPS) 133.61\n",
      "Epoch 00049 | Time(s) 0.0990 | Loss 1.5675 | Accuracy 0.6960 | ETputs(KTEPS) 134.01\n",
      "Epoch 00050 | Time(s) 0.0989 | Loss 1.5443 | Accuracy 0.7000 | ETputs(KTEPS) 134.06\n",
      "Epoch 00051 | Time(s) 0.0989 | Loss 1.5335 | Accuracy 0.7220 | ETputs(KTEPS) 134.17\n",
      "Epoch 00052 | Time(s) 0.0986 | Loss 1.5587 | Accuracy 0.7160 | ETputs(KTEPS) 134.55\n",
      "Epoch 00053 | Time(s) 0.0987 | Loss 1.5106 | Accuracy 0.7240 | ETputs(KTEPS) 134.33\n",
      "Epoch 00054 | Time(s) 0.0986 | Loss 1.5066 | Accuracy 0.7320 | ETputs(KTEPS) 134.56\n",
      "Epoch 00055 | Time(s) 0.0987 | Loss 1.4744 | Accuracy 0.7360 | ETputs(KTEPS) 134.39\n",
      "Epoch 00056 | Time(s) 0.0987 | Loss 1.4692 | Accuracy 0.7340 | ETputs(KTEPS) 134.42\n",
      "Epoch 00057 | Time(s) 0.0986 | Loss 1.4330 | Accuracy 0.7380 | ETputs(KTEPS) 134.54\n",
      "Epoch 00058 | Time(s) 0.0984 | Loss 1.4420 | Accuracy 0.7380 | ETputs(KTEPS) 134.75\n",
      "Epoch 00059 | Time(s) 0.0984 | Loss 1.4726 | Accuracy 0.7320 | ETputs(KTEPS) 134.79\n",
      "Epoch 00060 | Time(s) 0.0984 | Loss 1.4132 | Accuracy 0.7300 | ETputs(KTEPS) 134.74\n",
      "Epoch 00061 | Time(s) 0.0985 | Loss 1.3699 | Accuracy 0.7300 | ETputs(KTEPS) 134.64\n",
      "Epoch 00062 | Time(s) 0.0985 | Loss 1.3820 | Accuracy 0.7300 | ETputs(KTEPS) 134.66\n",
      "Epoch 00063 | Time(s) 0.0986 | Loss 1.3673 | Accuracy 0.7260 | ETputs(KTEPS) 134.53\n",
      "Epoch 00064 | Time(s) 0.0987 | Loss 1.3587 | Accuracy 0.7240 | ETputs(KTEPS) 134.40\n",
      "Epoch 00065 | Time(s) 0.0986 | Loss 1.3389 | Accuracy 0.7240 | ETputs(KTEPS) 134.53\n",
      "Epoch 00066 | Time(s) 0.0988 | Loss 1.3052 | Accuracy 0.7260 | ETputs(KTEPS) 134.30\n",
      "Epoch 00067 | Time(s) 0.0988 | Loss 1.2475 | Accuracy 0.7260 | ETputs(KTEPS) 134.29\n",
      "Epoch 00068 | Time(s) 0.0988 | Loss 1.3242 | Accuracy 0.7300 | ETputs(KTEPS) 134.25\n",
      "Epoch 00069 | Time(s) 0.0988 | Loss 1.2706 | Accuracy 0.7320 | ETputs(KTEPS) 134.28\n",
      "Epoch 00070 | Time(s) 0.0987 | Loss 1.2716 | Accuracy 0.7340 | ETputs(KTEPS) 134.44\n",
      "Epoch 00071 | Time(s) 0.0984 | Loss 1.2511 | Accuracy 0.7360 | ETputs(KTEPS) 134.74\n",
      "Epoch 00072 | Time(s) 0.0984 | Loss 1.2716 | Accuracy 0.7340 | ETputs(KTEPS) 134.86\n",
      "Epoch 00073 | Time(s) 0.0984 | Loss 1.1751 | Accuracy 0.7380 | ETputs(KTEPS) 134.81\n",
      "Epoch 00074 | Time(s) 0.0984 | Loss 1.2073 | Accuracy 0.7380 | ETputs(KTEPS) 134.81\n",
      "Epoch 00075 | Time(s) 0.0986 | Loss 1.2031 | Accuracy 0.7400 | ETputs(KTEPS) 134.46\n",
      "Epoch 00076 | Time(s) 0.0986 | Loss 1.1614 | Accuracy 0.7400 | ETputs(KTEPS) 134.54\n",
      "Epoch 00077 | Time(s) 0.0986 | Loss 1.1471 | Accuracy 0.7440 | ETputs(KTEPS) 134.56\n",
      "Epoch 00078 | Time(s) 0.0985 | Loss 1.1329 | Accuracy 0.7440 | ETputs(KTEPS) 134.63\n",
      "Epoch 00079 | Time(s) 0.0984 | Loss 1.1330 | Accuracy 0.7420 | ETputs(KTEPS) 134.84\n",
      "Epoch 00080 | Time(s) 0.0982 | Loss 1.1210 | Accuracy 0.7400 | ETputs(KTEPS) 135.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00081 | Time(s) 0.0981 | Loss 1.1044 | Accuracy 0.7400 | ETputs(KTEPS) 135.20\n",
      "Epoch 00082 | Time(s) 0.0982 | Loss 1.0644 | Accuracy 0.7340 | ETputs(KTEPS) 135.07\n",
      "Epoch 00083 | Time(s) 0.0984 | Loss 1.0747 | Accuracy 0.7380 | ETputs(KTEPS) 134.79\n",
      "Epoch 00084 | Time(s) 0.0984 | Loss 1.0160 | Accuracy 0.7400 | ETputs(KTEPS) 134.85\n",
      "Epoch 00085 | Time(s) 0.0985 | Loss 1.0281 | Accuracy 0.7480 | ETputs(KTEPS) 134.69\n",
      "Epoch 00086 | Time(s) 0.0985 | Loss 1.0202 | Accuracy 0.7480 | ETputs(KTEPS) 134.61\n",
      "Epoch 00087 | Time(s) 0.0985 | Loss 0.9799 | Accuracy 0.7540 | ETputs(KTEPS) 134.61\n",
      "Epoch 00088 | Time(s) 0.0987 | Loss 1.0294 | Accuracy 0.7560 | ETputs(KTEPS) 134.32\n",
      "Epoch 00089 | Time(s) 0.0988 | Loss 0.9637 | Accuracy 0.7560 | ETputs(KTEPS) 134.26\n",
      "Epoch 00090 | Time(s) 0.0987 | Loss 0.9290 | Accuracy 0.7600 | ETputs(KTEPS) 134.45\n",
      "Epoch 00091 | Time(s) 0.0985 | Loss 0.9405 | Accuracy 0.7600 | ETputs(KTEPS) 134.63\n",
      "Epoch 00092 | Time(s) 0.0984 | Loss 0.9641 | Accuracy 0.7600 | ETputs(KTEPS) 134.82\n",
      "Epoch 00093 | Time(s) 0.0985 | Loss 0.9490 | Accuracy 0.7660 | ETputs(KTEPS) 134.63\n",
      "Epoch 00094 | Time(s) 0.0984 | Loss 0.8790 | Accuracy 0.7640 | ETputs(KTEPS) 134.74\n",
      "Epoch 00095 | Time(s) 0.0984 | Loss 0.9421 | Accuracy 0.7700 | ETputs(KTEPS) 134.78\n",
      "Epoch 00096 | Time(s) 0.0983 | Loss 0.9377 | Accuracy 0.7680 | ETputs(KTEPS) 134.88\n",
      "Epoch 00097 | Time(s) 0.0983 | Loss 0.9246 | Accuracy 0.7640 | ETputs(KTEPS) 134.88\n",
      "Epoch 00098 | Time(s) 0.0983 | Loss 0.8678 | Accuracy 0.7620 | ETputs(KTEPS) 134.92\n",
      "Epoch 00099 | Time(s) 0.0991 | Loss 0.8892 | Accuracy 0.7580 | ETputs(KTEPS) 133.91\n",
      "Epoch 00100 | Time(s) 0.0990 | Loss 0.8601 | Accuracy 0.7640 | ETputs(KTEPS) 134.03\n",
      "Epoch 00101 | Time(s) 0.0991 | Loss 0.8142 | Accuracy 0.7640 | ETputs(KTEPS) 133.87\n",
      "Epoch 00102 | Time(s) 0.0994 | Loss 0.8311 | Accuracy 0.7740 | ETputs(KTEPS) 133.46\n",
      "Epoch 00103 | Time(s) 0.0995 | Loss 0.8036 | Accuracy 0.7760 | ETputs(KTEPS) 133.28\n",
      "Epoch 00104 | Time(s) 0.0995 | Loss 0.7833 | Accuracy 0.7760 | ETputs(KTEPS) 133.36\n",
      "Epoch 00105 | Time(s) 0.0994 | Loss 0.8135 | Accuracy 0.7780 | ETputs(KTEPS) 133.43\n",
      "Epoch 00106 | Time(s) 0.0994 | Loss 0.8127 | Accuracy 0.7800 | ETputs(KTEPS) 133.46\n",
      "Epoch 00107 | Time(s) 0.0993 | Loss 0.7562 | Accuracy 0.7780 | ETputs(KTEPS) 133.58\n",
      "Epoch 00108 | Time(s) 0.0993 | Loss 0.7747 | Accuracy 0.7800 | ETputs(KTEPS) 133.53\n",
      "Epoch 00109 | Time(s) 0.0992 | Loss 0.7680 | Accuracy 0.7800 | ETputs(KTEPS) 133.68\n",
      "Epoch 00110 | Time(s) 0.0991 | Loss 0.7756 | Accuracy 0.7800 | ETputs(KTEPS) 133.81\n",
      "Epoch 00111 | Time(s) 0.0991 | Loss 0.7824 | Accuracy 0.7780 | ETputs(KTEPS) 133.90\n",
      "Epoch 00112 | Time(s) 0.0990 | Loss 0.6984 | Accuracy 0.7740 | ETputs(KTEPS) 134.00\n",
      "Epoch 00113 | Time(s) 0.0988 | Loss 0.8206 | Accuracy 0.7760 | ETputs(KTEPS) 134.19\n",
      "Epoch 00114 | Time(s) 0.0988 | Loss 0.6981 | Accuracy 0.7800 | ETputs(KTEPS) 134.20\n",
      "Epoch 00115 | Time(s) 0.0988 | Loss 0.7541 | Accuracy 0.7800 | ETputs(KTEPS) 134.24\n",
      "Epoch 00116 | Time(s) 0.0990 | Loss 0.7424 | Accuracy 0.7800 | ETputs(KTEPS) 134.01\n",
      "Epoch 00117 | Time(s) 0.0989 | Loss 0.7223 | Accuracy 0.7760 | ETputs(KTEPS) 134.09\n",
      "Epoch 00118 | Time(s) 0.0989 | Loss 0.7139 | Accuracy 0.7740 | ETputs(KTEPS) 134.14\n",
      "Epoch 00119 | Time(s) 0.0991 | Loss 0.6908 | Accuracy 0.7820 | ETputs(KTEPS) 133.90\n",
      "Epoch 00120 | Time(s) 0.0991 | Loss 0.7110 | Accuracy 0.7820 | ETputs(KTEPS) 133.87\n",
      "Epoch 00121 | Time(s) 0.0991 | Loss 0.6781 | Accuracy 0.7840 | ETputs(KTEPS) 133.91\n",
      "Epoch 00122 | Time(s) 0.0990 | Loss 0.7318 | Accuracy 0.7800 | ETputs(KTEPS) 133.98\n",
      "Epoch 00123 | Time(s) 0.0990 | Loss 0.6943 | Accuracy 0.7700 | ETputs(KTEPS) 133.93\n",
      "Epoch 00124 | Time(s) 0.0990 | Loss 0.6556 | Accuracy 0.7760 | ETputs(KTEPS) 133.97\n",
      "Epoch 00125 | Time(s) 0.0990 | Loss 0.6357 | Accuracy 0.7760 | ETputs(KTEPS) 134.04\n",
      "Epoch 00126 | Time(s) 0.0988 | Loss 0.6516 | Accuracy 0.7780 | ETputs(KTEPS) 134.20\n",
      "Epoch 00127 | Time(s) 0.0987 | Loss 0.6874 | Accuracy 0.7760 | ETputs(KTEPS) 134.40\n",
      "Epoch 00128 | Time(s) 0.0986 | Loss 0.6492 | Accuracy 0.7760 | ETputs(KTEPS) 134.48\n",
      "Epoch 00129 | Time(s) 0.0985 | Loss 0.7006 | Accuracy 0.7800 | ETputs(KTEPS) 134.61\n",
      "Epoch 00130 | Time(s) 0.0987 | Loss 0.6804 | Accuracy 0.7780 | ETputs(KTEPS) 134.45\n",
      "Epoch 00131 | Time(s) 0.0987 | Loss 0.6560 | Accuracy 0.7760 | ETputs(KTEPS) 134.44\n",
      "Epoch 00132 | Time(s) 0.0987 | Loss 0.5870 | Accuracy 0.7740 | ETputs(KTEPS) 134.38\n",
      "Epoch 00133 | Time(s) 0.0988 | Loss 0.6381 | Accuracy 0.7720 | ETputs(KTEPS) 134.31\n",
      "Epoch 00134 | Time(s) 0.0987 | Loss 0.6064 | Accuracy 0.7720 | ETputs(KTEPS) 134.35\n",
      "Epoch 00135 | Time(s) 0.0988 | Loss 0.6552 | Accuracy 0.7700 | ETputs(KTEPS) 134.29\n",
      "Epoch 00136 | Time(s) 0.0988 | Loss 0.5740 | Accuracy 0.7720 | ETputs(KTEPS) 134.27\n",
      "Epoch 00137 | Time(s) 0.0989 | Loss 0.6177 | Accuracy 0.7760 | ETputs(KTEPS) 134.17\n",
      "Epoch 00138 | Time(s) 0.0989 | Loss 0.6330 | Accuracy 0.7720 | ETputs(KTEPS) 134.14\n",
      "Epoch 00139 | Time(s) 0.0989 | Loss 0.6394 | Accuracy 0.7740 | ETputs(KTEPS) 134.05\n",
      "Epoch 00140 | Time(s) 0.0989 | Loss 0.6181 | Accuracy 0.7760 | ETputs(KTEPS) 134.10\n",
      "Epoch 00141 | Time(s) 0.0989 | Loss 0.6393 | Accuracy 0.7780 | ETputs(KTEPS) 134.13\n",
      "Epoch 00142 | Time(s) 0.0990 | Loss 0.6418 | Accuracy 0.7740 | ETputs(KTEPS) 134.05\n",
      "Epoch 00143 | Time(s) 0.0994 | Loss 0.5982 | Accuracy 0.7780 | ETputs(KTEPS) 133.49\n",
      "Epoch 00144 | Time(s) 0.0994 | Loss 0.6025 | Accuracy 0.7820 | ETputs(KTEPS) 133.39\n",
      "Epoch 00145 | Time(s) 0.0995 | Loss 0.6033 | Accuracy 0.7820 | ETputs(KTEPS) 133.37\n",
      "Epoch 00146 | Time(s) 0.0994 | Loss 0.6057 | Accuracy 0.7800 | ETputs(KTEPS) 133.38\n",
      "Epoch 00147 | Time(s) 0.0994 | Loss 0.5897 | Accuracy 0.7840 | ETputs(KTEPS) 133.38\n",
      "Epoch 00148 | Time(s) 0.0994 | Loss 0.5353 | Accuracy 0.7840 | ETputs(KTEPS) 133.47\n",
      "Epoch 00149 | Time(s) 0.0994 | Loss 0.5647 | Accuracy 0.7800 | ETputs(KTEPS) 133.43\n",
      "Epoch 00150 | Time(s) 0.0993 | Loss 0.5891 | Accuracy 0.7740 | ETputs(KTEPS) 133.58\n",
      "Epoch 00151 | Time(s) 0.0993 | Loss 0.5528 | Accuracy 0.7740 | ETputs(KTEPS) 133.58\n",
      "Epoch 00152 | Time(s) 0.0992 | Loss 0.5454 | Accuracy 0.7740 | ETputs(KTEPS) 133.71\n",
      "Epoch 00153 | Time(s) 0.0991 | Loss 0.5824 | Accuracy 0.7760 | ETputs(KTEPS) 133.79\n",
      "Epoch 00154 | Time(s) 0.0991 | Loss 0.5423 | Accuracy 0.7800 | ETputs(KTEPS) 133.87\n",
      "Epoch 00155 | Time(s) 0.0990 | Loss 0.5522 | Accuracy 0.7780 | ETputs(KTEPS) 133.91\n",
      "Epoch 00156 | Time(s) 0.0990 | Loss 0.5920 | Accuracy 0.7760 | ETputs(KTEPS) 134.03\n",
      "Epoch 00157 | Time(s) 0.0989 | Loss 0.4996 | Accuracy 0.7780 | ETputs(KTEPS) 134.08\n",
      "Epoch 00158 | Time(s) 0.0991 | Loss 0.5650 | Accuracy 0.7760 | ETputs(KTEPS) 133.88\n",
      "Epoch 00159 | Time(s) 0.0990 | Loss 0.5050 | Accuracy 0.7780 | ETputs(KTEPS) 133.93\n",
      "Epoch 00160 | Time(s) 0.0990 | Loss 0.4993 | Accuracy 0.7740 | ETputs(KTEPS) 134.04\n",
      "Epoch 00161 | Time(s) 0.0989 | Loss 0.5427 | Accuracy 0.7840 | ETputs(KTEPS) 134.08\n",
      "Epoch 00162 | Time(s) 0.0989 | Loss 0.5715 | Accuracy 0.7820 | ETputs(KTEPS) 134.16\n",
      "Epoch 00163 | Time(s) 0.0988 | Loss 0.4666 | Accuracy 0.7780 | ETputs(KTEPS) 134.19\n",
      "Epoch 00164 | Time(s) 0.0988 | Loss 0.5232 | Accuracy 0.7840 | ETputs(KTEPS) 134.27\n",
      "Epoch 00165 | Time(s) 0.0988 | Loss 0.5431 | Accuracy 0.7860 | ETputs(KTEPS) 134.32\n",
      "Epoch 00166 | Time(s) 0.0987 | Loss 0.5180 | Accuracy 0.7840 | ETputs(KTEPS) 134.43\n",
      "Epoch 00167 | Time(s) 0.0986 | Loss 0.4956 | Accuracy 0.7820 | ETputs(KTEPS) 134.53\n",
      "Epoch 00168 | Time(s) 0.0986 | Loss 0.5057 | Accuracy 0.7800 | ETputs(KTEPS) 134.58\n",
      "Epoch 00169 | Time(s) 0.0985 | Loss 0.5319 | Accuracy 0.7800 | ETputs(KTEPS) 134.68\n",
      "Epoch 00170 | Time(s) 0.0984 | Loss 0.4944 | Accuracy 0.7740 | ETputs(KTEPS) 134.79\n",
      "Epoch 00171 | Time(s) 0.0983 | Loss 0.5209 | Accuracy 0.7740 | ETputs(KTEPS) 134.91\n",
      "Epoch 00172 | Time(s) 0.0983 | Loss 0.5444 | Accuracy 0.7780 | ETputs(KTEPS) 134.98\n",
      "Epoch 00173 | Time(s) 0.0984 | Loss 0.4394 | Accuracy 0.7780 | ETputs(KTEPS) 134.78\n",
      "Epoch 00174 | Time(s) 0.0984 | Loss 0.5179 | Accuracy 0.7780 | ETputs(KTEPS) 134.76\n",
      "Epoch 00175 | Time(s) 0.0985 | Loss 0.4768 | Accuracy 0.7760 | ETputs(KTEPS) 134.72\n",
      "Epoch 00176 | Time(s) 0.0985 | Loss 0.4762 | Accuracy 0.7780 | ETputs(KTEPS) 134.73\n",
      "Epoch 00177 | Time(s) 0.0984 | Loss 0.4942 | Accuracy 0.7760 | ETputs(KTEPS) 134.78\n",
      "Epoch 00178 | Time(s) 0.0983 | Loss 0.4965 | Accuracy 0.7760 | ETputs(KTEPS) 134.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00179 | Time(s) 0.0983 | Loss 0.5104 | Accuracy 0.7720 | ETputs(KTEPS) 135.00\n",
      "Epoch 00180 | Time(s) 0.0982 | Loss 0.4891 | Accuracy 0.7780 | ETputs(KTEPS) 135.10\n",
      "Epoch 00181 | Time(s) 0.0982 | Loss 0.4528 | Accuracy 0.7780 | ETputs(KTEPS) 135.01\n",
      "Epoch 00182 | Time(s) 0.0982 | Loss 0.4725 | Accuracy 0.7800 | ETputs(KTEPS) 135.05\n",
      "Epoch 00183 | Time(s) 0.0982 | Loss 0.4867 | Accuracy 0.7760 | ETputs(KTEPS) 135.05\n",
      "Epoch 00184 | Time(s) 0.0982 | Loss 0.4680 | Accuracy 0.7760 | ETputs(KTEPS) 135.06\n",
      "Epoch 00185 | Time(s) 0.0982 | Loss 0.5213 | Accuracy 0.7780 | ETputs(KTEPS) 135.06\n",
      "Epoch 00186 | Time(s) 0.0982 | Loss 0.4598 | Accuracy 0.7780 | ETputs(KTEPS) 135.06\n",
      "Epoch 00187 | Time(s) 0.0982 | Loss 0.4565 | Accuracy 0.7800 | ETputs(KTEPS) 135.06\n",
      "Epoch 00188 | Time(s) 0.0982 | Loss 0.4500 | Accuracy 0.7800 | ETputs(KTEPS) 135.05\n",
      "Epoch 00189 | Time(s) 0.0982 | Loss 0.4594 | Accuracy 0.7840 | ETputs(KTEPS) 135.13\n",
      "Epoch 00190 | Time(s) 0.0981 | Loss 0.4376 | Accuracy 0.7840 | ETputs(KTEPS) 135.19\n",
      "Epoch 00191 | Time(s) 0.0982 | Loss 0.4944 | Accuracy 0.7860 | ETputs(KTEPS) 135.12\n",
      "Epoch 00192 | Time(s) 0.0981 | Loss 0.5235 | Accuracy 0.7840 | ETputs(KTEPS) 135.16\n",
      "Epoch 00193 | Time(s) 0.0981 | Loss 0.4053 | Accuracy 0.7860 | ETputs(KTEPS) 135.21\n",
      "Epoch 00194 | Time(s) 0.0981 | Loss 0.4954 | Accuracy 0.7860 | ETputs(KTEPS) 135.16\n",
      "Epoch 00195 | Time(s) 0.0981 | Loss 0.4655 | Accuracy 0.7840 | ETputs(KTEPS) 135.20\n",
      "Epoch 00196 | Time(s) 0.0981 | Loss 0.4095 | Accuracy 0.7840 | ETputs(KTEPS) 135.21\n",
      "Epoch 00197 | Time(s) 0.0981 | Loss 0.4497 | Accuracy 0.7880 | ETputs(KTEPS) 135.25\n",
      "Epoch 00198 | Time(s) 0.0980 | Loss 0.4882 | Accuracy 0.7880 | ETputs(KTEPS) 135.34\n",
      "Epoch 00199 | Time(s) 0.0979 | Loss 0.4617 | Accuracy 0.7820 | ETputs(KTEPS) 135.42\n",
      "\n",
      "Test Accuracy 0.8120\n"
     ]
    }
   ],
   "source": [
    "!../../gcn_text_categorization/venv/bin/python3 train.py cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "Namespace(dataset='pubmed', dropout=0.5, gpu=-1, lr=0.01, n_epochs=200, n_hidden=16, n_layers=1, weight_decay=0.0005)\n",
      "Loading from cache failed, re-processing.\n",
      "Finished data loading and preprocessing.\n",
      "  NumNodes: 19717\n",
      "  NumEdges: 88651\n",
      "  NumFeats: 500\n",
      "  NumClasses: 3\n",
      "  NumTrainingSamples: 60\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done saving data into cached files.\n",
      "/4tb/nabarun/nlp/gcn_text_categorization/venv/lib/python3.6/site-packages/dgl/data/utils.py:285: UserWarning: Property dataset.num_labels will be deprecated, please use dataset.num_classes instead.\n",
      "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
      "/4tb/nabarun/nlp/gcn_text_categorization/venv/lib/python3.6/site-packages/dgl/data/utils.py:285: UserWarning: Property dataset.graph will be deprecated, please use dataset[0] instead.\n",
      "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
      "----Data statistics------'\n",
      "      #Edges 88651\n",
      "      #Classes 3\n",
      "      #Train samples 60\n",
      "      #Val samples 500\n",
      "      #Test samples 1000\n",
      "/4tb/nabarun/nlp/gcn_text_categorization/venv/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/4tb/nabarun/nlp/gcn_text_categorization/venv/lib/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Epoch 00000 | Time(s) nan | Loss 1.1630 | Accuracy 0.4160 | ETputs(KTEPS) nan\n",
      "Epoch 00001 | Time(s) nan | Loss 1.1470 | Accuracy 0.4160 | ETputs(KTEPS) nan\n",
      "Epoch 00002 | Time(s) nan | Loss 1.1377 | Accuracy 0.4160 | ETputs(KTEPS) nan\n",
      "Epoch 00003 | Time(s) 0.5477 | Loss 1.1286 | Accuracy 0.4200 | ETputs(KTEPS) 197.85\n",
      "Epoch 00004 | Time(s) 0.5392 | Loss 1.1002 | Accuracy 0.4140 | ETputs(KTEPS) 200.98\n",
      "Epoch 00005 | Time(s) 0.5448 | Loss 1.0795 | Accuracy 0.4040 | ETputs(KTEPS) 198.91\n",
      "Epoch 00006 | Time(s) 0.5302 | Loss 1.0792 | Accuracy 0.3880 | ETputs(KTEPS) 204.39\n",
      "Epoch 00007 | Time(s) 0.5338 | Loss 1.0862 | Accuracy 0.3920 | ETputs(KTEPS) 202.99\n",
      "Epoch 00008 | Time(s) 0.5273 | Loss 1.0581 | Accuracy 0.3860 | ETputs(KTEPS) 205.53\n",
      "Epoch 00009 | Time(s) 0.5265 | Loss 1.0619 | Accuracy 0.4140 | ETputs(KTEPS) 205.83\n",
      "Epoch 00010 | Time(s) 0.5196 | Loss 1.0510 | Accuracy 0.4480 | ETputs(KTEPS) 208.54\n",
      "Epoch 00011 | Time(s) 0.5206 | Loss 1.0524 | Accuracy 0.4780 | ETputs(KTEPS) 208.14\n",
      "Epoch 00012 | Time(s) 0.5180 | Loss 1.0410 | Accuracy 0.5140 | ETputs(KTEPS) 209.18\n",
      "Epoch 00013 | Time(s) 0.5188 | Loss 1.0150 | Accuracy 0.5360 | ETputs(KTEPS) 208.88\n",
      "Epoch 00014 | Time(s) 0.5236 | Loss 1.0118 | Accuracy 0.5660 | ETputs(KTEPS) 206.98\n",
      "Epoch 00015 | Time(s) 0.5233 | Loss 1.0033 | Accuracy 0.5900 | ETputs(KTEPS) 207.09\n",
      "Epoch 00016 | Time(s) 0.5245 | Loss 0.9929 | Accuracy 0.6060 | ETputs(KTEPS) 206.60\n",
      "Epoch 00017 | Time(s) 0.5248 | Loss 0.9768 | Accuracy 0.6160 | ETputs(KTEPS) 206.47\n",
      "Epoch 00018 | Time(s) 0.5246 | Loss 0.9651 | Accuracy 0.6180 | ETputs(KTEPS) 206.55\n",
      "Epoch 00019 | Time(s) 0.5299 | Loss 0.9725 | Accuracy 0.6280 | ETputs(KTEPS) 204.50\n",
      "Epoch 00020 | Time(s) 0.5397 | Loss 0.9536 | Accuracy 0.6280 | ETputs(KTEPS) 200.81\n",
      "Epoch 00021 | Time(s) 0.5388 | Loss 0.9381 | Accuracy 0.6240 | ETputs(KTEPS) 201.14\n",
      "Epoch 00022 | Time(s) 0.5373 | Loss 0.9535 | Accuracy 0.6140 | ETputs(KTEPS) 201.70\n",
      "Epoch 00023 | Time(s) 0.5362 | Loss 0.9504 | Accuracy 0.6100 | ETputs(KTEPS) 202.11\n",
      "Epoch 00024 | Time(s) 0.5367 | Loss 0.9186 | Accuracy 0.6000 | ETputs(KTEPS) 201.92\n",
      "Epoch 00025 | Time(s) 0.5386 | Loss 0.8795 | Accuracy 0.5980 | ETputs(KTEPS) 201.21\n",
      "Epoch 00026 | Time(s) 0.5367 | Loss 0.9087 | Accuracy 0.5960 | ETputs(KTEPS) 201.92\n",
      "Epoch 00027 | Time(s) 0.5346 | Loss 0.8933 | Accuracy 0.5920 | ETputs(KTEPS) 202.69\n",
      "Epoch 00028 | Time(s) 0.5339 | Loss 0.8665 | Accuracy 0.5960 | ETputs(KTEPS) 202.96\n",
      "Epoch 00029 | Time(s) 0.5358 | Loss 0.8854 | Accuracy 0.6000 | ETputs(KTEPS) 202.27\n",
      "Epoch 00030 | Time(s) 0.5343 | Loss 0.8474 | Accuracy 0.6000 | ETputs(KTEPS) 202.83\n",
      "Epoch 00031 | Time(s) 0.5353 | Loss 0.8289 | Accuracy 0.6260 | ETputs(KTEPS) 202.43\n",
      "Epoch 00032 | Time(s) 0.5350 | Loss 0.8289 | Accuracy 0.6400 | ETputs(KTEPS) 202.57\n",
      "Epoch 00033 | Time(s) 0.5355 | Loss 0.8093 | Accuracy 0.6460 | ETputs(KTEPS) 202.37\n",
      "Epoch 00034 | Time(s) 0.5356 | Loss 0.7747 | Accuracy 0.6700 | ETputs(KTEPS) 202.34\n",
      "Epoch 00035 | Time(s) 0.5350 | Loss 0.7764 | Accuracy 0.6980 | ETputs(KTEPS) 202.56\n",
      "Epoch 00036 | Time(s) 0.5333 | Loss 0.7869 | Accuracy 0.7160 | ETputs(KTEPS) 203.19\n",
      "Epoch 00037 | Time(s) 0.5347 | Loss 0.7787 | Accuracy 0.7160 | ETputs(KTEPS) 202.65\n",
      "Epoch 00038 | Time(s) 0.5345 | Loss 0.7581 | Accuracy 0.7220 | ETputs(KTEPS) 202.73\n",
      "Epoch 00039 | Time(s) 0.5345 | Loss 0.7214 | Accuracy 0.7240 | ETputs(KTEPS) 202.76\n",
      "Epoch 00040 | Time(s) 0.5330 | Loss 0.7144 | Accuracy 0.7200 | ETputs(KTEPS) 203.30\n",
      "Epoch 00041 | Time(s) 0.5341 | Loss 0.7146 | Accuracy 0.7200 | ETputs(KTEPS) 202.89\n",
      "Epoch 00042 | Time(s) 0.5325 | Loss 0.7178 | Accuracy 0.7220 | ETputs(KTEPS) 203.49\n",
      "Epoch 00043 | Time(s) 0.5317 | Loss 0.6778 | Accuracy 0.7260 | ETputs(KTEPS) 203.79\n",
      "Epoch 00044 | Time(s) 0.5300 | Loss 0.7089 | Accuracy 0.7300 | ETputs(KTEPS) 204.48\n",
      "Epoch 00045 | Time(s) 0.5296 | Loss 0.6583 | Accuracy 0.7380 | ETputs(KTEPS) 204.63\n",
      "Epoch 00046 | Time(s) 0.5280 | Loss 0.6722 | Accuracy 0.7400 | ETputs(KTEPS) 205.24\n",
      "Epoch 00047 | Time(s) 0.5278 | Loss 0.6275 | Accuracy 0.7400 | ETputs(KTEPS) 205.32\n",
      "Epoch 00048 | Time(s) 0.5264 | Loss 0.6471 | Accuracy 0.7400 | ETputs(KTEPS) 205.86\n",
      "Epoch 00049 | Time(s) 0.5261 | Loss 0.6325 | Accuracy 0.7460 | ETputs(KTEPS) 205.96\n",
      "Epoch 00050 | Time(s) 0.5260 | Loss 0.5571 | Accuracy 0.7520 | ETputs(KTEPS) 206.03\n",
      "Epoch 00051 | Time(s) 0.5257 | Loss 0.5781 | Accuracy 0.7520 | ETputs(KTEPS) 206.12\n",
      "Epoch 00052 | Time(s) 0.5245 | Loss 0.6077 | Accuracy 0.7600 | ETputs(KTEPS) 206.61\n",
      "Epoch 00053 | Time(s) 0.5250 | Loss 0.5721 | Accuracy 0.7600 | ETputs(KTEPS) 206.42\n",
      "Epoch 00054 | Time(s) 0.5243 | Loss 0.5528 | Accuracy 0.7660 | ETputs(KTEPS) 206.68\n",
      "Epoch 00055 | Time(s) 0.5240 | Loss 0.5618 | Accuracy 0.7680 | ETputs(KTEPS) 206.78\n",
      "Epoch 00056 | Time(s) 0.5244 | Loss 0.5458 | Accuracy 0.7680 | ETputs(KTEPS) 206.64\n",
      "Epoch 00057 | Time(s) 0.5242 | Loss 0.5874 | Accuracy 0.7720 | ETputs(KTEPS) 206.72\n",
      "Epoch 00058 | Time(s) 0.5227 | Loss 0.5269 | Accuracy 0.7740 | ETputs(KTEPS) 207.31\n",
      "Epoch 00059 | Time(s) 0.5221 | Loss 0.5160 | Accuracy 0.7740 | ETputs(KTEPS) 207.54\n",
      "Epoch 00060 | Time(s) 0.5217 | Loss 0.4902 | Accuracy 0.7740 | ETputs(KTEPS) 207.72\n",
      "Epoch 00061 | Time(s) 0.5210 | Loss 0.5037 | Accuracy 0.7780 | ETputs(KTEPS) 208.01\n",
      "Epoch 00062 | Time(s) 0.5214 | Loss 0.4814 | Accuracy 0.7800 | ETputs(KTEPS) 207.83\n",
      "Epoch 00063 | Time(s) 0.5215 | Loss 0.4847 | Accuracy 0.7800 | ETputs(KTEPS) 207.80\n",
      "Epoch 00064 | Time(s) 0.5208 | Loss 0.4681 | Accuracy 0.7840 | ETputs(KTEPS) 208.07\n",
      "Epoch 00065 | Time(s) 0.5207 | Loss 0.4433 | Accuracy 0.7840 | ETputs(KTEPS) 208.13\n",
      "Epoch 00066 | Time(s) 0.5207 | Loss 0.4577 | Accuracy 0.7880 | ETputs(KTEPS) 208.10\n",
      "Epoch 00067 | Time(s) 0.5201 | Loss 0.4656 | Accuracy 0.7940 | ETputs(KTEPS) 208.34\n",
      "Epoch 00068 | Time(s) 0.5195 | Loss 0.4362 | Accuracy 0.7900 | ETputs(KTEPS) 208.58\n",
      "Epoch 00069 | Time(s) 0.5212 | Loss 0.4023 | Accuracy 0.7940 | ETputs(KTEPS) 207.91\n",
      "Epoch 00070 | Time(s) 0.5227 | Loss 0.4549 | Accuracy 0.7900 | ETputs(KTEPS) 207.32\n",
      "Epoch 00071 | Time(s) 0.5245 | Loss 0.4228 | Accuracy 0.7840 | ETputs(KTEPS) 206.60\n",
      "Epoch 00072 | Time(s) 0.5263 | Loss 0.3977 | Accuracy 0.7780 | ETputs(KTEPS) 205.89\n",
      "Epoch 00073 | Time(s) 0.5286 | Loss 0.3815 | Accuracy 0.7760 | ETputs(KTEPS) 205.02\n",
      "Epoch 00074 | Time(s) 0.5290 | Loss 0.4555 | Accuracy 0.7760 | ETputs(KTEPS) 204.85\n",
      "Epoch 00075 | Time(s) 0.5288 | Loss 0.3698 | Accuracy 0.7760 | ETputs(KTEPS) 204.94\n",
      "Epoch 00076 | Time(s) 0.5283 | Loss 0.3999 | Accuracy 0.7780 | ETputs(KTEPS) 205.14\n",
      "Epoch 00077 | Time(s) 0.5282 | Loss 0.3524 | Accuracy 0.7800 | ETputs(KTEPS) 205.14\n",
      "Epoch 00078 | Time(s) 0.5292 | Loss 0.4411 | Accuracy 0.7860 | ETputs(KTEPS) 204.76\n",
      "Epoch 00079 | Time(s) 0.5311 | Loss 0.3886 | Accuracy 0.7960 | ETputs(KTEPS) 204.06\n",
      "Epoch 00080 | Time(s) 0.5315 | Loss 0.4247 | Accuracy 0.8040 | ETputs(KTEPS) 203.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00081 | Time(s) 0.5329 | Loss 0.3272 | Accuracy 0.8000 | ETputs(KTEPS) 203.34\n",
      "Epoch 00082 | Time(s) 0.5337 | Loss 0.3220 | Accuracy 0.8000 | ETputs(KTEPS) 203.06\n",
      "Epoch 00083 | Time(s) 0.5348 | Loss 0.3633 | Accuracy 0.8000 | ETputs(KTEPS) 202.63\n",
      "Epoch 00084 | Time(s) 0.5346 | Loss 0.3592 | Accuracy 0.8020 | ETputs(KTEPS) 202.68\n",
      "Epoch 00085 | Time(s) 0.5354 | Loss 0.3196 | Accuracy 0.8020 | ETputs(KTEPS) 202.41\n",
      "Epoch 00086 | Time(s) 0.5350 | Loss 0.3337 | Accuracy 0.7980 | ETputs(KTEPS) 202.55\n",
      "Epoch 00087 | Time(s) 0.5356 | Loss 0.3941 | Accuracy 0.8040 | ETputs(KTEPS) 202.31\n",
      "Epoch 00088 | Time(s) 0.5367 | Loss 0.3448 | Accuracy 0.7960 | ETputs(KTEPS) 201.92\n",
      "Epoch 00089 | Time(s) 0.5371 | Loss 0.3890 | Accuracy 0.7880 | ETputs(KTEPS) 201.75\n",
      "Epoch 00090 | Time(s) 0.5403 | Loss 0.3305 | Accuracy 0.7880 | ETputs(KTEPS) 200.57\n",
      "Epoch 00091 | Time(s) 0.5409 | Loss 0.2964 | Accuracy 0.7900 | ETputs(KTEPS) 200.33\n",
      "Epoch 00092 | Time(s) 0.5401 | Loss 0.3389 | Accuracy 0.7940 | ETputs(KTEPS) 200.65\n",
      "Epoch 00093 | Time(s) 0.5404 | Loss 0.3341 | Accuracy 0.7960 | ETputs(KTEPS) 200.53\n",
      "Epoch 00094 | Time(s) 0.5408 | Loss 0.3363 | Accuracy 0.7980 | ETputs(KTEPS) 200.39\n",
      "Epoch 00095 | Time(s) 0.5412 | Loss 0.3482 | Accuracy 0.8020 | ETputs(KTEPS) 200.23\n",
      "Epoch 00096 | Time(s) 0.5411 | Loss 0.3196 | Accuracy 0.8060 | ETputs(KTEPS) 200.28\n",
      "Epoch 00097 | Time(s) 0.5415 | Loss 0.3150 | Accuracy 0.8020 | ETputs(KTEPS) 200.12\n",
      "Epoch 00098 | Time(s) 0.5432 | Loss 0.3088 | Accuracy 0.8020 | ETputs(KTEPS) 199.49\n",
      "Epoch 00099 | Time(s) 0.5430 | Loss 0.3063 | Accuracy 0.8020 | ETputs(KTEPS) 199.58\n",
      "Epoch 00100 | Time(s) 0.5427 | Loss 0.2824 | Accuracy 0.8060 | ETputs(KTEPS) 199.68\n",
      "Epoch 00101 | Time(s) 0.5433 | Loss 0.3407 | Accuracy 0.8080 | ETputs(KTEPS) 199.44\n",
      "Epoch 00102 | Time(s) 0.5445 | Loss 0.2932 | Accuracy 0.7960 | ETputs(KTEPS) 199.00\n",
      "Epoch 00103 | Time(s) 0.5447 | Loss 0.2540 | Accuracy 0.7900 | ETputs(KTEPS) 198.95\n",
      "Epoch 00104 | Time(s) 0.5449 | Loss 0.2688 | Accuracy 0.7840 | ETputs(KTEPS) 198.89\n",
      "Epoch 00105 | Time(s) 0.5483 | Loss 0.2605 | Accuracy 0.7820 | ETputs(KTEPS) 197.62\n",
      "Epoch 00106 | Time(s) 0.5486 | Loss 0.2548 | Accuracy 0.7820 | ETputs(KTEPS) 197.54\n",
      "Epoch 00107 | Time(s) 0.5484 | Loss 0.2641 | Accuracy 0.7820 | ETputs(KTEPS) 197.59\n",
      "Epoch 00108 | Time(s) 0.5483 | Loss 0.3169 | Accuracy 0.7880 | ETputs(KTEPS) 197.64\n",
      "Epoch 00109 | Time(s) 0.5486 | Loss 0.2823 | Accuracy 0.7920 | ETputs(KTEPS) 197.54\n",
      "Epoch 00110 | Time(s) 0.5493 | Loss 0.2985 | Accuracy 0.8020 | ETputs(KTEPS) 197.28\n",
      "Epoch 00111 | Time(s) 0.5488 | Loss 0.2461 | Accuracy 0.8020 | ETputs(KTEPS) 197.47\n",
      "Epoch 00112 | Time(s) 0.5480 | Loss 0.2672 | Accuracy 0.8020 | ETputs(KTEPS) 197.76\n",
      "Epoch 00113 | Time(s) 0.5475 | Loss 0.2739 | Accuracy 0.8000 | ETputs(KTEPS) 197.94\n",
      "Epoch 00114 | Time(s) 0.5471 | Loss 0.2714 | Accuracy 0.8000 | ETputs(KTEPS) 198.08\n",
      "Epoch 00115 | Time(s) 0.5473 | Loss 0.2917 | Accuracy 0.8000 | ETputs(KTEPS) 198.01\n",
      "Epoch 00116 | Time(s) 0.5465 | Loss 0.2893 | Accuracy 0.8060 | ETputs(KTEPS) 198.30\n",
      "Epoch 00117 | Time(s) 0.5465 | Loss 0.2429 | Accuracy 0.8040 | ETputs(KTEPS) 198.28\n",
      "Epoch 00118 | Time(s) 0.5465 | Loss 0.2755 | Accuracy 0.8060 | ETputs(KTEPS) 198.30\n",
      "Epoch 00119 | Time(s) 0.5460 | Loss 0.2775 | Accuracy 0.8040 | ETputs(KTEPS) 198.46\n",
      "Epoch 00120 | Time(s) 0.5452 | Loss 0.2824 | Accuracy 0.8040 | ETputs(KTEPS) 198.76\n",
      "Epoch 00121 | Time(s) 0.5451 | Loss 0.2655 | Accuracy 0.7980 | ETputs(KTEPS) 198.81\n",
      "Epoch 00122 | Time(s) 0.5452 | Loss 0.2798 | Accuracy 0.8000 | ETputs(KTEPS) 198.77\n",
      "Epoch 00123 | Time(s) 0.5452 | Loss 0.2537 | Accuracy 0.8000 | ETputs(KTEPS) 198.75\n",
      "Epoch 00124 | Time(s) 0.5447 | Loss 0.2366 | Accuracy 0.8000 | ETputs(KTEPS) 198.93\n",
      "Epoch 00125 | Time(s) 0.5445 | Loss 0.2412 | Accuracy 0.7980 | ETputs(KTEPS) 199.03\n",
      "Epoch 00126 | Time(s) 0.5440 | Loss 0.2539 | Accuracy 0.8000 | ETputs(KTEPS) 199.21\n",
      "Epoch 00127 | Time(s) 0.5440 | Loss 0.2028 | Accuracy 0.7980 | ETputs(KTEPS) 199.19\n",
      "Epoch 00128 | Time(s) 0.5434 | Loss 0.2194 | Accuracy 0.8000 | ETputs(KTEPS) 199.41\n",
      "Epoch 00129 | Time(s) 0.5431 | Loss 0.2753 | Accuracy 0.7980 | ETputs(KTEPS) 199.55\n",
      "Epoch 00130 | Time(s) 0.5424 | Loss 0.2441 | Accuracy 0.8000 | ETputs(KTEPS) 199.80\n",
      "Epoch 00131 | Time(s) 0.5423 | Loss 0.2269 | Accuracy 0.7980 | ETputs(KTEPS) 199.84\n",
      "Epoch 00132 | Time(s) 0.5420 | Loss 0.2608 | Accuracy 0.8000 | ETputs(KTEPS) 199.92\n",
      "Epoch 00133 | Time(s) 0.5417 | Loss 0.2395 | Accuracy 0.8020 | ETputs(KTEPS) 200.05\n",
      "Epoch 00134 | Time(s) 0.5413 | Loss 0.2559 | Accuracy 0.8000 | ETputs(KTEPS) 200.19\n",
      "Epoch 00135 | Time(s) 0.5410 | Loss 0.2635 | Accuracy 0.8000 | ETputs(KTEPS) 200.29\n",
      "Epoch 00136 | Time(s) 0.5404 | Loss 0.2203 | Accuracy 0.8080 | ETputs(KTEPS) 200.54\n",
      "Epoch 00137 | Time(s) 0.5405 | Loss 0.2397 | Accuracy 0.8060 | ETputs(KTEPS) 200.50\n",
      "Epoch 00138 | Time(s) 0.5403 | Loss 0.2811 | Accuracy 0.8100 | ETputs(KTEPS) 200.58\n",
      "Epoch 00139 | Time(s) 0.5401 | Loss 0.2486 | Accuracy 0.8020 | ETputs(KTEPS) 200.63\n",
      "Epoch 00140 | Time(s) 0.5399 | Loss 0.2064 | Accuracy 0.8000 | ETputs(KTEPS) 200.72\n",
      "Epoch 00141 | Time(s) 0.5398 | Loss 0.2301 | Accuracy 0.8040 | ETputs(KTEPS) 200.74\n",
      "Epoch 00142 | Time(s) 0.5393 | Loss 0.1955 | Accuracy 0.8080 | ETputs(KTEPS) 200.92\n",
      "Epoch 00143 | Time(s) 0.5391 | Loss 0.2913 | Accuracy 0.8020 | ETputs(KTEPS) 201.00\n",
      "Epoch 00144 | Time(s) 0.5386 | Loss 0.2230 | Accuracy 0.8020 | ETputs(KTEPS) 201.19\n",
      "Epoch 00145 | Time(s) 0.5387 | Loss 0.2619 | Accuracy 0.8040 | ETputs(KTEPS) 201.14\n",
      "Epoch 00146 | Time(s) 0.5384 | Loss 0.2444 | Accuracy 0.8040 | ETputs(KTEPS) 201.28\n",
      "Epoch 00147 | Time(s) 0.5381 | Loss 0.2113 | Accuracy 0.8020 | ETputs(KTEPS) 201.39\n",
      "Epoch 00148 | Time(s) 0.5375 | Loss 0.2465 | Accuracy 0.8100 | ETputs(KTEPS) 201.59\n",
      "Epoch 00149 | Time(s) 0.5373 | Loss 0.2027 | Accuracy 0.8040 | ETputs(KTEPS) 201.69\n",
      "Epoch 00150 | Time(s) 0.5369 | Loss 0.2155 | Accuracy 0.8060 | ETputs(KTEPS) 201.82\n",
      "Epoch 00151 | Time(s) 0.5369 | Loss 0.1710 | Accuracy 0.8000 | ETputs(KTEPS) 201.83\n",
      "Epoch 00152 | Time(s) 0.5367 | Loss 0.2402 | Accuracy 0.7980 | ETputs(KTEPS) 201.90\n",
      "Epoch 00153 | Time(s) 0.5366 | Loss 0.2281 | Accuracy 0.7980 | ETputs(KTEPS) 201.93\n",
      "Epoch 00154 | Time(s) 0.5361 | Loss 0.2087 | Accuracy 0.8020 | ETputs(KTEPS) 202.12\n",
      "Epoch 00155 | Time(s) 0.5361 | Loss 0.2057 | Accuracy 0.8000 | ETputs(KTEPS) 202.14\n",
      "Epoch 00156 | Time(s) 0.5358 | Loss 0.1905 | Accuracy 0.7980 | ETputs(KTEPS) 202.24\n",
      "Epoch 00157 | Time(s) 0.5357 | Loss 0.1847 | Accuracy 0.7980 | ETputs(KTEPS) 202.28\n",
      "Epoch 00158 | Time(s) 0.5353 | Loss 0.2053 | Accuracy 0.7980 | ETputs(KTEPS) 202.45\n",
      "Epoch 00159 | Time(s) 0.5353 | Loss 0.2316 | Accuracy 0.8040 | ETputs(KTEPS) 202.43\n",
      "Epoch 00160 | Time(s) 0.5350 | Loss 0.2062 | Accuracy 0.8060 | ETputs(KTEPS) 202.56\n",
      "Epoch 00161 | Time(s) 0.5349 | Loss 0.1846 | Accuracy 0.8040 | ETputs(KTEPS) 202.59\n",
      "Epoch 00162 | Time(s) 0.5347 | Loss 0.2108 | Accuracy 0.8080 | ETputs(KTEPS) 202.68\n",
      "Epoch 00163 | Time(s) 0.5347 | Loss 0.2493 | Accuracy 0.8080 | ETputs(KTEPS) 202.65\n",
      "Epoch 00164 | Time(s) 0.5343 | Loss 0.1545 | Accuracy 0.8080 | ETputs(KTEPS) 202.83\n",
      "Epoch 00165 | Time(s) 0.5345 | Loss 0.1606 | Accuracy 0.8060 | ETputs(KTEPS) 202.75\n",
      "Epoch 00166 | Time(s) 0.5344 | Loss 0.2279 | Accuracy 0.8020 | ETputs(KTEPS) 202.79\n",
      "Epoch 00167 | Time(s) 0.5340 | Loss 0.2184 | Accuracy 0.8020 | ETputs(KTEPS) 202.93\n",
      "Epoch 00168 | Time(s) 0.5337 | Loss 0.2072 | Accuracy 0.8020 | ETputs(KTEPS) 203.03\n",
      "Epoch 00169 | Time(s) 0.5338 | Loss 0.2082 | Accuracy 0.8040 | ETputs(KTEPS) 203.02\n",
      "Epoch 00170 | Time(s) 0.5334 | Loss 0.2166 | Accuracy 0.8020 | ETputs(KTEPS) 203.16\n",
      "Epoch 00171 | Time(s) 0.5336 | Loss 0.2267 | Accuracy 0.8000 | ETputs(KTEPS) 203.09\n",
      "Epoch 00172 | Time(s) 0.5331 | Loss 0.1883 | Accuracy 0.8060 | ETputs(KTEPS) 203.28\n",
      "Epoch 00173 | Time(s) 0.5331 | Loss 0.1616 | Accuracy 0.8060 | ETputs(KTEPS) 203.25\n",
      "Epoch 00174 | Time(s) 0.5329 | Loss 0.2137 | Accuracy 0.8060 | ETputs(KTEPS) 203.36\n",
      "Epoch 00175 | Time(s) 0.5330 | Loss 0.1797 | Accuracy 0.8040 | ETputs(KTEPS) 203.33\n",
      "Epoch 00176 | Time(s) 0.5327 | Loss 0.2002 | Accuracy 0.8060 | ETputs(KTEPS) 203.43\n",
      "Epoch 00177 | Time(s) 0.5327 | Loss 0.1850 | Accuracy 0.8040 | ETputs(KTEPS) 203.42\n",
      "Epoch 00178 | Time(s) 0.5323 | Loss 0.2074 | Accuracy 0.8040 | ETputs(KTEPS) 203.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00179 | Time(s) 0.5321 | Loss 0.2293 | Accuracy 0.8040 | ETputs(KTEPS) 203.66\n",
      "Epoch 00180 | Time(s) 0.5321 | Loss 0.2032 | Accuracy 0.8040 | ETputs(KTEPS) 203.65\n",
      "Epoch 00181 | Time(s) 0.5318 | Loss 0.2337 | Accuracy 0.8080 | ETputs(KTEPS) 203.78\n",
      "Epoch 00182 | Time(s) 0.5320 | Loss 0.1816 | Accuracy 0.8000 | ETputs(KTEPS) 203.70\n",
      "Epoch 00183 | Time(s) 0.5324 | Loss 0.1828 | Accuracy 0.7940 | ETputs(KTEPS) 203.53\n",
      "Epoch 00184 | Time(s) 0.5321 | Loss 0.2060 | Accuracy 0.7940 | ETputs(KTEPS) 203.64\n",
      "Epoch 00185 | Time(s) 0.5321 | Loss 0.1884 | Accuracy 0.7980 | ETputs(KTEPS) 203.64\n",
      "Epoch 00186 | Time(s) 0.5318 | Loss 0.1851 | Accuracy 0.8060 | ETputs(KTEPS) 203.77\n",
      "Epoch 00187 | Time(s) 0.5318 | Loss 0.1931 | Accuracy 0.8060 | ETputs(KTEPS) 203.78\n",
      "Epoch 00188 | Time(s) 0.5314 | Loss 0.2060 | Accuracy 0.8040 | ETputs(KTEPS) 203.93\n",
      "Epoch 00189 | Time(s) 0.5314 | Loss 0.1830 | Accuracy 0.8080 | ETputs(KTEPS) 203.91\n",
      "Epoch 00190 | Time(s) 0.5311 | Loss 0.1358 | Accuracy 0.8060 | ETputs(KTEPS) 204.03\n",
      "Epoch 00191 | Time(s) 0.5311 | Loss 0.2106 | Accuracy 0.8000 | ETputs(KTEPS) 204.05\n",
      "Epoch 00192 | Time(s) 0.5311 | Loss 0.2253 | Accuracy 0.8060 | ETputs(KTEPS) 204.04\n",
      "Epoch 00193 | Time(s) 0.5316 | Loss 0.1871 | Accuracy 0.8100 | ETputs(KTEPS) 203.85\n",
      "Epoch 00194 | Time(s) 0.5313 | Loss 0.1660 | Accuracy 0.8080 | ETputs(KTEPS) 203.97\n",
      "Epoch 00195 | Time(s) 0.5323 | Loss 0.1680 | Accuracy 0.8000 | ETputs(KTEPS) 203.59\n",
      "Epoch 00196 | Time(s) 0.5329 | Loss 0.2108 | Accuracy 0.8040 | ETputs(KTEPS) 203.34\n",
      "Epoch 00197 | Time(s) 0.5343 | Loss 0.1738 | Accuracy 0.8120 | ETputs(KTEPS) 202.84\n",
      "Epoch 00198 | Time(s) 0.5354 | Loss 0.1838 | Accuracy 0.8120 | ETputs(KTEPS) 202.41\n",
      "Epoch 00199 | Time(s) 0.5356 | Loss 0.2073 | Accuracy 0.8100 | ETputs(KTEPS) 202.32\n",
      "\n",
      "Test Accuracy 0.7930\n"
     ]
    }
   ],
   "source": [
    "!../../gcn_text_categorization/venv/bin/python3 train.py pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "Namespace(dataset='citeseer', dropout=0.5, gpu=-1, lr=0.01, n_epochs=200, n_hidden=16, n_layers=1, weight_decay=0.0005)\n",
      "Loading from cache failed, re-processing.\n",
      "/4tb/nabarun/nlp/gcn_text_categorization/venv/lib/python3.6/site-packages/dgl/data/citation_graph.py:258: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n",
      "Finished data loading and preprocessing.\n",
      "  NumNodes: 3327\n",
      "  NumEdges: 9228\n",
      "  NumFeats: 3703\n",
      "  NumClasses: 6\n",
      "  NumTrainingSamples: 120\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done saving data into cached files.\n",
      "/4tb/nabarun/nlp/gcn_text_categorization/venv/lib/python3.6/site-packages/dgl/data/utils.py:285: UserWarning: Property dataset.num_labels will be deprecated, please use dataset.num_classes instead.\n",
      "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
      "/4tb/nabarun/nlp/gcn_text_categorization/venv/lib/python3.6/site-packages/dgl/data/utils.py:285: UserWarning: Property dataset.graph will be deprecated, please use dataset[0] instead.\n",
      "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
      "----Data statistics------'\n",
      "      #Edges 9228\n",
      "      #Classes 6\n",
      "      #Train samples 120\n",
      "      #Val samples 500\n",
      "      #Test samples 1000\n",
      "/4tb/nabarun/nlp/gcn_text_categorization/venv/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/4tb/nabarun/nlp/gcn_text_categorization/venv/lib/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Epoch 00000 | Time(s) nan | Loss 1.8247 | Accuracy 0.1380 | ETputs(KTEPS) nan\n",
      "Epoch 00001 | Time(s) nan | Loss 1.8178 | Accuracy 0.1380 | ETputs(KTEPS) nan\n",
      "Epoch 00002 | Time(s) nan | Loss 1.8073 | Accuracy 0.1380 | ETputs(KTEPS) nan\n",
      "Epoch 00003 | Time(s) 0.1601 | Loss 1.8059 | Accuracy 0.1380 | ETputs(KTEPS) 77.67\n",
      "Epoch 00004 | Time(s) 0.1534 | Loss 1.7966 | Accuracy 0.1380 | ETputs(KTEPS) 81.06\n",
      "Epoch 00005 | Time(s) 0.1466 | Loss 1.7978 | Accuracy 0.1380 | ETputs(KTEPS) 84.78\n",
      "Epoch 00006 | Time(s) 0.1403 | Loss 1.7958 | Accuracy 0.1400 | ETputs(KTEPS) 88.62\n",
      "Epoch 00007 | Time(s) 0.1378 | Loss 1.7902 | Accuracy 0.1420 | ETputs(KTEPS) 90.24\n",
      "Epoch 00008 | Time(s) 0.1426 | Loss 1.7845 | Accuracy 0.1440 | ETputs(KTEPS) 87.19\n",
      "Epoch 00009 | Time(s) 0.1422 | Loss 1.7816 | Accuracy 0.1460 | ETputs(KTEPS) 87.44\n",
      "Epoch 00010 | Time(s) 0.1425 | Loss 1.7792 | Accuracy 0.1480 | ETputs(KTEPS) 87.22\n",
      "Epoch 00011 | Time(s) 0.1420 | Loss 1.7790 | Accuracy 0.1480 | ETputs(KTEPS) 87.56\n",
      "Epoch 00012 | Time(s) 0.1409 | Loss 1.7705 | Accuracy 0.1480 | ETputs(KTEPS) 88.21\n",
      "Epoch 00013 | Time(s) 0.1398 | Loss 1.7642 | Accuracy 0.1500 | ETputs(KTEPS) 88.91\n",
      "Epoch 00014 | Time(s) 0.1408 | Loss 1.7580 | Accuracy 0.1500 | ETputs(KTEPS) 88.26\n",
      "Epoch 00015 | Time(s) 0.1410 | Loss 1.7537 | Accuracy 0.1520 | ETputs(KTEPS) 88.16\n",
      "Epoch 00016 | Time(s) 0.1412 | Loss 1.7437 | Accuracy 0.1560 | ETputs(KTEPS) 88.07\n",
      "Epoch 00017 | Time(s) 0.1413 | Loss 1.7513 | Accuracy 0.1780 | ETputs(KTEPS) 87.97\n",
      "Epoch 00018 | Time(s) 0.1432 | Loss 1.7472 | Accuracy 0.2220 | ETputs(KTEPS) 86.82\n",
      "Epoch 00019 | Time(s) 0.1438 | Loss 1.7247 | Accuracy 0.2720 | ETputs(KTEPS) 86.44\n",
      "Epoch 00020 | Time(s) 0.1430 | Loss 1.7373 | Accuracy 0.3160 | ETputs(KTEPS) 86.94\n",
      "Epoch 00021 | Time(s) 0.1442 | Loss 1.7255 | Accuracy 0.3180 | ETputs(KTEPS) 86.18\n",
      "Epoch 00022 | Time(s) 0.1449 | Loss 1.7308 | Accuracy 0.3040 | ETputs(KTEPS) 85.80\n",
      "Epoch 00023 | Time(s) 0.1440 | Loss 1.7199 | Accuracy 0.3100 | ETputs(KTEPS) 86.32\n",
      "Epoch 00024 | Time(s) 0.1431 | Loss 1.7030 | Accuracy 0.3060 | ETputs(KTEPS) 86.88\n",
      "Epoch 00025 | Time(s) 0.1433 | Loss 1.6999 | Accuracy 0.3080 | ETputs(KTEPS) 86.73\n",
      "Epoch 00026 | Time(s) 0.1432 | Loss 1.7021 | Accuracy 0.3080 | ETputs(KTEPS) 86.83\n",
      "Epoch 00027 | Time(s) 0.1430 | Loss 1.6939 | Accuracy 0.3180 | ETputs(KTEPS) 86.91\n",
      "Epoch 00028 | Time(s) 0.1429 | Loss 1.6798 | Accuracy 0.3220 | ETputs(KTEPS) 87.00\n",
      "Epoch 00029 | Time(s) 0.1425 | Loss 1.6811 | Accuracy 0.3300 | ETputs(KTEPS) 87.24\n",
      "Epoch 00030 | Time(s) 0.1421 | Loss 1.6728 | Accuracy 0.3420 | ETputs(KTEPS) 87.47\n",
      "Epoch 00031 | Time(s) 0.1413 | Loss 1.6653 | Accuracy 0.3580 | ETputs(KTEPS) 87.96\n",
      "Epoch 00032 | Time(s) 0.1412 | Loss 1.6634 | Accuracy 0.3880 | ETputs(KTEPS) 88.02\n",
      "Epoch 00033 | Time(s) 0.1406 | Loss 1.6483 | Accuracy 0.4040 | ETputs(KTEPS) 88.39\n",
      "Epoch 00034 | Time(s) 0.1405 | Loss 1.6270 | Accuracy 0.4160 | ETputs(KTEPS) 88.49\n",
      "Epoch 00035 | Time(s) 0.1400 | Loss 1.6317 | Accuracy 0.4180 | ETputs(KTEPS) 88.82\n",
      "Epoch 00036 | Time(s) 0.1405 | Loss 1.6160 | Accuracy 0.4460 | ETputs(KTEPS) 88.49\n",
      "Epoch 00037 | Time(s) 0.1415 | Loss 1.6243 | Accuracy 0.4600 | ETputs(KTEPS) 87.87\n",
      "Epoch 00038 | Time(s) 0.1413 | Loss 1.5793 | Accuracy 0.4700 | ETputs(KTEPS) 87.96\n",
      "Epoch 00039 | Time(s) 0.1411 | Loss 1.5820 | Accuracy 0.4980 | ETputs(KTEPS) 88.13\n",
      "Epoch 00040 | Time(s) 0.1408 | Loss 1.6083 | Accuracy 0.5120 | ETputs(KTEPS) 88.29\n",
      "Epoch 00041 | Time(s) 0.1408 | Loss 1.5529 | Accuracy 0.5140 | ETputs(KTEPS) 88.28\n",
      "Epoch 00042 | Time(s) 0.1405 | Loss 1.5581 | Accuracy 0.5300 | ETputs(KTEPS) 88.47\n",
      "Epoch 00043 | Time(s) 0.1406 | Loss 1.5585 | Accuracy 0.5380 | ETputs(KTEPS) 88.43\n",
      "Epoch 00044 | Time(s) 0.1405 | Loss 1.5401 | Accuracy 0.5500 | ETputs(KTEPS) 88.50\n",
      "Epoch 00045 | Time(s) 0.1404 | Loss 1.5221 | Accuracy 0.5580 | ETputs(KTEPS) 88.52\n",
      "Epoch 00046 | Time(s) 0.1407 | Loss 1.5153 | Accuracy 0.5820 | ETputs(KTEPS) 88.34\n",
      "Epoch 00047 | Time(s) 0.1407 | Loss 1.4813 | Accuracy 0.6080 | ETputs(KTEPS) 88.33\n",
      "Epoch 00048 | Time(s) 0.1402 | Loss 1.4848 | Accuracy 0.6260 | ETputs(KTEPS) 88.64\n",
      "Epoch 00049 | Time(s) 0.1398 | Loss 1.4698 | Accuracy 0.6300 | ETputs(KTEPS) 88.95\n",
      "Epoch 00050 | Time(s) 0.1393 | Loss 1.4524 | Accuracy 0.6200 | ETputs(KTEPS) 89.23\n",
      "Epoch 00051 | Time(s) 0.1390 | Loss 1.4422 | Accuracy 0.6140 | ETputs(KTEPS) 89.41\n",
      "Epoch 00052 | Time(s) 0.1388 | Loss 1.4427 | Accuracy 0.6140 | ETputs(KTEPS) 89.56\n",
      "Epoch 00053 | Time(s) 0.1388 | Loss 1.4182 | Accuracy 0.6080 | ETputs(KTEPS) 89.54\n",
      "Epoch 00054 | Time(s) 0.1387 | Loss 1.4066 | Accuracy 0.6200 | ETputs(KTEPS) 89.65\n",
      "Epoch 00055 | Time(s) 0.1383 | Loss 1.3661 | Accuracy 0.6200 | ETputs(KTEPS) 89.88\n",
      "Epoch 00056 | Time(s) 0.1383 | Loss 1.3683 | Accuracy 0.6240 | ETputs(KTEPS) 89.91\n",
      "Epoch 00057 | Time(s) 0.1380 | Loss 1.3890 | Accuracy 0.6260 | ETputs(KTEPS) 90.07\n",
      "Epoch 00058 | Time(s) 0.1381 | Loss 1.3850 | Accuracy 0.6380 | ETputs(KTEPS) 90.01\n",
      "Epoch 00059 | Time(s) 0.1385 | Loss 1.3890 | Accuracy 0.6560 | ETputs(KTEPS) 89.78\n",
      "Epoch 00060 | Time(s) 0.1384 | Loss 1.3177 | Accuracy 0.6560 | ETputs(KTEPS) 89.79\n",
      "Epoch 00061 | Time(s) 0.1383 | Loss 1.3649 | Accuracy 0.6640 | ETputs(KTEPS) 89.90\n",
      "Epoch 00062 | Time(s) 0.1381 | Loss 1.2940 | Accuracy 0.6700 | ETputs(KTEPS) 90.01\n",
      "Epoch 00063 | Time(s) 0.1378 | Loss 1.2963 | Accuracy 0.6780 | ETputs(KTEPS) 90.21\n",
      "Epoch 00064 | Time(s) 0.1375 | Loss 1.2686 | Accuracy 0.6740 | ETputs(KTEPS) 90.38\n",
      "Epoch 00065 | Time(s) 0.1373 | Loss 1.2713 | Accuracy 0.6700 | ETputs(KTEPS) 90.53\n",
      "Epoch 00066 | Time(s) 0.1371 | Loss 1.2638 | Accuracy 0.6680 | ETputs(KTEPS) 90.69\n",
      "Epoch 00067 | Time(s) 0.1369 | Loss 1.2369 | Accuracy 0.6640 | ETputs(KTEPS) 90.80\n",
      "Epoch 00068 | Time(s) 0.1367 | Loss 1.2352 | Accuracy 0.6640 | ETputs(KTEPS) 90.94\n",
      "Epoch 00069 | Time(s) 0.1367 | Loss 1.2593 | Accuracy 0.6660 | ETputs(KTEPS) 90.95\n",
      "Epoch 00070 | Time(s) 0.1368 | Loss 1.2214 | Accuracy 0.6740 | ETputs(KTEPS) 90.90\n",
      "Epoch 00071 | Time(s) 0.1370 | Loss 1.1934 | Accuracy 0.6760 | ETputs(KTEPS) 90.71\n",
      "Epoch 00072 | Time(s) 0.1370 | Loss 1.2033 | Accuracy 0.6800 | ETputs(KTEPS) 90.71\n",
      "Epoch 00073 | Time(s) 0.1371 | Loss 1.1258 | Accuracy 0.6860 | ETputs(KTEPS) 90.70\n",
      "Epoch 00074 | Time(s) 0.1371 | Loss 1.1715 | Accuracy 0.6820 | ETputs(KTEPS) 90.68\n",
      "Epoch 00075 | Time(s) 0.1374 | Loss 1.1724 | Accuracy 0.6900 | ETputs(KTEPS) 90.50\n",
      "Epoch 00076 | Time(s) 0.1371 | Loss 1.1258 | Accuracy 0.6960 | ETputs(KTEPS) 90.66\n",
      "Epoch 00077 | Time(s) 0.1370 | Loss 1.1437 | Accuracy 0.7040 | ETputs(KTEPS) 90.75\n",
      "Epoch 00078 | Time(s) 0.1370 | Loss 1.1027 | Accuracy 0.7000 | ETputs(KTEPS) 90.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00079 | Time(s) 0.1370 | Loss 1.1095 | Accuracy 0.7000 | ETputs(KTEPS) 90.73\n",
      "Epoch 00080 | Time(s) 0.1371 | Loss 1.1028 | Accuracy 0.6940 | ETputs(KTEPS) 90.68\n",
      "Epoch 00081 | Time(s) 0.1372 | Loss 1.0926 | Accuracy 0.6860 | ETputs(KTEPS) 90.63\n",
      "Epoch 00082 | Time(s) 0.1374 | Loss 1.0636 | Accuracy 0.6840 | ETputs(KTEPS) 90.45\n",
      "Epoch 00083 | Time(s) 0.1373 | Loss 1.0715 | Accuracy 0.6860 | ETputs(KTEPS) 90.53\n",
      "Epoch 00084 | Time(s) 0.1371 | Loss 1.0460 | Accuracy 0.6880 | ETputs(KTEPS) 90.68\n",
      "Epoch 00085 | Time(s) 0.1369 | Loss 1.0364 | Accuracy 0.6920 | ETputs(KTEPS) 90.82\n",
      "Epoch 00086 | Time(s) 0.1367 | Loss 1.0464 | Accuracy 0.6980 | ETputs(KTEPS) 90.96\n",
      "Epoch 00087 | Time(s) 0.1365 | Loss 1.0224 | Accuracy 0.7000 | ETputs(KTEPS) 91.09\n",
      "Epoch 00088 | Time(s) 0.1363 | Loss 1.0106 | Accuracy 0.6980 | ETputs(KTEPS) 91.19\n",
      "Epoch 00089 | Time(s) 0.1362 | Loss 0.9926 | Accuracy 0.6920 | ETputs(KTEPS) 91.27\n",
      "Epoch 00090 | Time(s) 0.1362 | Loss 0.9758 | Accuracy 0.7000 | ETputs(KTEPS) 91.29\n",
      "Epoch 00091 | Time(s) 0.1361 | Loss 0.9824 | Accuracy 0.7060 | ETputs(KTEPS) 91.31\n",
      "Epoch 00092 | Time(s) 0.1362 | Loss 0.9547 | Accuracy 0.7080 | ETputs(KTEPS) 91.29\n",
      "Epoch 00093 | Time(s) 0.1363 | Loss 0.9076 | Accuracy 0.7120 | ETputs(KTEPS) 91.23\n",
      "Epoch 00094 | Time(s) 0.1364 | Loss 0.9564 | Accuracy 0.7100 | ETputs(KTEPS) 91.11\n",
      "Epoch 00095 | Time(s) 0.1363 | Loss 0.9434 | Accuracy 0.7020 | ETputs(KTEPS) 91.19\n",
      "Epoch 00096 | Time(s) 0.1362 | Loss 0.9024 | Accuracy 0.6960 | ETputs(KTEPS) 91.28\n",
      "Epoch 00097 | Time(s) 0.1360 | Loss 0.8810 | Accuracy 0.6920 | ETputs(KTEPS) 91.40\n",
      "Epoch 00098 | Time(s) 0.1359 | Loss 0.9450 | Accuracy 0.6980 | ETputs(KTEPS) 91.45\n",
      "Epoch 00099 | Time(s) 0.1358 | Loss 0.8976 | Accuracy 0.6940 | ETputs(KTEPS) 91.55\n",
      "Epoch 00100 | Time(s) 0.1357 | Loss 0.9021 | Accuracy 0.6960 | ETputs(KTEPS) 91.63\n",
      "Epoch 00101 | Time(s) 0.1356 | Loss 0.8656 | Accuracy 0.6980 | ETputs(KTEPS) 91.69\n",
      "Epoch 00102 | Time(s) 0.1356 | Loss 0.8989 | Accuracy 0.7040 | ETputs(KTEPS) 91.69\n",
      "Epoch 00103 | Time(s) 0.1355 | Loss 0.8897 | Accuracy 0.6980 | ETputs(KTEPS) 91.75\n",
      "Epoch 00104 | Time(s) 0.1354 | Loss 0.8679 | Accuracy 0.6980 | ETputs(KTEPS) 91.84\n",
      "Epoch 00105 | Time(s) 0.1354 | Loss 0.8938 | Accuracy 0.7000 | ETputs(KTEPS) 91.80\n",
      "Epoch 00106 | Time(s) 0.1355 | Loss 0.8254 | Accuracy 0.7000 | ETputs(KTEPS) 91.72\n",
      "Epoch 00107 | Time(s) 0.1354 | Loss 0.8120 | Accuracy 0.7020 | ETputs(KTEPS) 91.80\n",
      "Epoch 00108 | Time(s) 0.1354 | Loss 0.8304 | Accuracy 0.7020 | ETputs(KTEPS) 91.82\n",
      "Epoch 00109 | Time(s) 0.1353 | Loss 0.8292 | Accuracy 0.6980 | ETputs(KTEPS) 91.89\n",
      "Epoch 00110 | Time(s) 0.1352 | Loss 0.8193 | Accuracy 0.6940 | ETputs(KTEPS) 91.96\n",
      "Epoch 00111 | Time(s) 0.1352 | Loss 0.7650 | Accuracy 0.6980 | ETputs(KTEPS) 91.96\n",
      "Epoch 00112 | Time(s) 0.1352 | Loss 0.7888 | Accuracy 0.7020 | ETputs(KTEPS) 91.94\n",
      "Epoch 00113 | Time(s) 0.1353 | Loss 0.8179 | Accuracy 0.7000 | ETputs(KTEPS) 91.85\n",
      "Epoch 00114 | Time(s) 0.1353 | Loss 0.7659 | Accuracy 0.7040 | ETputs(KTEPS) 91.89\n",
      "Epoch 00115 | Time(s) 0.1353 | Loss 0.8109 | Accuracy 0.7060 | ETputs(KTEPS) 91.90\n",
      "Epoch 00116 | Time(s) 0.1353 | Loss 0.8266 | Accuracy 0.7020 | ETputs(KTEPS) 91.91\n",
      "Epoch 00117 | Time(s) 0.1354 | Loss 0.7742 | Accuracy 0.7060 | ETputs(KTEPS) 91.80\n",
      "Epoch 00118 | Time(s) 0.1354 | Loss 0.7828 | Accuracy 0.7080 | ETputs(KTEPS) 91.81\n",
      "Epoch 00119 | Time(s) 0.1354 | Loss 0.7560 | Accuracy 0.7060 | ETputs(KTEPS) 91.81\n",
      "Epoch 00120 | Time(s) 0.1353 | Loss 0.7003 | Accuracy 0.7080 | ETputs(KTEPS) 91.87\n",
      "Epoch 00121 | Time(s) 0.1353 | Loss 0.7416 | Accuracy 0.7040 | ETputs(KTEPS) 91.89\n",
      "Epoch 00122 | Time(s) 0.1353 | Loss 0.7493 | Accuracy 0.7000 | ETputs(KTEPS) 91.86\n",
      "Epoch 00123 | Time(s) 0.1352 | Loss 0.8282 | Accuracy 0.7040 | ETputs(KTEPS) 91.92\n",
      "Epoch 00124 | Time(s) 0.1351 | Loss 0.7746 | Accuracy 0.7060 | ETputs(KTEPS) 91.99\n",
      "Epoch 00125 | Time(s) 0.1350 | Loss 0.7400 | Accuracy 0.7020 | ETputs(KTEPS) 92.05\n",
      "Epoch 00126 | Time(s) 0.1349 | Loss 0.7327 | Accuracy 0.6980 | ETputs(KTEPS) 92.12\n",
      "Epoch 00127 | Time(s) 0.1350 | Loss 0.7584 | Accuracy 0.6960 | ETputs(KTEPS) 92.10\n",
      "Epoch 00128 | Time(s) 0.1349 | Loss 0.7311 | Accuracy 0.7040 | ETputs(KTEPS) 92.14\n",
      "Epoch 00129 | Time(s) 0.1351 | Loss 0.7650 | Accuracy 0.7100 | ETputs(KTEPS) 92.02\n",
      "Epoch 00130 | Time(s) 0.1350 | Loss 0.6927 | Accuracy 0.7100 | ETputs(KTEPS) 92.09\n",
      "Epoch 00131 | Time(s) 0.1350 | Loss 0.7221 | Accuracy 0.7100 | ETputs(KTEPS) 92.11\n",
      "Epoch 00132 | Time(s) 0.1350 | Loss 0.7114 | Accuracy 0.7120 | ETputs(KTEPS) 92.10\n",
      "Epoch 00133 | Time(s) 0.1350 | Loss 0.6923 | Accuracy 0.7100 | ETputs(KTEPS) 92.06\n",
      "Epoch 00134 | Time(s) 0.1350 | Loss 0.7366 | Accuracy 0.7100 | ETputs(KTEPS) 92.11\n",
      "Epoch 00135 | Time(s) 0.1349 | Loss 0.7304 | Accuracy 0.7060 | ETputs(KTEPS) 92.18\n",
      "Epoch 00136 | Time(s) 0.1351 | Loss 0.7193 | Accuracy 0.6960 | ETputs(KTEPS) 92.04\n",
      "Epoch 00137 | Time(s) 0.1354 | Loss 0.7115 | Accuracy 0.7000 | ETputs(KTEPS) 91.80\n",
      "Epoch 00138 | Time(s) 0.1357 | Loss 0.7012 | Accuracy 0.6980 | ETputs(KTEPS) 91.63\n",
      "Epoch 00139 | Time(s) 0.1356 | Loss 0.6828 | Accuracy 0.7000 | ETputs(KTEPS) 91.66\n",
      "Epoch 00140 | Time(s) 0.1357 | Loss 0.6635 | Accuracy 0.7020 | ETputs(KTEPS) 91.59\n",
      "Epoch 00141 | Time(s) 0.1356 | Loss 0.6916 | Accuracy 0.6980 | ETputs(KTEPS) 91.68\n",
      "Epoch 00142 | Time(s) 0.1355 | Loss 0.6917 | Accuracy 0.7060 | ETputs(KTEPS) 91.72\n",
      "Epoch 00143 | Time(s) 0.1354 | Loss 0.6399 | Accuracy 0.7100 | ETputs(KTEPS) 91.80\n",
      "Epoch 00144 | Time(s) 0.1354 | Loss 0.7043 | Accuracy 0.7040 | ETputs(KTEPS) 91.80\n",
      "Epoch 00145 | Time(s) 0.1354 | Loss 0.6391 | Accuracy 0.7020 | ETputs(KTEPS) 91.83\n",
      "Epoch 00146 | Time(s) 0.1354 | Loss 0.6914 | Accuracy 0.7020 | ETputs(KTEPS) 91.84\n",
      "Epoch 00147 | Time(s) 0.1354 | Loss 0.7033 | Accuracy 0.6940 | ETputs(KTEPS) 91.84\n",
      "Epoch 00148 | Time(s) 0.1354 | Loss 0.6227 | Accuracy 0.6960 | ETputs(KTEPS) 91.84\n",
      "Epoch 00149 | Time(s) 0.1354 | Loss 0.6089 | Accuracy 0.6980 | ETputs(KTEPS) 91.81\n",
      "Epoch 00150 | Time(s) 0.1353 | Loss 0.6149 | Accuracy 0.6980 | ETputs(KTEPS) 91.88\n",
      "Epoch 00151 | Time(s) 0.1355 | Loss 0.6438 | Accuracy 0.7040 | ETputs(KTEPS) 91.77\n",
      "Epoch 00152 | Time(s) 0.1354 | Loss 0.6583 | Accuracy 0.7080 | ETputs(KTEPS) 91.80\n",
      "Epoch 00153 | Time(s) 0.1354 | Loss 0.6794 | Accuracy 0.7040 | ETputs(KTEPS) 91.84\n",
      "Epoch 00154 | Time(s) 0.1354 | Loss 0.6583 | Accuracy 0.7080 | ETputs(KTEPS) 91.83\n",
      "Epoch 00155 | Time(s) 0.1354 | Loss 0.6464 | Accuracy 0.7140 | ETputs(KTEPS) 91.82\n",
      "Epoch 00156 | Time(s) 0.1353 | Loss 0.6373 | Accuracy 0.7120 | ETputs(KTEPS) 91.85\n",
      "Epoch 00157 | Time(s) 0.1353 | Loss 0.6070 | Accuracy 0.7100 | ETputs(KTEPS) 91.87\n",
      "Epoch 00158 | Time(s) 0.1353 | Loss 0.6373 | Accuracy 0.7120 | ETputs(KTEPS) 91.87\n",
      "Epoch 00159 | Time(s) 0.1353 | Loss 0.6215 | Accuracy 0.7140 | ETputs(KTEPS) 91.89\n",
      "Epoch 00160 | Time(s) 0.1353 | Loss 0.5885 | Accuracy 0.7140 | ETputs(KTEPS) 91.87\n",
      "Epoch 00161 | Time(s) 0.1352 | Loss 0.6232 | Accuracy 0.7080 | ETputs(KTEPS) 91.93\n",
      "Epoch 00162 | Time(s) 0.1353 | Loss 0.6414 | Accuracy 0.7020 | ETputs(KTEPS) 91.90\n",
      "Epoch 00163 | Time(s) 0.1352 | Loss 0.6041 | Accuracy 0.7040 | ETputs(KTEPS) 91.92\n",
      "Epoch 00164 | Time(s) 0.1352 | Loss 0.5860 | Accuracy 0.7020 | ETputs(KTEPS) 91.94\n",
      "Epoch 00165 | Time(s) 0.1351 | Loss 0.6182 | Accuracy 0.6960 | ETputs(KTEPS) 92.02\n",
      "Epoch 00166 | Time(s) 0.1350 | Loss 0.6523 | Accuracy 0.7000 | ETputs(KTEPS) 92.08\n",
      "Epoch 00167 | Time(s) 0.1350 | Loss 0.5876 | Accuracy 0.7040 | ETputs(KTEPS) 92.11\n",
      "Epoch 00168 | Time(s) 0.1349 | Loss 0.5244 | Accuracy 0.7120 | ETputs(KTEPS) 92.15\n",
      "Epoch 00169 | Time(s) 0.1348 | Loss 0.6087 | Accuracy 0.7080 | ETputs(KTEPS) 92.19\n",
      "Epoch 00170 | Time(s) 0.1349 | Loss 0.6009 | Accuracy 0.7100 | ETputs(KTEPS) 92.16\n",
      "Epoch 00171 | Time(s) 0.1348 | Loss 0.6622 | Accuracy 0.7060 | ETputs(KTEPS) 92.19\n",
      "Epoch 00172 | Time(s) 0.1348 | Loss 0.6650 | Accuracy 0.7020 | ETputs(KTEPS) 92.22\n",
      "Epoch 00173 | Time(s) 0.1348 | Loss 0.5795 | Accuracy 0.7040 | ETputs(KTEPS) 92.24\n",
      "Epoch 00174 | Time(s) 0.1349 | Loss 0.5723 | Accuracy 0.7040 | ETputs(KTEPS) 92.18\n",
      "Epoch 00175 | Time(s) 0.1348 | Loss 0.6088 | Accuracy 0.7060 | ETputs(KTEPS) 92.21\n",
      "Epoch 00176 | Time(s) 0.1348 | Loss 0.6243 | Accuracy 0.7040 | ETputs(KTEPS) 92.24\n",
      "Epoch 00177 | Time(s) 0.1347 | Loss 0.5937 | Accuracy 0.7060 | ETputs(KTEPS) 92.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00178 | Time(s) 0.1348 | Loss 0.5939 | Accuracy 0.7000 | ETputs(KTEPS) 92.23\n",
      "Epoch 00179 | Time(s) 0.1348 | Loss 0.5386 | Accuracy 0.7000 | ETputs(KTEPS) 92.18\n",
      "Epoch 00180 | Time(s) 0.1348 | Loss 0.6128 | Accuracy 0.7080 | ETputs(KTEPS) 92.24\n",
      "Epoch 00181 | Time(s) 0.1347 | Loss 0.5567 | Accuracy 0.7100 | ETputs(KTEPS) 92.27\n",
      "Epoch 00182 | Time(s) 0.1347 | Loss 0.5935 | Accuracy 0.7080 | ETputs(KTEPS) 92.29\n",
      "Epoch 00183 | Time(s) 0.1347 | Loss 0.5599 | Accuracy 0.7020 | ETputs(KTEPS) 92.31\n",
      "Epoch 00184 | Time(s) 0.1346 | Loss 0.5854 | Accuracy 0.7080 | ETputs(KTEPS) 92.36\n",
      "Epoch 00185 | Time(s) 0.1345 | Loss 0.5689 | Accuracy 0.7080 | ETputs(KTEPS) 92.42\n",
      "Epoch 00186 | Time(s) 0.1345 | Loss 0.5714 | Accuracy 0.7180 | ETputs(KTEPS) 92.40\n",
      "Epoch 00187 | Time(s) 0.1345 | Loss 0.5616 | Accuracy 0.7100 | ETputs(KTEPS) 92.39\n",
      "Epoch 00188 | Time(s) 0.1345 | Loss 0.5664 | Accuracy 0.7060 | ETputs(KTEPS) 92.42\n",
      "Epoch 00189 | Time(s) 0.1345 | Loss 0.5757 | Accuracy 0.7000 | ETputs(KTEPS) 92.45\n",
      "Epoch 00190 | Time(s) 0.1345 | Loss 0.5267 | Accuracy 0.7000 | ETputs(KTEPS) 92.46\n",
      "Epoch 00191 | Time(s) 0.1345 | Loss 0.5449 | Accuracy 0.7020 | ETputs(KTEPS) 92.46\n",
      "Epoch 00192 | Time(s) 0.1344 | Loss 0.5477 | Accuracy 0.7080 | ETputs(KTEPS) 92.47\n",
      "Epoch 00193 | Time(s) 0.1344 | Loss 0.5855 | Accuracy 0.7060 | ETputs(KTEPS) 92.47\n",
      "Epoch 00194 | Time(s) 0.1345 | Loss 0.5887 | Accuracy 0.7080 | ETputs(KTEPS) 92.45\n",
      "Epoch 00195 | Time(s) 0.1344 | Loss 0.5655 | Accuracy 0.7060 | ETputs(KTEPS) 92.47\n",
      "Epoch 00196 | Time(s) 0.1344 | Loss 0.5482 | Accuracy 0.7120 | ETputs(KTEPS) 92.50\n",
      "Epoch 00197 | Time(s) 0.1345 | Loss 0.5302 | Accuracy 0.7120 | ETputs(KTEPS) 92.43\n",
      "Epoch 00198 | Time(s) 0.1345 | Loss 0.5374 | Accuracy 0.7140 | ETputs(KTEPS) 92.43\n",
      "Epoch 00199 | Time(s) 0.1344 | Loss 0.5336 | Accuracy 0.7100 | ETputs(KTEPS) 92.46\n",
      "\n",
      "Test Accuracy 0.7190\n"
     ]
    }
   ],
   "source": [
    "!../../gcn_text_categorization/venv/bin/python3 train.py citeseer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/4tb/nabarun/nlp/SGC\n"
     ]
    }
   ],
   "source": [
    "%cd /4tb/nabarun/nlp/SGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1048s\n",
      "Validation Accuracy: 0.7920 Test Accuracy: 0.8070\n",
      "Pre-compute time: 0.1048s, train time: 0.3237s, total: 0.4285s\n"
     ]
    }
   ],
   "source": [
    "!../gcn_text_categorization/venv/bin/python3 citation.py --dataset cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0070s\n",
      "Validation Accuracy: 0.7720 Test Accuracy: 0.7770\n",
      "Pre-compute time: 0.0070s, train time: 0.1229s, total: 0.1299s\n"
     ]
    }
   ],
   "source": [
    "!../gcn_text_categorization/venv/bin/python3 citation.py --dataset pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/4tb/nabarun/nlp/SGC/normalization.py:24: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n",
      "0.0083s\n",
      "Validation Accuracy: 0.6960 Test Accuracy: 0.6800\n",
      "Pre-compute time: 0.0083s, train time: 0.1318s, total: 0.1401s\n"
     ]
    }
   ],
   "source": [
    "!../gcn_text_categorization/venv/bin/python3 citation.py --dataset citeseer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
